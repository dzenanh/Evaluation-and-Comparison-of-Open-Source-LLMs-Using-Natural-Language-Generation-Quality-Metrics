Raw model,Model,,Average ⬆️,IFEval,BBH,MATH Lvl 5,GPQA,MUSR,MMLU-PRO,Type
"0: {model: 'meta-llama-3-70b-instruct', runs: 58600000}",meta-llama-3-70b-instruct,,36.18,80.99,50.19,23.34,4.92,10.92,46.74,"💬 chat models (RLHF, DPO, IFT, ...)"
"1: {model: 'meta-llama-3-8b', runs: 48900000}",meta-llama-3-8b,,13.41,14.55,24.5,3.25,7.38,6.24,24.55,🟢 pretrained
"2: {model: 'meta-llama-3-8b-instruct', runs: 35100000}",meta-llama-3-8b-instruct,,23.91,74.08,28.24,8.69,1.23,1.6,29.6,"💬 chat models (RLHF, DPO, IFT, ...)"
"3: {model: 'mixtral-8x7b-instruct-v0.1', runs: 8800000}",mixtral-8x7b-instruct-v0.1,,24.35,53.95,34.02,9.06,7.61,12.11,29.36,"💬 chat models (RLHF, DPO, IFT, ...)"
"4: {model: 'llama-2-7b-chat', runs: 8400000}",llama-2-7b-chat,,9.4,39.65,4.49,0.68,0.56,3.48,7.52,"💬 chat models (RLHF, DPO, IFT, ...)"
"5: {model: 'llama-2-70b-chat', runs: 7400000}",llama-2-70b-chat,,12.73,49.58,4.61,0.91,1.9,3.48,15.92,"💬 chat models (RLHF, DPO, IFT, ...)"
"6: {model: 'llama-2-13b-chat', runs: 4400000}",llama-2-13b-chat,,11,39.85,7.16,0.6,0,8.16,10.26,"💬 chat models (RLHF, DPO, IFT, ...)"
"7: {model: 'mistral-7b-instruct-v0.2', runs: 2900000}",mistral-7b-instruct-v0.2,,18.44,54.96,22.91,2.64,3.47,7.61,19.08,"💬 chat models (RLHF, DPO, IFT, ...)"
"8: {model: 'mistral-7b-v0.1', runs: 1700000}",mistral-7b-v0.1,,14.5,23.86,22.02,2.49,5.59,10.68,22.36,🟢 pretrained
"9: {model: 'mistral-7b-instruct-v0.1', runs: 902000}",mistral-7b-instruct-v0.1,,13.57,45.02,13.79,1.51,0,5.77,15.34,"💬 chat models (RLHF, DPO, IFT, ...)"
"10: {model: 'dolly-v2-12b', runs: 453100}",dolly-v2-12b,,6.38,23.55,6.38,1.44,0,5.5,1.43,🔶 fine-tuned on domain-specific datasets
"11: {model: 'meta-llama-3-70b', runs: 339400}",meta-llama-3-70b,,26.37,16.03,48.71,16.54,19.69,16.01,41.21,🟢 pretrained
"12: {model: 'yi-34b-chat', runs: 253900}",yi-34b-chat,,23.9,46.99,37.62,4.31,11.74,8.36,34.37,"💬 chat models (RLHF, DPO, IFT, ...)"
"13: {model: 'vicuna-13b', runs: 251200}",vicuna-13b,,,,,,,,,
"14: {model: 'yi-6b', runs: 158100}",yi-6b,,13.6,28.93,19.41,1.51,2.57,7.04,22.12,🟢 pretrained
"15: {model: 'flan-t5-xl', runs: 136600}",flan-t5-xl,,,,,,,,,
"16: {model: 'stablelm-tuned-alpha-7b', runs: 111600}",stablelm-tuned-alpha-7b,,,,,,,,,
"17: {model: 'llama-7b', runs: 98500}",llama-7b,,,,,,,,,
"18: {model: 'gemma-2b-it', runs: 82300}",gemma-2b-it,,7.22,26.9,5.21,0.45,3.8,3.03,3.92,"💬 chat models (RLHF, DPO, IFT, ...)"
"19: {model: 'gemma-7b-it', runs: 64900.00000000001}",gemma-7b-it,,12.84,38.68,11.94,1.59,4.59,12.53,7.72,"💬 chat models (RLHF, DPO, IFT, ...)"
"20: {model: 'nous-hermes-2-solar-10.7b', runs: 60400}",nous-hermes-2-solar-10.7b,,23.32,52.79,34.99,5.21,5.82,13.83,27.31,"💬 chat models (RLHF, DPO, IFT, ...)"
"21: {model: 'oasst-sft-1-pythia-12b', runs: 32400}",oasst-sft-1-pythia-12b,,3.67,10.55,4.78,1.44,1.01,2.99,1.25,🔶 fine-tuned on domain-specific datasets
"22: {model: 'nous-hermes-2-yi-34b-gguf', runs: 8500}",nous-hermes-2-yi-34b-gguf,,,,,,,,,
"23: {model: 'gpt-j-6b', runs: 8400}",gpt-j-6b,,6.55,25.22,4.91,1.21,0,5.25,2.68,🟢 pretrained
"24: {model: 'nous-hermes-llama2-awq', runs: 7200}",nous-hermes-llama2-awq,,,,,,,,,
"25: {model: 'gemma-7b', runs: 6800}",gemma-7b,,15.28,26.59,21.12,6.42,4.92,10.98,21.64,🟢 pretrained
"26: {model: 'yi-6b-chat', runs: 4000}",yi-6b-chat,,14,33.95,17,0.68,5.93,3.57,22.9,"💬 chat models (RLHF, DPO, IFT, ...)"
"27: {model: 'qwen1.5-72b', runs: 3800}",qwen1.5-72b,,,,,,,,,
"28: {model: 'phi-2', runs: 2700}",phi-2,,15.45,27.39,28.04,2.42,2.91,13.84,18.09,🟢 pretrained
"29: {model: 'replit-code-v1-3b', runs: 1900}",replit-code-v1-3b,,,,,,,,,
"30: {model: 'gemma-2b', runs: 588}",gemma-2b,,7.27,20.38,8.25,2.72,0.67,7.56,4.06,🟢 pretrained
"31: {model: 'qwen1.5-14b', runs: 281}",qwen1.5-14b,,20.22,29.05,30.06,16.47,5.93,10.46,29.37,🟢 pretrained
"32: {model: 'mamba-2.8b', runs: 216}",mamba-2.8b,,,,,,,,,
"33: {model: 'phixtral-2x2_​8', runs: 197}",phixtral-2x2_​8,,,,,,,,,
"34: {model: 'qwen1.5-7b', runs: 109}",qwen1.5-7b,,15.22,26.84,23.08,4.46,6.49,9.16,21.29,🟢 pretrained
"35: {model: 'mamba-130m', runs: 101}",mamba-130m,,,,,,,,,
"36: {model: 'olmo-7b', runs: 74}",olmo-7b,,,,,,,,,
"37: {model: 'mamba-1.4b', runs: 45}",mamba-1.4b,,,,,,,,,
"38: {model: 'mamba-2.8b-slimpj', runs: 45}",mamba-2.8b-slimpj,,,,,,,,,
"39: {model: 'mamba-370m', runs: 33}",mamba-370m,,,,,,,,,
"40: {model: 'mamba-790m', runs: 27}",mamba-790m,,,,,,,,,